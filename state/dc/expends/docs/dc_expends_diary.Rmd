---
title: "District Contributions"
author: "Kiernan Nicholls"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This package contains
functions custom made to help facilitate the processing of campaign finance data.

```{r load_packages, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tidytext, # text analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  scales, # format strings
  knitr, # knit documents
  vroom, # read files fast
  glue, # combine strings
  here, # relative storage
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where does this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

Data comes courtesy of the Washington, [DC Office of Campaign Finance (OCF)][03].

The data was published 2016-10-06 and was last updated 2019-05-07. Each record represents a single
contribution made.

As the [OCF website][04] explains: 

> The Office of Campaign Finance (OCF) provides easy access to all contributions and expenditures reported from 2003, through the current reporting period. Because the system is updated on a daily basis, you may be able to retrieve data received by OCF after the latest reporting period. This data is as reported, but may not be complete.

[03]: https://ocf.dc.gov/ "OCF"
[04]: https://ocf.dc.gov/service/view-contributions-expenditures

### About

The data is found on the dc.gov [OpenData website][05]. The file abstract reads:

> The Office of Campaign Finance (OCF), in cooperation with OCTO, is pleased to publicly share
election campaign expenditures data. The Campaign Finance Office is charged with administering and
enforcing the District of Columbia laws pertaining to campaign finance operations, lobbying
activities, conflict of interest matters, the ethical conduct of public officials, and constituent
service and statehood fund programs. OCF provides easy access to all contributions and expenditures
reported from 2003, through the current reporting period. Because the system is updated on a daily
basis, you may be able to retrieve data received by OCF after the latest reporting period. This
data is as reported, but may not be complete. Visit the http://ocf.dc.gov for more information.

[05]: https://opendata.dc.gov/datasets/campaign-financial-expenditures
[06]: http://geospatial.dcgis.dc.gov/ocf/

## Import

The most recent file can be read directly from the OCF with `readr::read_csv()`.

```{r read_raw_csv}
dir_raw <- here("dc", "expends", "data", "raw")
dir_create(dir_raw)
raw_url <- "https://opendata.arcgis.com/datasets/f9d727168d204aa79c8be9091c967604_35.csv"

dc <- 
  read_csv(raw_url) %>% 
  clean_names("snake") %>%
  mutate_if(is_character, str_to_upper) %>% 
  select(1:7, 8)
```

## Explore

There are `r nrow(dc)` records of `r length(dc)` variables in the full database.

```{r glimpse}
head(dc)
tail(dc)
glimpse(dc)
```

### Distinct

The variables range in their degree of distinctness.

```{r n_distinct}
glimpse_fun(dc, n_distinct)
```

The `purpose` variable is an open ended text field.

```{r sample_purpose}
sample(dc$purpose, 10) %>% 
  cat(sep = "\n")
```

But we can perform token analysis on the strings.

```{r}
dc %>% 
  filter(!is.na(purpose)) %>% 
  unnest_tokens(word, purpose) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE) %>% 
  head(20) %>% 
  ggplot(aes(reorder(word, n), n)) +
  geom_col(aes(fill = n)) +
  scale_y_continuous(labels = comma) +
  scale_fill_continuous(guide = FALSE) +
  coord_flip() +
  labs(
    title = "DC Expenditure Purposes",
    x = "Word", 
    y = "Frequency"
  )
```

### Missing

There are several variables missing key values:

```{r count_na}
glimpse_fun(dc, count_na)
```

Any row with a missing `candidatename`, `payee`, _or_ `amount` will have a `TRUE` value in the new
`na_flag` variable.

```{r na_flag, collapse=TRUE}
dc <- flag_na(dc, candidatename, payee, amount)
sum(dc$na_flag)
percent(mean(dc$na_flag))
```

### Duplicates

There are no duplicate records.

```{r get_dupes, collapse=TRUE}
dc <- flag_dupes(dc, everything())
sum(dc$dupe_flag)
dc <- select(dc, -dupe_flag)
```

### Ranges

#### Amounts

The `amount` varies from `r scales::dollar(min(dc$amount, na.rm = T))` to 
`r scales::dollar(max(dc$amount, na.rm = T))`.

```{r amount_range, collapse=TRUE}
summary(dc$amount)
sum(dc$amount < 0, na.rm = TRUE)
```

```{r amount_hist, echo=FALSE}
dc %>% 
  ggplot(aes(amount)) +
  geom_histogram(fill = RColorBrewer::brewer.pal(3, "Dark2")[3]) +
  scale_x_continuous(
    labels = scales::dollar, 
    trans = "log10"
  )
```

#### Dates

The dates range from `r min(dc$dc$transactiondate)` and `r max(dc$dc$transactiondate)`. There are
`r sum(dc$transactiondate > today())` records with a date greater than `r today()`.

```{r date_range, collapse=TRUE}
summary(as_date(dc$transactiondate))
sum(dc$transactiondate > today())
```

```{r year_bar, echo=FALSE}
dc %>% 
  count(year = year(transactiondate)) %>% 
  mutate(even = is_even(year)) %>% 
  ggplot(aes(x = year, y = n)) +
  geom_col(aes(fill = even)) +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = dollar) +
  theme(legend.position = "bottom") +
  labs(
    title = "DC Expenditures by Year",
    fill = "Election Year",
    x = "Year",
    y = "Expenditures Made"
  )
```

```{r amount_line_month, echo=FALSE}
dc %>%
  mutate(year = year(transactiondate), even = is_even(year)) %>% 
  group_by(year, even) %>% 
  summarise(median_amount = median(amount, na.rm = TRUE)) %>% 
  ggplot(aes(year, median_amount)) +
  geom_col(aes(fill = even)) +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = scales::dollar) +
  theme(legend.position = "bottom") +
  labs(
    title = "DC Median Expenditure",
    x = "Year",
    y = "Median Expenditure Cost"
  )
```

Since we've already used `readr::parse_datetime()`, we can use `lubridate::year()` to create a new
variable representing the year of the receipt.

```{r mutate_year}
dc <- mutate(dc, transactionyear = year(transactiondate))
```

## Separate

We will have to break the `address` variable into distinct variables for each component (address,
city, state, ZIP).

```{r head_address}
select(sample_frac(dc), address)
```

First, we can split the `address` variable into new columns at each comma in the original variable
using `tidyr::separate()`

```{r separate_address}
dc %>% separate(
  col = address,
  into = c(glue("street{1:5}"), "city_sep", "state_zip"),
  sep = ",\\s",
  remove = FALSE,
  extra = "merge",
  fill = "left"
) -> dc
```

Since the street address portion of the `address` variable can contain a wide variety of
components, we have split the original column into an excessive number of new columns. Now, we can
use `tidyr::unite()` to merge those many columns back into a single `address_sep` variable.

```{r unite_address}
dc %>% unite(
  starts_with("street"),
  col = "address_sep",
  sep = " ",
  na.rm = TRUE,
  remove = TRUE
) -> dc
```

Finally, the state and ZIP code portion of the string is not separated by a comma, so we will have
to separate this into two strings based on the space before the ZIP code digits.

```{r separate_zip}
dc %>% separate(
  col = state_zip,
  into = c("state_sep", "zip_sep"),  
  sep = "\\s{1,}(?=\\d)",
  remove = TRUE
) -> dc
```

```{r show_address_sep, echo=FALSE}
dc %>% 
  sample_frac() %>% 
  select(
    address,
    address_sep,
    city_sep,
    state_sep,
    zip_sep
  )
```

There are a number of columns where the lack of a component in the original `address` has caused
the separation to incorrectly shift content.

```{r view_separation, echo=FALSE}
dc %>% 
  select(
    address,
    address_sep,
    city_sep,
    state_sep,
    zip_sep
  ) %>% 
  filter(state_sep %out% valid_state) %>% 
  drop_na() %>%
  distinct() %>% 
  sample_frac()
```

We can fix many of these errors using index subsetting. The most common error is the original 
`address` leaving out the "DC" part of the string.

```{r fix_separation}
z <- dc[which(dc$state_sep == "WASHINGTON" & dc$address_sep == ""), ]
z$address_sep <- z$city_sep
z$city_sep <- z$state_sep
z$state_sep <- "DC"
dc[which(dc$state_sep == "WASHINGTON" & dc$address_sep == ""), ] <- z
z <- dc[which(dc$state_sep %out% valid_state & !is.na(dc$state_sep) & dc$address_sep == ""), ]
z$address_sep <- z$city_sep
z$city_sep <- z$state_sep
z$state_sep <- NA
dc[which(dc$state_sep %out% valid_state & !is.na(dc$state_sep) & dc$address_sep == ""), ] <- z
```

The remaining invalid separations are simply long-form state names, which can be fixed in the
normalization stage.

```{r view_bad_sep, echo=FALSE}
dc %>% 
  select(
    address,
    address_sep,
    city_sep,
    state_sep,
    zip_sep
  ) %>% 
  filter(state_sep %out% valid_state) %>% 
  drop_na() %>%
  distinct() %>% 
  sample_frac()
```

## Normalize

Once these components of `address` have been separated into their respective columns, we can use
the `campfin::normal_*()` functions to improve searchability.

### Address

The `campfin::normal_address()` function can be used to improve consistency by removing punctuation
and expanding abbreviations.

```{r normal_address}
dc <- dc %>% 
  mutate(
    address_norm = normal_address(
      address = address_sep,
      add_abbs = usps_street,
      na_rep = TRUE
    )
  )
```

```{r show_address_norm, echo=FALSE}
dc %>% 
  select(
    address_sep, 
    address_norm
  ) %>% 
  distinct() %>% 
  sample_frac()
```

### ZIP

Similarly, the `campfin::normal_zip()` function can be used to form valid 5-digit US ZIP codes.

```{r normal_zip}
dc <- dc %>% 
  mutate(
    zip_norm = normal_zip(
      zip = zip_sep,
      na_rep = TRUE
    )
  )
```

```{r zip_change, echo=FALSE}
dc %>% 
  filter(zip_sep != zip_norm) %>% 
  select(zip_sep, zip_norm) %>% 
  distinct() %>% 
  sample_frac()
```

This process improves the consistency of our ZIP code variable and removes some obviously invalid
ZIP codes (e.g., 00000, 99999).

```{r zip_progress}
progress_table(
  dc$zip_sep,
  dc$zip_norm,
  compare = valid_zip
)
```

### State

There also a handful of invalid state abbreviations in `state_sep`, which can be repaired or
removed with `campfin::normal_state()`

```{r state_setdiff}
setdiff(dc$state_sep, valid_state)
```

```{r normal_state}
dc <- dc %>% 
  mutate(
    state_norm = normal_state(
      state = state_sep,
      abbreviate = TRUE,
      na = c("", "NA"),
      na_rep = TRUE,
      valid = NULL
    )
  )
```

```{r state_progress}
progress_table(
  dc$state_sep,
  dc$state_norm,
  compare = valid_state
)
```

### City

The `city_sep` variable is the most difficult to normalize due to the sheer number of possible valid values and the variety in which those values can be types. There is a four stage process we can use to make extremely confident changes.

1. **Normalize** the values with `campfin::normal_zip()`.
1. **Compare** to the _expected_ value with `dplyr::left_join()`.
1. **Swap** with some expected values using `campfin::str_dist()` and `campfin::is_abbrev()`.
1. **Refine** the remaining similar values with `refinr::n_gram_merge()`.

#### Normal City

```{r normal_city}
dc <- dc %>% 
  mutate(
    city_norm = normal_city(
      city = str_replace(city_sep, "DC", "WASHINGTON"),
      geo_abbs = usps_city,
      st_abbs = c("DC", "D C"),
      na = invalid_city,
      na_rep = TRUE
    ),
    city_norm = expand_abbrev(city_norm, c("HTS" = "HEIGHTS"))
  )
```

```{r city_change, echo=FALSE}
dc %>% 
  filter(str_trim(city_sep) != city_norm) %>% 
  count(city_sep, city_norm, state_norm, sort = TRUE)
```

#### Match City

To assess the normalization of city values, it's useful to compare our `city_norm` value to the 
_expected_ city value for that record's state and ZIP code. To do this, we can use
`dplyr::left_join()` with the `campfin::zipcodes` data frame.

```{r match_city}
dc <- dc %>% 
  left_join(zipcodes, by = c("zip_norm" = "zip", "state_norm" = "state")) %>% 
  rename(city_match = city)
```

Most of our `city_match` values are the same as `city_norm`, and most of the different values are
records where no matched city could be found for a record's state and/or ZIP code.

```{r match_assess}
percent(mean(dc$city_norm == dc$city_match, na.rm = TRUE))
percent(prop_na(dc$city_match))
```

#### Swap city

The next step involves comparing our `city_norm` values to `city_match`. We want to check whether
`city_match` might be the valid value for any invalid `city_norm`. We only want to use this matched
value if we can be very confident. To do this, we'll use two tests: (1) `campfin::str_dist()`
checks the string distance between the two values, (2) `campfin::is_abbrev()` checks whether
`city_norm` might be an abbreviation of `city_match`. See the help files (`?is_abbrev`) to
understand exactly what these two functions test.

```{r compare_match}
dc <- dc %>%
  # remove any empty strings
  map_if(is_character, na_if, "") %>% 
  as_tibble() %>% 
  mutate(
    match_dist = str_dist(city_norm, city_match),
    match_abb = is_abbrev(city_norm, city_match)
  )
```

```{r check_comp_stats}
summary(dc$match_dist)
sum(dc$match_abb, na.rm = TRUE)
```

If a `city_norm` value has either (1) a really small string distance or (2) appears to be an
abbreviation of `city_match`, we can confidently use the matched value of the messy `city_norm`.

```{r swap_city}
dc <- dc %>% 
  mutate(
    city_swap = if_else(
      condition = match_abb | match_dist == 1,
      true = city_match,
      false = city_norm
    )
  )
```

Here we can see the `r sum(dc$match_abb, na.rm = TRUE)` records where `city_norm` appears to be an abbreviation of `city_match`, so the later was used in `city_swap`.

```{r view_city_abbs}
dc %>% 
  filter(match_abb) %>% 
  count(state_norm, zip_norm, city_norm, city_match, match_abb, sort = TRUE)
```

Furthermore, `r sum(dc$match_dist == 1, na.rm = TRUE)` records has a string distance of only 1,
meaning only 1 character was different between `city_norm` and `city_match`, so again the later was
used in `city_swap`.

```{r view_city_dist}
dc %>% 
  filter(match_dist == 1) %>% 
  count(state_norm, zip_norm, city_norm, city_match, match_abb, sort = TRUE)
```

#### Refine City

The above steps catch most changes, but we can do one last check using the OpenRefine key collision
and n-gram merging algorithms to check for any further valid fixes. These algorithms group similar
values and use the most common value in each group.

```{r refine_city}
good_refine <- dc %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge(dict = valid_city) %>% 
      n_gram_merge(numgram = 1)
  ) %>% 
  # keep only rows where a change was made
  filter(city_refine != city_swap) %>% 
  # keep only rows where a _correct_ change was made
  inner_join(
    y = zipcodes,
    by = c(
      "city_refine" = "city",
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  )
```

Very few changes were made this way, but they are useful changes nonetheless.

```{r count_refine}
count(x = good_refine, state_norm, city_swap, city_refine, sort = TRUE)
```

```{r join_refine}
dc <- dc %>% 
  left_join(good_refine) %>% 
  mutate(city_refine = coalesce(city_refine, city_swap))
```

#### City Progress

```{r}
dc %>% 
  filter(city_refine %out% valid_city) %>% 
  count(state_norm, zip_norm, city_refine, sort = TRUE) %>% 
  drop_na()
```

By two common Washington/Maryland suburbs to our list of common cities, we can see our normalization process has brought us above 99% "valid."

```{r expand_valid}
valid_city <- c(valid_city, "LANDOVER", "CHEVERLY")
```

```{r city_progress}
progress_table(
  dc$city_sep,
  dc$city_norm,
  dc$city_swap,
  dc$city_refine,
  compare = valid_city
)
```

```{r save_table, echo=FALSE}
pg <- progress_table(
  dc$city_sep,
  dc$city_norm,
  dc$city_swap,
  dc$city_refine,
  compare = valid_city
)
```

```{r prop_valid_bar, echo=FALSE}
pg %>% 
  mutate(stage = as_factor(c("Separate", "Normalize", "Swap", "Refine"))) %>% 
  ggplot(aes(x = stage, y = prop_in)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = RColorBrewer::brewer.pal(3, "Dark2")[2]) +
  coord_cartesian(ylim = c(0.9, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "DC City Cleaning Progress",
    y = "Percent Valid",
    x = "Wrangling Stage"
  )
```

```{r distinct_val_bar, echo=FALSE}
pg %>% 
  mutate(
    stage = as_factor(c("Separate", "Normalize", "Swap", "Refine")),
    Valid = n_distinct - n_diff,
    Unknown = n_diff
  ) %>% 
  pivot_longer(cols = c(Valid, Unknown)) %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = name)) +
  scale_y_continuous(labels = comma) +
  scale_fill_manual(values = RColorBrewer::brewer.pal(9, "Paired")[c(6, 4)]) +
  labs(
    title = "DC City Cleaning Progress",
    y = "Distinct Values",
    x = "Wrangling Stage",
    fill = "Value Status"
  )
```

## Conclude

```{r conclue_amount, echo=FALSE}
min_amount <- scales::dollar(min(dc$amount, na.rm = TRUE))
max_amount <- scales::dollar(max(dc$amount, na.rm = TRUE))
```

```{r conclue_date, echo=FALSE}
min_date <- as.character(min(dc$transactiondate, na.rm = TRUE))
max_date <- as.character(max(dc$transactiondate, na.rm = TRUE))
```

```{r conclue_na, echo=FALSE}
not_na <- scales::percent(mean(!dc$na_flag))
```

1. How are `r nrow(dc)` records in the database.
1. There are `r sum(dc$dupe_flag)` duplicate records.
1. The `amount` values range from `r min_amount` to `r max_amount`.
1. The `transactiondate` ranges from `r print(min_date)` to `r print(max_date)`.
1. The `r sum(dc$na_flag)` records missing a `candidatename` or `payee` value  are flagged with the
logical `na_flag` variable.
1. Consistency in ZIP codes and state abbreviations has been fixed from `address`.
1. The `zip_clean` variable contains the 5 digit ZIP from `address`.
1. The `transactionyear` variable contains the 4 digit year of the receipt.
1. Only `r not_na` of records contain all the data needed to identify the transaction.

## Lookup

```{r lookup_city}
lookup <- read_csv("dc/expends/data/dc_city_lookup.csv") %>% select(1:2)
dc <- left_join(dc, lookup, by = "city_refine")

progress_table(
  dc$city_refine, 
  dc$city_refine2, 
  compare = valid_city
)
```

## Write

```{r}
dir_proc <- here("dc", "expends", "data", "processed")
dir_create(dir_proc)
raw_file <- glue("{dir_proc}/dc_expends_clean.csv")

dc <- dc %>% 
  select(
    -address_sep,
    -city_sep,
    -state_sep,
    -zip_sep,
    -city_norm,
    -city_match,
    -match_dist,
    -match_abb,
    -city_swap,
    -city_refine
  )

if (!this_file_new(raw_file)) {
  write_csv(dc, raw_file, na = "")
}
```
