---
title: "Alaska Expenditures"
author: "Kiernan Nicholls"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  echo = TRUE,
  dfrning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

```{r p_load, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_current_gh("kiernann/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  tidyverse, # data manipulation
  RSelenium, # navigate browser
  lubridate, # datetime strings
  magrittr, # pipe opperators
  janitor, # dataframe clean
  zipcode, # clean & database
  refinr, # cluster & merge
  vroom, # quickly read files
  glue, # create strings
  here, # relative storage
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

Data is obtained through the Alaska Public Offices Commission [APOC] Online Reports tool. 

> To encourage public confidence, the commission staff provides free reports available online. Here
you can discover submissions to or decisions by the Alaska Public Offices Commission... For current
information (e.g.from 2011 to present), please go to [APOC Online Reports][03]. Specific links are
also provided below, under Current Reporting Systems.

From this page we will chose "Search Expenditures." On the [Independent Expenditures][04] page, we
can select "All Complete Forms."

[03]: https://aws.state.ak.us/ApocReports/ "reports"
[04]: https://aws.state.ak.us/ApocReports/IndependentExpenditures/IEExpenditures.aspx "expends"

## Import

To import the data, we will have to first download a raw immutable version to disk. The files can
be downloaded as CSV, DOC, or TXT.

### Download

Since the download process is hidden behind a search port, we will have to use the `RSelenium`
package to navigate the web page in a remote web browser.

```{r raw_dir}
# create a directory for the raw data
raw_dir <- here("ak", "expends", "data", "raw")
dir_create(raw_dir)
```

```{r download_raw, eval=FALSE}
# open the driver with auto download options
remote_driver <- rsDriver(
  port = 4444L,
  browser = "firefox",
  extraCapabilities = makeFirefoxProfile(
    list(
      browser.download.dir = raw_dir,
      browser.download.folderList = 2L,
      browser.helperApps.neverAsk.saveToDisk = "text/csv"
    )
  )
)

# navigate to the APOC download site
expend_url <- "https://aws.state.ak.us/ApocReports/IndependentExpenditures/IEExpenditures.aspx"
remote_browser <- remote_driver$client
remote_browser$navigate(expend_url)

# click the export button
export_button <- remote_browser$findElement("css", "#M_C_csfFilter_btnExport")
export_button$clickElement()

# click the CSV option button
csv_button <- remote_browser$findElement("css", "#M_C_csfFilter_ExportDialog_hlAllCSV")
csv_button$clickElement()

# close the browser and driver
remote_browser$close()
remote_driver$server$stop()
```

### Read

```{r read_raw}
ak <- 
  read_csv(
    file = glue("{raw_dir}/IE_Expenditure_{format(today(), \"%m-%d-%Y\")}.CSV"),
    na = c("", "na", "n/a", "NA", "N/A", "-", "none", "NONE", "UNK"),
    col_types = cols(
      .default = col_character(),
      Date = col_date("%m/%e/%Y"),
      Amount = col_number(),
      `Election Year` = col_integer(),
      `Report Year` = col_integer(),
      Submitted = col_date("%m/%d/%Y")
    )
  ) %>% 
  clean_names() %>% 
  mutate_if(is_character, str_to_upper)
```

## Explore

There are `nrow(df)` records of `length(df)` variables in the full database.

```{r glimpse}
glimpse(sample_frac(ak))
```

### Distinct

The variables range in their degree of distinctness.

```{r n_distinct}
ak %>% glimpse_fun(n_distinct)
```

```{r payment_type_bar, echo=FALSE}
ggplot(data = ak) + 
  geom_bar(mapping = aes(payment_type)) +
  coord_flip()
```

```{r state_bar, echo=FALSE}
ak %>% 
  count(recipient_state, sort = TRUE) %>%
  mutate(p = n/sum(n)) %>% 
  head() %>% 
  ggplot() + 
  geom_col(mapping = aes(reorder(recipient_state, p), p)) +
  scale_y_continuous(labels = scales::percent) +
  coord_flip()
```

```{r country_bar, echo=FALSE}
ggplot(data = ak) + 
  geom_bar(mapping = aes(recipient_country)) +
  scale_y_log10()
```

```{r year_bar, echo=FALSE}
ggplot(data = ak) +
  geom_bar(mapping = aes(election_year)) +
  scale_x_continuous(breaks = 2012:2019) +
  coord_cartesian(xlim = c(2012:2019))
```

```{r filer_type_bar, echo=FALSE}
ggplot(data = ak) +
  geom_bar(mapping = aes(filer_type))
```

### Missing

There are relatively few variables with much missing information, aside from `payment_detail`.

```{r count_na}
ak %>% glimpse_fun(count_na)
```

We will flag any records missing key values used to identify an expenditure.

```{r na_flag}
ak <- ak %>% mutate(na_flag = is.na(recipient))
```

### Duplicates

There are no duplicate rows.

```{r get_dupes, collapse=TRUE}
ak_dupes <- get_dupes(ak)
nrow(ak_dupes)
rm(ak_dupes)
```

### Ranges

Explore the continuous variables with `ggplot2::geom_histogram()` and `base::summary()`

#### Amounts

There are `sum(ak$amount == 0)` recods with an `amount` value of zero.

Below is a `glimpse()` at the smallest and largest `amount` records.

```{r glimpse_min_max}
glimpse(ak %>% filter(amount == min(amount)))
glimpse(ak %>% filter(amount == max(amount)))
```

### Dates

```{r date_future, collapse=TRUE}
max(ak$date, na.rm = TRUE)
sum(ak$date > today(), na.rm = T)
ak <- ak %>% mutate(date_flag = date > today())
```

```{r date_past, collapse=TRUE}
min(ak$date, na.rm = TRUE)
sum(year(ak$date) < 2010, na.rm = T)
```

## Wrangle

### Year

Add a `year` variable from `date` using `lubridate::year()` after parsing the variable with 
`readr::col_date()`.

```{r add_year}
ak <- ak %>% mutate(year = year(date))
```

### Address

The `address` variable should be minimally cleaned by removing punctuation and fixing white-space.

```{r clean_address}
ak <- ak %>% 
  mutate(
    normal_address = normal_address(
      address = recipient_address,
      add_abbs = usps,
      na_rep = TRUE
    )
  )
```

### Zipcode

```{r clean_zipcodes, collapse=TRUE}
# normalize character string
ak <- ak %>% mutate(zip_clean = normal_zip(recipient_zip, na_rep = TRUE))
# remove foreign zips
ak$zip_clean[which(ak$recipient_country != "USA")] <- NA
# check improvement
mean(ak$recipient_zip %in% valid_zip)
mean(ak$zip_clean %in% valid_zip)
unique(ak$zip_clean[ak$zip_clean %out% valid_zip])
```

```{r view_bad}
ak %>% 
  filter(zip_clean %out% valid_zip) %>% 
  filter(!is.na(zip_clean)) %>% 
  select(
    recipient_city,
    recipient_state,
    recipient_zip,
    zip_clean
  ) %>% 
  distinct()
```

### State

The database uses full state names instead of the 2 character abbreviations typically used. We can
convert between them.
  
```{r view_states}
sample(ak$recipient_state, 10)
```

```{r abbreviate_states, collapse=TRUE}
ak <- ak %>%
  mutate(
    state_clean = normal_state(
      state = recipient_state,
      na_rep = TRUE,
      abbreviate = TRUE
    )
  )

prop_in(ak$state_clean, valid_state)
```

### City

Cleaning city values is the most complicated. This process involves four steps:

1. Prepare raw city values by removing invalid data and reducing inconsistencies
1. Match prepared city values with the _actual_ city name of that record's ZIP code
1. swap prepared city values with the ZIP code match _if_ only 1 edit is needed
1. Refine swapped city values with key collision and n-gram fingerprints

```{r count_city, collapse=TRUE}
n_distinct(ak$recipient_city)
mean(ak$recipient_city %in% valid_city)
```

#### Prep

```{r prep_city, collapse=TRUE}
ak <- ak %>% 
  mutate(
    city_prep = normal_city(
      city = recipient_city %>% str_replace("^ANC$", "ANCHORAGE"),
      st_abbs = c("AK", "ALASKA", "DC", "MA"),
      geo_abbs = usps_city,
      na = na_city
    )
  )

n_distinct(ak$city_prep)
mean(ak$city_prep %in% valid_city)
```

#### Swap

```{r match_dist, collapse=TRUE}
ak <- ak %>%
  left_join(
    geo,
    by = c("zip_clean" = "zip")
  ) %>%
  rename(city_match = city) %>%
  select(-state) %>% 
  mutate(
    match_dist = stringdist(city_match, city_prep),
    city_swap = if_else(
      condition = !is.na(match_dist) & match_dist <= 2,
      true = city_match,
      false = city_prep
    )
  )

summary(ak$match_dist)
n_distinct(ak$city_swap)
mean(ak$city_swap %in% valid_city)
```

#### Lookup

```{r}
lookup <- read_csv(file = here("ak", "expends", "data", "ak_city_lookup.csv"))
lookup <- select(lookup, city_swap, city_new)
ak <- left_join(ak, lookup)
n_distinct(ak$city_new)
prop_in(ak$city_new, valid_city)
```

## Conclude

1. There are `r nrow(ak)` records in the database
1. There are `r sum(ak$dupe_flag)` records with duplicate rows
1. The ranges for dates and amounts are reasonable
1. Consistency in strings has been fixed with the custom `normalize_*()` functions
1. The five-digit `zip_clean` variable has been created
1. The `expenditure_year` variable has been created with `lubridate::year()`
1. There are `r sum(is.na(ak$recipient))` records with missing `recipient` values flagged with 
`na_flag`

## Write

```{r write_clean}
dir_create("ak/expends/data/processed")
ak %>% 
  select(
   -recipient_address,
   -recipient_zip,
   -recipient_state,
   -recipient_city,
   -city_prep,
   -city_match,
   -match_dist,
   -city_swap
  ) %>% 
  write_csv(
    path = "ak/expends/data/processed/ak_expends_clean.csv",
    na = ""
  )
```

