---
title: "Utah Lobbying Registration Data Diary"
author: "Yanqi Xu"
date: "`r format(Sys.time())`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 3
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
set.seed(5)
```

## Project


The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.


Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:


1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved


## Objectives


This document describes the process used to complete the following objectives:


1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction


## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

```{r p_load, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_current_gh("irworkshop/campfin")
pacman::p_load(
  rvest, # read html tables
  httr, # interact with http requests
  stringdist, # levenshtein value
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tidytext, # string analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  knitr, # knit documents
  glue, # combine strings
  scales, #format strings
  here, # relative storage
  fs, # search storage 
  vroom, #read deliminated files
  readxl #read excel files
)
```

```{r fix_fun, echo=FALSE, collapse = TRUE}
# fix conflict
here <- here::here
```
This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.


The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Download
Set the download directory first.
```{r create raw_dir}
# create a directory for the raw data
raw_dir <- here("ut", "lobby", "data", "raw","reg")
# create a docs directory for this diary
doc_dir <- here("ut", "lobby", "docs")
dir_create(c(raw_dir, doc_dir))
```
According to [Utah Code 36-11 & Utah Code 36-11a] [03], 

Definition of a lobbyist:
> Generally, you are a lobbyist if you get paid to communicate with a public official, local official, or education
official for the purpose of influencing legislative, executive, local, or education action.

There are two types of lobbyists, a) state lobbyist b)local and education lobbyist.
> You are a state lobbyist if you lobby state legislators, elected state executive branch officials (such as the
governor), and non-elected officials within the state executive or legislative branch with certain decisionmaking powers.
> You are a local and education lobbyist if you lobby:  
 Elected members in local governments and non-elected officials within local governments that have
certain decision-making powers.
     Education officials, including elected members of the State Board of Education, State Charter School
      Board, local school boards, charter school governing boards, and non-elected officials within these
organizations that have certain decision-making powers.
If you lobby officials in both categories, register as a state lobbyist.

Reporting Requirements:
> A lobbyist is not required to file a quarterly financial report (Q1, Q2, Q3) if he or she
has not made an expenditure during that reporting period. All lobbyists – state, local, and education – are still
required to file the Quarter 4 (Year End) Report by January 10 of each year.

[03]: https://elections.utah.gov/Media/Default/Lobbyist/2019%20Lobbyist/Lobbyist%20Frequently%20Asked%20Questions%202019%20(updated%20after%20session).pdf

This Rmd file documents the wrangling process of UT registration data only, whereas the expenditure data is wrangled in a separate data diary.

IRW obtained a copy of lobbying registration data from Utah Lieutenant Governor's Office. The data is as current as Jan. 9 ,2020. 

## Reading
We can read the xls file here.
```{r read csv}
ut_reg <- dir_ls(raw_dir, glob = "*.xlsx") %>% read_xlsx() %>% clean_names() %>% mutate_if(is.character, str_to_upper)
glimpse(ut_reg)
```

###Columns
#### Year
Here we read everything as strings, and we will need to convert them back to numeric or datetime objects.
```{r create year}}
ut_reg <- ut_reg %>% mutate (year = year(date_organization_created))
```
#### Date
```{r create date}
ut_reg <- ut_reg %>% mutate (date = as_date(date_organization_created))
```

#### Name
We'll separate first and last names from the name field. 
```{r full name}
ut_reg <- ut_reg %>% 
  mutate(first_name = str_match(lobbyist_name, ",\\s*(.[^,]+$)")[,2],
         last_name = str_remove(lobbyist_name, first_name) %>% str_remove(",") %>% str_trim())
```

## Explore

### Duplicates

We'll use the `flag_dupes()` function to see if there are records identical to one another and flag the duplicates. A new variable `dupe_flag` will be created.

```{r flag dupe}
ut_reg <- flag_dupes(ut_reg, dplyr::everything())
```
We can see that there's no duplicates in the data.
### Missing

```{r count_na}
ut_reg  %>% col_stats(count_na)
```

We'll flag entries where the `name`, `organization_name`, and `city` fields are missing.

```{r flag_na}
ut_reg <- ut_reg %>% flag_na(lobbyist_name, organization_name, city)
```

```{r }
ut_reg %>% 
  group_by(year) %>% 
  ggplot(aes(year)) +
  scale_x_continuous(breaks = 2012:2020) +
  geom_bar(fill = RColorBrewer::brewer.pal(3, "Dark2")[1]) +
  labs(
    title = "Utah Lobbyists Registration by Year",
    caption = "Source : Utah Lieutenant Governor's Office",
    x = "Year",
    y = "Count"
  )
```

## Wrangling

### Phone
```{r normal phone}
ut_reg <- ut_reg %>% mutate(phone_norm = normal_phone(phone))
```

### Address
```{r normal address}
ut_reg <- ut_reg %>%
  unite(
  address_1,
  address_2,
  col = address_combined,
  sep = " ",
  remove = FALSE,
  na.rm = TRUE
  ) %>%
  mutate(address_clean = normal_address(
  address = address_combined,
  abbs = usps_city,
  na_rep = TRUE
  )) %>% 
  select(-address_combined)
```


### ZIP 
We can use the `norm_zip` function to clean up the ZIP code fields.
```{r client normal zip}
prop_in(ut_reg$zip, valid_zip, na.rm = TRUE) %>% percent()

ut_reg <- ut_reg %>% 
  mutate(zip5 = normal_zip(zip, na_rep = TRUE))

prop_in(ut_reg$zip5, valid_zip, na.rm = TRUE) %>% percent()
```

### City
```{r}
prop_in(ut_reg$city, valid_city, na.rm = TRUE) %>% percent()

ut_reg <- ut_reg %>% 
 mutate(city_norm = normal_city(city = city,
                                            abbs = usps_city,
                                            states = c(valid_state),
                                            na = invalid_city,
                                            na_rep = TRUE))
```

### State
We'll see that although information about cities and zips are present, the data file is missing a `state` column. We'll create a data column and determine the states based on `zip`. 
```{r clients clean state}
ut_reg <- ut_reg %>% 
  left_join(zipcodes, by = c("zip5" = "zip")) %>% 
    rename(city_match = city.y,
         city = city.x)
prop_in(ut_reg$city_norm, valid_city, na.rm = TRUE)
```


#### Swap
Then, we will compare these normalized `city_norm` values to the _expected_ city value for that
vendor's ZIP code. If the [levenshtein distance][09] is less than 3, we can confidently swap these
two values.

[09]: https://en.wikipedia.org/wiki/Levenshtein_distance

```{r cl swap_city}
ut_reg <- ut_reg %>% 
  mutate(
    match_abb = is_abbrev(city_norm, city_match),
    match_dist = str_dist(city_norm, city_match),
    city_swap = if_else(
      condition = match_abb | match_dist == 1,
      true = city_match,
      false = city_norm
    )
  ) %>% 
  select(
    -city_match,
    -match_dist,
    -match_abb
  )

prop_in(ut_reg$city_swap, valid_city, na.rm = TRUE) %>% percent()
```

Besides the `valid_city` vector, there is another vector of `extra_city` that contains other locales. We'll incorporate that in our comparison.

```{r valid_place check, echo=FALSE}
valid_place <- c(valid_city, extra_city) %>% unique()
prop_in(ut_reg$city_swap, valid_place, na.rm = TRUE) %>% percent()
```

The `campfin` package uses the `check_city` function to check for misspelled cities by matching the returned results of the misspelled cities from the Google Maps Geocoding API. The function also pulls the clean city and place names in the `lobbyist_city_fetch` column for us to inspect and approve. 

```{r lobbyist check_city}
api_key <- Sys.getenv("GEOCODING_API")

ut_reg_out <- ut_reg %>% 
  filter(city_swap %out% valid_place) %>% 
  drop_na(city_swap,state) %>% 
  count(city_swap, state) 

ut_reg_out <- ut_reg_out %>% cbind(
  pmap_dfr(.l = list(ut_reg_out$city_swap, ut_reg_out$state), .f = check_city, key = api_key, guess = T))

ut_reg_out <- ut_reg_out %>%
  mutate(guess_place = str_replace(guess_place,"^WEST$", "SALT LAKE CITY"))
```
Then we'll join the results back to the original dataframe.
```{r}
ut_reg_out <- ut_reg_out %>% mutate(city_fetch = coalesce(guess_city, guess_place))

ut_reg <- ut_reg_out %>% 
  filter(!check_city_flag) %>% 
  select(city_swap, state, city_fetch) %>% 
  right_join(ut_reg, by = c("city_swap","state")) 

ut_reg <- ut_reg %>% mutate(city_clean = coalesce(city_fetch, city_swap))
```
We can view the normalization progress here.
```{r prog_table, echo=FALSE}
progress_table(
  ut_reg$city,
  ut_reg$city_norm,
  ut_reg$city_swap,
  ut_reg$city_clean,
  compare = valid_place
) %>% 
  kable()
```


We can now get rid of the iterative columns generated while we were processing the data.
```{r}
ut_reg <- ut_reg %>% 
  select(-c(city_norm,city_fetch,city_swap))
```

This is a very fast way to increase the valid proportion in the lobbyist data frame to
`r percent(prop_in(ut_reg$city_clean, valid_place, na.rm = TRUE))` and reduce the number of distinct
_invalid_ values from `r length(setdiff(ut_reg$city, valid_place))` to only
`r length(setdiff(ut_reg$city_clean, valid_place))`


## Join
We'll see that the data frame includes both the business organizations that they work for (lobbying firms), and the clients they represent. Running the following commands tells us that the lobbyists' affiliationn is unique for each year. Thus we can separate registration to clients and organizations into two (BUSINESS and PRINCIPAL) and bind them back together.

```{r}
freq_tb <- ut_reg %>% count(lobbyist_name,organization_type, year)
bus <- freq_tb %>% filter(organization_type == "BUSINESS") %>% arrange(desc(n))
prin <- freq_tb %>% filter(organization_type == "PRINCIPAL") %>% arrange(desc(n))
```

```{r join pre}
ut_business <- ut_reg %>% 
  filter(organization_type == "BUSINESS") %>% 
  rename(business = organization_name) %>% 
  select(-organization_type)

ut_prin <- ut_reg %>% 
  filter(organization_type == "PRINCIPAL") %>% 
  rename(principal = organization_name) %>% 
  select(-organization_type)
# names(ut_business) %>% setdiff(names(ut_prin))just captures what's inside business but not in prin, and we only need to eliminate this column when we apply left_join to ut_prin.  
ut_bind <- ut_prin %>% 
  left_join(ut_business, by = names(ut_business) %>% setdiff(names(ut_business) %>% setdiff(names(ut_prin))))

sample_frac(ut_bind)
```

## Export

```{r write clean}
clean_dir <- here("ut", "lobby", "data", "processed","reg")
dir_create(clean_dir)
ut_bind %>% 
  mutate_if(is.character, str_to_upper) %>% 
  write_csv(
    path = glue("{clean_dir}/ut_lobby_reg.csv"),
    na = ""
  )
```


