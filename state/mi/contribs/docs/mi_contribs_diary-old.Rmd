---
title: "Michigan Contributions"
author: "Kiernan Nicholls"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
if (!interactive()) {
  options(width = 120)
  set.seed(5)
}
```

```{r create_docs_dir, eval=FALSE, echo=FALSE, include=FALSE}
doc_dir <- fs::dir_create(here::here("mi", "contribs", "docs"))
```

## Project

The Accountability Project is an effort to cut across data silos and give
journalists, policy professionals, activists, and the public at large a simple
way to search across huge volumes of public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each
dataset row as a transaction. For each transaction there should be (at least) 3
variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `zip`
1. Create a `year` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze,
and communicate these results. The `pacman` package will facilitate their
installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This
package contains functions custom made to help facilitate the processing of
campaign finance data.

```{r load_packages, message=FALSE, warning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  tidyverse, # data manipulation
  lubridate, # datetime strings
  magrittr, # pipe opperators
  gluedown, # printing markdown
  janitor, # dataframe clean
  refinr, # cluster and merge
  scales, # format strings
  knitr, # knit documents
  rvest, # read html pages
  vroom, # read files fast
  glue, # combine strings
  here, # relative storage
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a
sub-directory of the more general, language-agnostic
[`irworkshop/accountability_datacleaning`][repo] GitHub repository.

The `R_campfin` project uses the [RStudio projects][rproj] feature and should be
run as such. The project also uses the dynamic `here::here()` tool for file
paths relative to _your_ machine.

```{r where_here}
# where does this document knit?
here::here()
```

[repo]: https://github.com/irworkshop/accountability_datacleaning
[rproj]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

## Data

This data is obtained from the Michigan [Board of Elections (BOE)][boe] 
[Campaign Finance Reporting (CFR)][cfr] system. The data is provided as 
[annual ZIP archive files][data] for the years 1998 through 2020. These files
are updated nightly.

[boe]: https://www.michigan.gov/sos/0,4670,7-127-1633---,00.html
[cfr]: https://www.michigan.gov/sos/0,4670,7-127-1633_8723---,00.html
[data]: https://miboecfr.nictusa.com/cfr/dumpall/cfrdetail/

The CFR also provides a README file with a record layout.

```{r key_page, echo=FALSE}
raw_base <- "https://miboecfr.nictusa.com/cfr/dumpall/cfrdetail"
key_url <- str_c(raw_base, "ReadMe_CONTRIBUTIONS.html", sep = "/")
key_page <- read_html(key_url)
```

```{r key_desc, results='asis', echo=FALSE}
key_page %>% 
  html_node("p") %>% 
  html_text() %>% 
  md_quote()
```

```{r key_vars, results='asis', echo=FALSE}
key_page %>% 
  html_node("table") %>% 
  html_table() %>% 
  as_tibble() %>% 
  mutate(
    X1 = md_code(X1),
    X2 = str_trim(str_squish(str_trunc(X2, 90)))
  ) %>% 
  md_table(col.names = c("Variable", "Description"))
```

## Import

To import the data for processing, we will have to download each archive file
and read them together into a single data frame object.

### Download

We will scrape the download page for every archive link, then downloading each
to a local directory.

```{r download_raw}
raw_dir <- dir_create(here("mi", "contribs", "data", "raw"))
raw_page <- read_html(raw_base)
raw_urls <- raw_page %>% 
  html_node("table") %>% 
  html_nodes("a") %>% 
  html_attr("href") %>% 
  str_subset("contributions") %>% 
  str_c(raw_base, ., sep = "/")
raw_paths <- path(raw_dir, basename(raw_urls))
if (!all(this_file_new(raw_paths))) {
  for (i in seq_along(raw_paths)) {
    if (file_exists(raw_paths[i])) {
      skip(); message("file exists")
    } else {
      download.file(raw_urls[i], raw_paths[i])
    }
  }
}
```

```{r download_raw2, include=FALSE, eval=FALSE}
write_zip <- function(url, dir) {
  path <- fs::path(dir, basename(url))
  r <- httr::GET(url, httr::write_disk(path, overwrite = TRUE))
}
map(raw_urls, write_zip, raw_dir)
```

### Read

Since the larger files are split with the column header only in the first, we
will have to read these headers separately. The last column only records the
time the files are downloaded.

```{r mic_names}
mic_names <- str_split(read_lines(raw_paths[1])[1], "\t")[[1]]
mic_names <- mic_names[-length(mic_names)]
mic_names[1:3] <- c("doc_id", "page_no", "cont_id")
mic_names[length(mic_names)] <- "runtime"
```

Using `vroom::vroom()`, we can read all `r length(dir_ls(raw_dir))` archive
files at once.

```{r read_tsv, eval=TRUE}
mic <- vroom(
  file = raw_paths,
  delim = "\t",
  skip = 1,
  col_names = mic_names,
  col_types = cols(
    .default = col_character(),
    page_no = col_integer(),
    doc_stmnt_year = col_integer(),
    received_date = col_date_usa(),
    amount = col_double(),
    aggregate = col_double(),
    runtime = col_skip()
  )
)
```

```{r read_map, echo=FALSE, eval=FALSE}
mic <- map_dfr(
  .x = raw_paths,
  .f = read_delim,
  delim = "\t",
  skip = 1,
  escape_backslash = FALSE, 
  escape_double = FALSE,
  col_names = mic_names,
  col_types = cols(
    .default = col_character(),
    page_no = col_integer(),
    doc_stmnt_year = col_integer(),
    received_date = col_date_usa(),
    amount = col_double(),
    aggregate = col_double(),
    runtime = col_skip()
  )
)
```

Some of the columns have an inconsistent number of spacing, which we can trim.

```{r str_trim, eval=FALSE}
mic <- mutate_if(mic, is_character, str_trim)
```

## Explore

```{r glimpse}
glimpse(mic)
tail(mic)
```

```{r summary_amount}
summary(mic$amount)
```

```{r amount_histogram, echo=FALSE}
mic %>%
  ggplot(aes(amount)) +
  geom_histogram(fill = dark2["purple"]) +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "Michigan Contribution Amount Distribution",
    subtitle = "from 1998 to 2020",
    caption = "Source: Michigan Board of Elections",
    x = "Amount",
    y = "Count"
  )
```

```{r amount_violin}
mic %>% 
  filter(amount > 1, amount < 1000000) %>% 
  ggplot(aes(x = com_type, y = amount, fill = com_type)) +
  geom_violin(draw_quantiles = TRUE, scale = "width", ) +
  scale_y_continuous(labels = dollar, trans = "log10") +
  scale_fill_brewer(palette = "Dark2", guide = FALSE) +
  labs(
    title = "Michigan Contribution Amount Distribution by Committee Type",
    subtitle = "from 1998 to 2020",
    caption = "Source: Michigan Board of Elections",
    x = "Committee Type",
    y = "Amount"
  )
```

We can add a new `received_year` variable using `lubridate::year()`.

```{r year_add}
mic <- mutate(mic, received_year = year(received_date))
```

Since we know the records cover the years 1998 through 2020, we will have to do
some fixing to dates from the distant past or future.

```{r date_range}
# view file name dates
unique(str_extract(dir_ls(raw_dir), "\\d{4}"))
# count and fix old dates
min(mic$received_date, na.rm = TRUE)
sum(mic$received_year < 1998, na.rm = TRUE)
which_old <- which(mic$received_year < 1990)
fix_old <- mic$received_year[which_old] %>% 
  str_replace("\\d(?=\\d{1}$)", "9") %>% 
  str_pad(width = 4, side = "left", pad = "1") %>% 
  as.numeric()
mic$received_year[which_old] <- fix_old

# count and fix future dates
max(mic$received_date, na.rm = TRUE)
sum(mic$received_date > today(), na.rm = TRUE)
mic$received_year[which(mic$received_year > 2020)] <- c(2011, 2006)
```

```{r year_bar, echo=FALSE}
mic %>% 
  count(received_year) %>% 
  ggplot(aes(received_year, n)) +
  geom_col(fill = dark2["purple"]) +
  coord_cartesian(xlim = c(1998, 2020)) +
  scale_y_continuous(labels = comma) +
  labs(
    title = "Michigan Contributions Received by Year",
    subtitle = "from 1998 to 2020",
    caption = "Source: Michigan Board of Elections",
    x = "Year Received",
    y = "Count"
  )
```

## Wrangle

To improve the consistency and searchability of the database, we will have to
do some text normalization. The `campfin::normal_*()` functions help us make
consistent strings.

### Address

The `address` variable can be cleaned with `campfin::normal_address()`, which
will force consistent case, remove punctuation, and replace street suffix 
variations with the official USPS abbreviation.

```{r address_norm}
mic <- mutate(
  .data = mic,
  address_norm = normal_address(
    address = address,
    abbs = usps_street,
    na = invalid_city,
    na_rep = TRUE
  )
)
```

```{r address_view, echo=FALSE}
mic %>% 
  select(contains("address")) %>% 
  distinct() %>% 
  sample_n(10)
```

This process also automatically removed a number of invalid values.

```{r address_removed}
prop_na(mic$address)
prop_na(mic$address_norm)
mic %>% 
  select(contains("address")) %>% 
  filter(!is.na(address), is.na(address_norm)) %>% 
  count(address, sort = TRUE)
```

### ZIP

Similarly, we can use the `campfin::normal_zip()` function to try and repair
some common problems with ZIP codes, primarily removing any ZIP+4 suffixes.

```{r zip_norm}
sample(mic$zip, 5)
mic <- mutate(
  .data = mic,
  zip_norm = normal_zip(
    zip = zip,
    na_rep = TRUE
  )
)
```

```{r zip_progress}
progress_table(
  mic$zip,
  mic$zip_norm,
  compare = valid_zip
)
```

### State

The `campfin::normal_state()` function will make valid 2-digit USPS state
abbreviations.

```{r state_fix}
mic <- mutate(mic, state_norm = state)
# state is first 2 of zip from MI
state_zip <- mic$state_norm == str_sub(mic$zip_norm, end = 2)
mi_zip <- mic$zip_norm %in% zipcodes$zip[zipcodes$state == "MI"]
mic$state_norm[which(state_zip & mi_zip)] <- "MI"
# state is invalid but close to MI
ends_mi <- str_detect(mic$state_norm, "(^M|I$)")
out_state <- mic$state_norm %out% c(valid_state, "MX", "MB")
mic$state_norm[which(out_state & ends_mi)] <- "MI"
```

```{r state_norm}
mic <- mutate(
  .data = mic,
  state_norm = normal_state(
    state = state_norm,
    abbreviate = TRUE,
    na_rep = TRUE,
    valid = valid_state
  )
)
```

```{r state_progress}
progress_table(
  mic$state,
  mic$state_norm,
  compare = valid_state
)
```

### City

Cities are the most difficult variable to normalize due to the number and
variety of valid values. 

#### Normalize

The `campfin::normal_city()` function first forces consistent capitalization,
removes punctuation, and expands common abbreviations.

```{r city_norm}
mic <- mutate(
  .data = mic,
  city_norm = normal_city(
    city = city, 
    abbs = usps_city,
    states = c("MI", "DC", "MICHIGAN"),
    na = invalid_city,
    na_rep = TRUE
  )
)
```

```{r city_view, echo=FALSE}
mic %>% 
  select(contains("city")) %>% 
  filter(city != city_norm) %>% 
  distinct() %>% 
  sample_n(10)
```

#### Swap

We can further reduce these inconsistencies by comparing our normalized value
to the _expected_ value for that record's (normalized) state and ZIP code. Using
`campfin::is_abbrev()` and `campfin::str_dist()`, we can test whether the
expected value is either an abbreviation for or within one character of our
normalized value.

```{r city_swap}
mic <- mic %>% 
  rename(city_raw = city) %>% 
  left_join(
    y = zipcodes,
    by = c(
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_abb = is_abbrev(city_norm, city_match),
    match_dist = str_dist(city_norm, city_match),
    city_swap = if_else(
      condition = !is.na(city_match) & (match_abb | match_dist == 1),
      true = city_match,
      false = city_norm
    )
  ) %>% 
  select(
    -match_abb,
    -match_dist,
    -city_match
  )
```

#### Refine

Additionally, we can pass these swapped `city_swap` values to the OpenRefine
cluster and merge algorithms. These two algorithms cluster similar values and
replace infrequent values with their more common counterparts. This process can
be harmful by making _incorrect_ changes. We will only keep changes where the
state, ZIP code, _and_ new city value all match a valid combination.

```{r refine_city}
good_refine <- mic %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge() %>% 
      n_gram_merge(numgram = 1)
  ) %>% 
  filter(city_refine != city_swap) %>% 
  inner_join(
    y = zipcodes,
    by = c(
      "city_refine" = "city",
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  )
```

```{r view_city_refines, echo=FALSE}
good_refine %>%
  count(
    state_norm, 
    zip_norm, 
    city_swap, 
    city_refine,
    sort = TRUE
  )
```

We can join these good refined values back to the original data and use them
over their incorrect `city_swap` counterparts in a new `city_refine` variable.

```{r join_refine}
mic <- mic %>% 
  left_join(good_refine) %>% 
  mutate(city_refine = coalesce(city_refine, city_swap))
```

#### Check

We can use the `campfin::check_city()` function to pass the remaining unknown
`city_refine` values (and their `state_norm`) to the Google Geocode API. The
function returns the name of the city or locality which most associated with
those values.

This is an easy way to both check for typos and check whether an unknown
`city_refine` value is actually a completely acceptable neighborhood, census
designated place, or some other locality not found in our `valid_city` vector
from our `zipcodes` database.

First, we'll filter out any known valid city and aggregate the remaining records
by their city and state. Then, we will only query those unknown cities which
appear at least ten times.

```{r check_filter}
mic_out <- mic %>% 
  filter(city_refine %out% c(valid_city, extra_city)) %>% 
  count(city_refine, state_norm, sort = TRUE) %>% 
  drop_na() %>% 
  filter(n > 1)
```

Passing these values to `campfin::check_city()` with `purrr::pmap_dfr()` will
return a single tibble of the rows returned by each city/state combination.

First, we'll check to see if the API query has already been done and a file
exist on disk. If such a file exists, we can read it using `readr::read_csv()`.
If not, the query will be sent and the file will be written using
`readr::write_csv()`.

```{r check_send}
check_file <- here("mi", "contribs", "data", "api_check.csv")
if (file_exists(check_file)) {
  check <- read_csv(
    file = check_file
  )
} else {
  check <- pmap_dfr(
    .l = list(
      mic_out$city_refine, 
      mic_out$state_norm
    ), 
    .f = check_city, 
    key = Sys.getenv("GEOCODE_KEY"), 
    guess = TRUE
  ) %>% 
    mutate(guess = coalesce(guess_city, guess_place)) %>% 
    select(-guess_city, -guess_place)
  write_csv(
    x = check,
    path = check_file
  )
}
```

Any city/state combination with a `check_city_flag` equal to `TRUE` returned a
matching city string from the API, indicating this combination is valid enough
to be ignored.

```{r check_accept}
valid_locality <- check$guess[check$check_city_flag]
```

Then we can perform some simple comparisons between the queried city and the
returned city. If they are extremely similar, we can accept those returned
locality strings and add them to our list of accepted additional localities.

```{r check_compare}
valid_locality <- check %>% 
  filter(!check_city_flag) %>% 
  mutate(
    abb = is_abbrev(original_city, guess),
    dist = str_dist(original_city, guess)
  ) %>%
  filter(abb | dist <= 3) %>% 
  pull(guess) %>% 
  c(valid_locality)
```

#### Progress

```{r city_other}
many_city <- c(valid_city, extra_city, valid_locality)
mic$city_refine <- str_remove(mic$city_refine, "\\sTOWNSHIP$")
mic %>% 
  filter(city_refine %out% many_city) %>% 
  count(city_refine, sort = TRUE)
```

```{r city_progress, echo=FALSE}
progress <- progress_table(
  mic$city_raw,
  mic$city_norm,
  mic$city_swap,
  mic$city_refine,
  compare = many_city
) %>% mutate(stage = as_factor(stage))
```

```{r progress_print, echo=FALSE}
kable(progress, digits = 3)
```

You can see how the percentage of valid values increased with each stage.

```{r progress_bar, echo=FALSE}
progress %>% 
  ggplot(aes(x = stage, y = prop_in)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = RColorBrewer::brewer.pal(3, "Dark2")[3]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Michigan City Normalization Progress",
    x = "Stage",
    y = "Percent Valid"
  )
```

More importantly, the number of distinct values decreased each stage. We were
able to confidently change many distinct invalid values to their valid
equivalent.

```{r distinct_bar}
progress %>% 
  select(
    stage, 
    all = n_distinct,
    bad = n_diff
  ) %>% 
  mutate(good = all - bad) %>% 
  pivot_longer(c("good", "bad")) %>% 
  mutate(name = name == "good") %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = name)) +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  theme(legend.position = "bottom") +
  labs(
    title = "Michigan City Normalization Progress",
    subtitle = "Distinct values, valid and invalid",
    x = "Stage",
    y = "Percent Valid",
    fill = "Valid"
  )
  
```

## Conclude

1. There are `comma(nrow(mic))` records in the database.
1. There are `comma(sum(mic$dupe_flag))` duplicate records in the database.
1. The range and distribution of `amount` and `date` seem reasonable.
1. There are `comma(sum(mic$na_flag))` records missing a key variable.
1. Consistency in geographic data was improved with `campfin::normal_*()`.
1. The 5-digit `zip_norm` variable was created with `campfin::normal_zip()`.
1. The 4-digit `year` variable was created with `lubridate::year()`.

## Export

```{r clean_write}
clean_dir <- dir_create(here("mi", "contribs", "data", "clean"))
clean_path <- path(clean_dir, "mi_contribs_clean.csv")
write_csv(mic, clean_path, na = "")
file_size(clean_path)
guess_encoding(clean_path)
```
