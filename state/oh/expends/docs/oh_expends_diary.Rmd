---
title: "Ohio Expenditures"
author: "Kiernan Nicholls & Yanqi Xu"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
if (!interactive()) {
  options(width = 99)
  set.seed(5)
}
```

```{r create_docs_dir, eval=FALSE, echo=FALSE, include=FALSE}
fs::dir_create(here::here("state","oh", "expends", "docs"))
```

## Project

The Accountability Project is an effort to cut across data silos and give
journalists, policy professionals, activists, and the public at large a simple
way to search across huge volumes of public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each
dataset row as a transaction. For each transaction there should be (at least) 3
variables:

1. All **parties** to a transaction.
2. The **date** of the transaction.
3. The **amount** of money involved.

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for entirely duplicated records.
1. Check ranges of continuous variables.
1. Is there anything blank or missing?
1. Check for consistency issues.
1. Create a five-digit ZIP Code called `zip`.
1. Create a `year` field from the transaction date.
1. Make sure there is data on both parties to a transaction.

## Packages

The following packages are needed to collect, manipulate, visualize, analyze,
and communicate these results. The `pacman` package will facilitate their
installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This
package contains functions custom made to help facilitate the processing of
campaign finance data.

```{r load_packages, message=FALSE, warning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  tidyverse, # data manipulation
  lubridate, # datetime strings
  gluedown, # printing markdown
  janitor, # clean data frames
  refinr, # cluster and merge
  scales, # format strings
  aws.s3, # upload to AWS
  knitr, # knit documents
  vroom, # read files fast
  rvest, # html scraping
  glue, # combine strings
  here, # relative paths
  httr, # http requests
  fs # local storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a
sub-directory of the more general, language-agnostic
[`irworkshop/accountability_datacleaning`][tap] GitHub repository.

The `R_campfin` project uses the [RStudio projects][rproj] feature and should be
run as such. The project also uses the dynamic `here::here()` tool for file
paths relative to _your_ machine.

```{r where_here}
# where does this document knit?
here::here()
raw_dir <- dir_create(here("state","oh", "expends", "data", "raw"))
```

[tap]: https://github.com/irworkshop/accountability_datacleaning
[rproj]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

## Data

The data is obtained from the [Ohio Secretary of State][sos]. The OH SOS offers
a file transfer page (FTP) to download data in bulk rather than via searches.

>  Welcome to the Ohio Secretary of State's Campaign Finance File Transfer Page.
This page was developed to allow users to obtain large sets of data faster than
the normal query process. At this page you can download files of pre-queried
data, such as all candidate Expenditures for a particular year or a list of all
active political action committees registered with the Secretary of State. In
addition, campaign finance data filed prior to 2000 is available only on this
site. These files contain all relevant and frequently requested information. If
you are looking for smaller or very specific sets of data please use the regular
Campaign Finance queries listed on the tabs above.

[sos]: https://www.ohiosos.gov/

## Import

### Download

> On the FTP page, please decide which information you would like to download.
Click "Download File" on the right hand side. The system will then proceed to
download the file into Microsoft Excel or provide you will an opportunity to
download the file to the location on your computer (the settings on your
computer will dictate this). You may see a series of dialog boxes on your screen
asking you if you want to run or save the zipped `.exe` file. Follow the dialog
boxes for whichever you chose telling the computer where you want the files
saved. The end result will be a `.csv` file that you can open in Microsoft Excel
or some other database application.

We can download all the Expenditure files by reading the FTP website itself
and scraping each of the "Download" links in the table. This process needs to
be repeated for candidates, PACs, and parties.

```{r raw_urls, eval=FALSE}
ftp_base <- "https://www6.ohiosos.gov/ords/"
#t <- c("CAN", "PAC", "PARTY")
t <- c("CAN")
ftp_url <- glue("f?p=CFDISCLOSURE:73:7027737052457:{t}:NO:RP:P73_TYPE:{t}:")
ftp_url <- str_c(ftp_base, ftp_url)
ftp_params <- character()
ftp_table <- rep(list(NA), length(t))
for (i in seq_along(t)) {
  ftp_page <- read_html(ftp_url[i])
  #table_id <- paste0("#", str_extract(ftp_page, '(?<=id\\=")report_.*(?="\\s)'))
  table_id <- ".info-report > table"
  ftp_table[[i]] <- ftp_page %>%
    html_node(table_id) %>%
    html_table() %>%
    as_tibble() %>%
    select(-last_col()) %>%
    set_names(c("file", "date", "size")) %>%
    mutate_at(vars(2), parse_date_time, "%m/%d/%Y %H:%M:%S %p")
  con_index <- str_which(ftp_table[[i]]$file, "Expenditures\\s-\\s\\d+")
  ftp_params <- ftp_page %>%
    html_node(table_id) %>%
    html_nodes("tr") %>%
    html_nodes("a") %>%
    html_attr("href") %>%
    str_subset("f\\?p") %>%
    `[`(con_index) %>%
    append(ftp_params)
}
```

Then each link can be downloaded to the `/data/raw` directory.

```{r wget, eval = F}
wget <- function(url, dir) {
  system2(
    command = "wget",
    args = c(
      "--no-verbose",
      "--content-disposition",
      url,
      paste("-P", raw_dir),
      wait = 1
    )
  )
}
```

```{r raw_download,eval = F}
raw_urls <- paste0(ftp_base, ftp_params)
if (length(dir_ls(raw_dir)) < 84) {
  map(raw_urls, wget, raw_dir)
}
```

```{r raw_info}
raw_info <- dir_info(raw_dir)
sum(raw_info$size)
(raw_files <- raw_info %>%
  select(file_path = path, size, modification_time) %>% 
  mutate(file_id = as.character(row_number()), .before = 1) %>% 
  mutate(across(file_path, basename)))
```

### Read

> The data is in a "comma delimited" format that loads easily into Microsoft
Excel or Access as well as many other spreadsheet or database programs. Many of
the available files contain a significant quantity of data records. A
spreadsheet program, such as Microsoft Excel, may not allow all of the data in a
file to be loaded because of a limit on the number of available rows. For this
reason, it is advised that a database application be utilized to load and work
with the data available at this site...

We can read all `r nrow(raw_info)` raw CSV files into a single data frame
using `purrr::map_df()` and `readr::read_csv()`. There are some columns that
only exist in the files containing contributions from a PAC, party, etc. Most
columns are shared across all files, so when we join them together into a single
data frame, empty rows will be created for those unique columns.

```{r raw_read}
ohe <- map_df(
  .x = raw_info$path,
  .f = read_csv,
  .id = "file_id",
  na = c("", "NA", "N/A"),
  col_types = cols(
    .default = col_character(),
    MASTER_KEY = col_integer(),
    RPT_YEAR = col_integer(),
    REPORT_KEY = col_integer(),
    EXPEND_DATE = col_date_usa(),
    AMOUNT = col_double(),
    EVENT_DATE = col_date_usa(),
    INKIND = col_logical(),
    DISTRICT = col_integer()
  )
)
```

```{r raw_rename, echo=FALSE}
old_names <- names(ohe)[-1]
ohe <- ohe %>% 
  clean_names() %>%
  left_join(raw_files[, 1:2]) %>% 
  select(-file_id) %>% 
  rename(
    rpt_desc = report_description,
    rpt_key = report_key,
    desc = short_description,
    first = first_name,
    middle = middle_name,
    last = last_name,
    suffix = suffix_name,
    non_ind = non_individual,
    date = expend_date,
    event = event_date,
    cand_first = candidate_first_name,
    cand_last = candidate_last_name
  )
```

We can identify the transaction year and filer type from the source file name.

```{r fil_types}
fil_types <- ohe %>% 
  count(file_path, sort = TRUE) %>% 
  mutate(
    file_type = case_when(str_detect(file_path,"CAC|CAN") ~ "CANDIDATE",
                          str_detect(file_path,"PAC") ~ "PAC",
                          str_detect(file_path, "PPC|PAR") ~ "PARTY"),
    file_year = str_extract(file_path, "(?=.+)\\d{4}.CSV") %>% str_remove(".CSV") %>% as.numeric()
  )
```

```{r fil_plot, echo=FALSE}
fil_types %>% 
  ggplot(aes(x = file_year, y = n)) +
  geom_col(aes(fill = file_type)) +
  scale_fill_brewer(palette = "Dark2", name = "Filer Type") +
  theme(legend.position = "bottom") +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(breaks = seq(min(fil_types$file_year), max(fil_types$file_year), by = 2)) +
  labs(
    title = "Contributions by Filter Type",
    x = "File Year",
    y = "Count"
  )
```

```{r fil_join}
ohe <- left_join(ohe, fil_types %>% select(-n), by = "file_path")
```

## Explore

There are `r comma(nrow(ohe))` rows of `r ncol(ohe)` columns.

```{r glimpse}
glimpse(ohe)
tail(ohe)
```

## Previous

Another way is to read in the previous update and filter out the rows already in the old file. 
```{r read prev, eval=FALSE}
prev_file <- here("state","oh","expends","data","previous")

oh_prev <- read_csv(prev_file %>% dir_ls())

oh_prev <- oh_prev %>% select(intersect(oh_prev %>% names(), ohe %>% names())) %>% filter(file_year == 2020)

ohe <- ohe %>% setdiff(oh_prev)
```


### Missing

Columns vary in their degree of missing values.

```{r na_count}
col_stats(ohe, count_na)
```

We can flag records missing a name, date, or amount after uniting the multiple
contributor name columns into a single variable.

```{r na_flag}
ohe <- ohe %>% 
  unite(
    first, middle, last, suffix, non_ind,
    col = pay_name,
    sep = " ",
    remove = FALSE,
    na.rm = TRUE
  ) %>% 
  mutate(across(where(is.character), na_if, "")) %>% 
  relocate(pay_name, .after = last_col()) %>% 
  flag_na(date, pay_name, amount, com_name)
```

`r percent(mean(ohe$na_flag), 0.01)` of rows are missing a key variable.

```{r na_prop}
sum(ohe$na_flag)
ohe %>% 
  filter(na_flag) %>% 
  select(date, pay_name, amount, com_name)
```

### Duplicate

There are actually quite a few duplicate values in the data. While it's possible
for the same person to contribute the same amount to the same committee on the
same day, we can flag these values anyway.

```{r dupe_flag}
# d1 <- duplicated(ohe, fromLast = FALSE)
# d2 <- duplicated(ohe, fromLast = TRUE)
# ohe <- mutate(ohe, dupe_flag = d1 | d2)
# rm(d1, d2); 
ohe <- ohe %>% flag_dupes(dplyr::everything())
flush_memory()
```

`r percent(mean(ohe$dupe_flag), 0.1)` of rows are duplicated at least once.

```{r dupe_view}
ohe %>% 
  filter(dupe_flag) %>% 
  arrange(date, pay_name) %>% 
  select(date, pay_name, amount, com_name)
```

### Amounts

```{r amount_summary}
summary(ohe$amount)
prop_na(ohe$amount)
mean(ohe$amount <= 0, na.rm = TRUE)
```

There are the smallest and largest transactions.

```{r amount_minmax}
glimpse(ohe[c(which.min(ohe$amount), which.max(ohe$amount)), ])
```

The `amount` values are logarithmically normally distributed.

```{r hist_amount, echo=FALSE}
ohe %>%
  filter(amount > 1, amount <= 1e6) %>% 
  ggplot(aes(amount)) +
  geom_histogram(fill = dark2["purple"]) +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "Ohio Expenditure Amount Distribution",
    x = "Amount",
    y = "Count"
  )
```

### Dates

We can create a new column with a 4-digit year from the `date`.

```{r date_year}
ohe <- mutate(ohe, year = year(date))
```

There are few `date` values with typos making them really small or large.

```{r date_range}
min(ohe$date, na.rm = TRUE)
sum(ohe$year < 1990, na.rm = TRUE)
max(ohe$date, na.rm = TRUE)
sum(ohe$date > today(), na.rm = TRUE)
```

```{r bar_year, echo=FALSE}
ohe %>% 
  count(year) %>% 
  mutate(even = is_even(year)) %>% 
  ggplot(aes(x = year, y = n)) +
  geom_col(aes(fill = even)) + 
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(breaks = seq(1990, 2020, by = 2)) +
  coord_cartesian(xlim = c(1990, 2020)) +
  theme(legend.position = "bottom") +
  labs(
    title = "Ohio Expenditures by Year",
    caption = "Source: {source}",
    fill = "Election Year",
    x = "Year Made",
    y = "Count"
  )
```

## Wrangle

To improve the searchability of the database, we will perform some consistent,
confident string normalization. For geographic variables like city names and
ZIP codes, the corresponding `campfin::normal_*()` functions are tailor made to 
facilitate this process.

### Address

For the street `addresss` variable, the `campfin::normal_address()` function
will force consistence case, remove punctuation, and abbreviate official 
USPS suffixes.

```{r address_norm}
oh_addr_norm <- tibble(
  address = unique(ohe$address),
  address_norm = normal_address(
    address = address,
    abbs = usps_street,
    na_rep = TRUE
  )
)
```

```{r address_join}
ohe <- left_join(ohe, oh_addr_norm, by = "address")
```

```{r address_view, echo=FALSE}
oh_addr_norm
```

### ZIP

For ZIP codes, the `campfin::normal_zip()` function will attempt to create
valid _five_ digit codes by removing the ZIP+4 suffix and returning leading
zeroes dropped by other programs like Microsoft Excel.

```{r zip_norm}
ohe <- mutate(
  .data = ohe,
  zip_norm = normal_zip(
    zip = zip,
    na_rep = TRUE
  )
)
```

```{r zip_progress}
progress_table(
  ohe$zip,
  ohe$zip_norm,
  compare = valid_zip
)
```

### State

```{r state_norm}
prop_in(ohe$state, valid_state)
ohe <- mutate(ohe, state_norm = state)
ohe$state_norm[which(ohe$state == "0H")] <- "OH"
ohe$state_norm[which(ohe$state == "IH")] <- "OH"
ohe$state_norm[which(ohe$state == "PH")] <- "OH"
ohe$state_norm[which(ohe$state == "O")]  <- "OH"
ohe$state_norm[str_which(ohe$state, "^O\\W$")]  <- "OH"
ohe$state_norm[str_which(ohe$state, "^\\WH$")]  <- "OH"
prop_in(ohe$state_norm, valid_state)
```

### City

Cities are the most difficult geographic variable to normalize, simply due to
the wide variety of valid cities and formats.

#### Normal

The `campfin::normal_city()` function is a good start, again converting case,
removing punctuation, but _expanding_ USPS abbreviations. We can also remove
`invalid_city` values.

```{r city_norm}
ohe <- mutate(
  .data = ohe,
  city_norm = normal_city(
    city = city, 
    abbs = usps_city,
    states = c("OH", "DC", "OHIO"),
    na = invalid_city,
    na_rep = TRUE
  )
)
```

#### Swap

We can further improve normalization by comparing our normalized value
against the _expected_ value for that record's state abbreviation and ZIP code.
If the normalized value is either an abbreviation for or very similar to the
expected value, we can confidently swap those two.

```{r city_swap}
ohe <- ohe %>% 
  rename(city_raw = city) %>% 
  left_join(
    y = zipcodes,
    by = c(
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_abb = is_abbrev(city_norm, city_match),
    match_dist = str_dist(city_norm, city_match),
    city_swap = if_else(
      condition = !is.na(match_dist) & (match_abb | match_dist == 1),
      true = city_match,
      false = city_norm
    )
  ) %>% 
  select(
    -city_match,
    -match_dist,
    -match_abb
  )
```

#### Refine

The [OpenRefine] algorithms can be used to group similar strings and replace the
less common versions with their most common counterpart. This can greatly 
reduce inconsistency, but with low confidence; we will only keep any refined
strings that have a valid city/state/zip combination.

[or]: https://openrefine.org/
  
```{r city_refine}
good_refine <- ohe %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge() %>% 
      n_gram_merge(numgram = 1)
  ) %>% 
  filter(city_refine != city_swap) %>% 
  inner_join(
    y = zipcodes,
    by = c(
      "city_refine" = "city",
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  )
```

```{r city_count, echo=FALSE}
good_refine %>%
  count(
    state_norm, 
    zip_norm, 
    city_swap, 
    city_refine,
    sort = TRUE
  )
```

Then we can join the refined values back to the database.

```{r city_join}
ohe <- ohe %>% 
  left_join(good_refine, by = names(.)) %>% 
  mutate(city_refine = coalesce(city_refine, city_swap))
```

#### Progress

```{r city_progress, echo=FALSE}
many_city <- c(valid_city, extra_city)
progress <- progress_table(
  ohe$city_raw,
  ohe$city_norm,
  ohe$city_swap,
  ohe$city_refine,
  compare = many_city
) %>% mutate(stage = as_factor(stage))
kable(progress, digits = 3)
```

You can see how the percentage of valid values increased with each stage.

```{r bar_progress, echo=FALSE}
raw_in <- percent(prop_in(ohe$city_raw, valid_city))
progress %>% 
  ggplot(aes(x = stage, y = prop_in)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = dark2["purple"]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Ohio City Normalization Progress",
    subtitle = glue("Raw at {raw_in} before conversion to uppercase"),
    x = "Stage",
    y = "Percent Valid"
  )
```

More importantly, the number of distinct values decreased each stage. We were
able to confidently change many distinct invalid values to their valid
equivalent.

```{r bar_distinct, echo=FALSE}
progress %>% 
  select(
    stage, 
    all = n_distinct,
    bad = n_diff
  ) %>% 
  mutate(good = all - bad) %>% 
  pivot_longer(c("good", "bad")) %>% 
  mutate(name = name == "good") %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = name)) +
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  scale_y_continuous(labels = comma) +
  theme(legend.position = "bottom") +
  labs(
    title = "Ohio City Normalization Progress",
    subtitle = "Distinct values, valid and invalid",
    x = "Stage",
    y = "Distinct Values",
    fill = "Valid"
  )
```

## Conclude

```{r clean_select}
ohe <- ohe %>% 
  select(
    -city_norm,
    -city_swap,
    city_clean = city_refine
  ) %>% 
  rename_all(~str_remove(., "_raw")) %>% 
  rename_all(~str_replace(., "_norm", "_clean"))
```

```{r clean_glimpse}
glimpse(sample_n(ohe, 20))
```

1. There are `r comma(nrow(ohe))` records in the database.
1. The range and distribution of `amount` and `date` seem reasonable.
1. There are `r comma(sum(ohe$na_flag))` records missing a key variable.
1. Consistency in geographic data has been improved with `campfin::normal_*()`.
1. The 4-digit `year` variable has been created with `lubridate::year()`.

## Export

```{r clean_dir}
clean_dir <- dir_create(here("state","oh", "expends", "data", "clean"))
clean_path <- path(clean_dir, "oh_expends_clean.csv")
write_csv(ohe, clean_path, na = "")
file_size(clean_path)
file_encoding(clean_path) %>% 
  mutate(across(path, path.abbrev))
```

## Upload

We can use the `aws.s3::put_object()` to upload the text file to the IRW server.

```{r s3_upload, eval=FALSE}
s3_path <- path("csv", basename(clean_path))
if (!object_exists(s3_path, "publicaccountability")) {
  put_object(
    file = clean_path,
    object = s3_path, 
    bucket = "publicaccountability",
    acl = "public-read",
    multipart = TRUE,
    show_progress = TRUE
  )
}
```

```{r s3_size,eval=FALSE}
as_fs_bytes(object_size(s3_path, "publicaccountability"))
```

## Dictionary

The following table describes the variables in our final exported file:

```{r dict_make, echo=FALSE}
dict_raw <- tibble(
  var = md_code(names(ohe)),
  old = c(md_code(old_names), rep("",  ncol(ohe) - length(old_names))),
  type = md_code(map_chr(ohe, typeof)),
  def = c(
    "Spending committee name",
    "Master key",
    "Year report filed",
    "Unique report key",
    "Type of report filed",
    "Description of report",
    "Full contributor name",
    "Contributor first name",
    "Contributor middle name",
    "Contributor last name",
    "Contributor name suffix",
    "Contributor non-individual name",
    "Contributor street address",
    "Contributor city name",
    "Contributor state abbreviation",
    "Contributor ZIP+4 code",
    "Date contribution made",
    "Contribution amount",
    "Date fundraising event hosted",
    "Contribution purpose",
    "Flag indicating in-kind contribution",
    "Receiving candidate first name",
    "Receiving candidate last name",
    "Office sought by candidate",
    "District sought by candidate",
    "Candidate political party",
    "Data source file name",
    "Data source file type",
    "Data source file year",
    "Flag for missing date, amount, or name",
    "Flag for completely duplicated record",
    "Calendar year of contribution date",
    "Normalized combined street address",
    "Normalized 5-digit ZIP code",
    "Normalized state abbreviation",
    "Normalized city name"
  )
)
```

```{r dict_md, echo=FALSE}
(dict_md <- kable(
  x = dict_raw,
  format = "markdown",
  col.names = c("Column", "Original", "Type", "Definition")
))
```
