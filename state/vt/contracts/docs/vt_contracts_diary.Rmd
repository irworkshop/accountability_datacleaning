---
title: "Vermont Contracts"
author: "Kiernan Nicholls"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
if (!interactive()) {
  options(width = 99)
  set.seed(5)
}
```

```{r create_docs_dir, eval=FALSE, echo=FALSE, include=FALSE}
doc_dir <- fs::dir_create(here::here("vt", "contracts", "docs"))
```

## Project

The Accountability Project is an effort to cut across data silos and give
journalists, policy professionals, activists, and the public at large a simple
way to search across huge volumes of public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each
dataset row as a transaction. For each transaction there should be (at least) 3
variables:

1. All **parties** to a transaction.
2. The **date** of the transaction.
3. The **amount** of money involved.

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for entirely duplicated records.
1. Check ranges of continuous variables.
1. Is there anything blank or missing?
1. Check for consistency issues.
1. Create a five-digit ZIP Code called `zip`.
1. Create a `year` field from the transaction date.
1. Make sure there is data on both parties to a transaction.

## Packages

The following packages are needed to collect, manipulate, visualize, analyze,
and communicate these results. The `pacman` package will facilitate their
installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This
package contains functions custom made to help facilitate the processing of
campaign finance data.

```{r load_packages, message=FALSE, warning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  tidyverse, # data manipulation
  lubridate, # datetime strings
  gluedown, # printing markdown
  magrittr, # pipe operators
  janitor, # clean data frames
  refinr, # cluster and merge
  scales, # format strings
  knitr, # knit documents
  vroom, # read files fast
  rvest, # html scraping
  glue, # combine strings
  here, # relative paths
  httr, # http requests
  fs # local storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a
sub-directory of the more general, language-agnostic
[`irworkshop/accountability_datacleaning`][tap] GitHub repository.

The `R_campfin` project uses the [RStudio projects][rproj] feature and should be
run as such. The project also uses the dynamic `here::here()` tool for file
paths relative to _your_ machine.

```{r where_here}
# where does this document knit?
here::here()
```

[tap]: https://github.com/irworkshop/accountability_datacleaning
[rproj]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

## Data

Contracts data can be obtained from the [Vermont Department of Finance][vdp], 
hosted on the state [Open Data portal][odp] under the title "Vermont Vendor 
Payments" in the finance category. The data file was originally uploaded on
October 13, 2016 and was last updated May 15, 2020. 

[vdp]: http://finance.vermont.gov/
[odp]: https://data.vermont.gov/

> The payments shown here are exclusive of direct payments to state employees
for salaries, benefits, and, prior to May 2013, employee reimbursable expenses.
The payments are also exclusive of any payments deemed confidential by state
and/or federal statutes and rules, or the confidential nature of the recipients
of certain payments, like direct program benefit payments. (Approximately 1% of
all non-employee payments are excluded under these guidelines.)  
>
> Payments are made through the VISION statewide financial system. Agencies and
departments are responsible for entering their transactions into VISION. While
VISION is the stateâ€™s principal financial system, it is not the sole financial
system in use by the state.
>
> This data is not intended to be legal advice nor is it designed or intended to
be relied upon as authoritative financial, investment, or professional advice.
No entity affiliated with, employed by, or constituting part of the state of
Vermont warrants, endorses, assures the accuracy of, or accepts liability for
the content of any information on this site.  

## Read

The data file can be read directly from the portal with `vroom::vroom()`.

```{r raw_read}
vtc <- vroom(
  file = "https://data.vermont.gov/api/views/786x-sbp3/rows.tsv",
  .name_repair = make_clean_names,
  delim = "\t",
  col_types = cols(
    .default = col_character(),
    `Quarter Ending` = col_date_usa(),
    `Amount` = col_number()
  )
)
```

```{r raw_filter, echo=FALSE}
vtc <- filter(
  .data = vtc,
  !is.na(department)
)
```

## Explore

There are `r comma(nrow(vtc))` rows of `r comma(ncol(vtc))` columns.

```{r glimpse}
glimpse(vtc)
tail(vtc)
```

### Missing

The columns vary in their degree of missing values, but none are missing from
the variables we need to identify transaction parties.

```{r na_count}
col_stats(vtc, count_na)
```

```{r na_flag}
vtc <- vtc %>% flag_na(quarter_ending, vendor, amount, department)
if (sum(vtc$na_flag) == 0) {
  vtc <- select(vtc, -na_flag)
} else {
  vtc %>% 
    filter(na_flag) %>% 
    select(quarter_ending, vendor, amount, department)
}
```

### Duplicates

There are a number of records that are entirely duplicated across every column.
These records can be flagged with `campfin::flag_na()`.

```{r dupe_flag}
vtc <- flag_dupes(vtc, everything())
sum(vtc$dupe_flag)
```

These may be legitimate contracts/payments made on the same day for the same
amount, but they are flagged nonetheless.

```{r dupe_view}
vtc %>% 
  filter(dupe_flag) %>% 
  select(quarter_ending, vendor, amount, department)
```

### Categorical

```{r distinct_count}
col_stats(vtc, n_distinct)
```

```{r distinct_plots, echo=FALSE}
explore_plot(vtc, department) + scale_x_truncate()
explore_plot(vtc, fund_description) + scale_x_truncate()
explore_plot(vtc, account) + scale_x_truncate()
```

### Amounts

A small percentage of `amount` values are less than or equal to zero, but the
range appears otherwise normal.

```{r amount_summary}
noquote(map_chr(summary(vtc$amount), dollar))
percent(mean(vtc$amount <= 0), 0.01)
```

These are the largest and smallest contract `amount` values:

```{r amnount_minmax}
glimpse(mutate(vtc[which.min(vtc$amount), ], across(amount, dollar)))
glimpse(mutate(vtc[which.max(vtc$amount), ], across(amount, dollar)))
```

The distribution of `amount` values is log-normal, as we would expect.

```{r hist_amount, echo=FALSE}
vtc %>%
  ggplot(aes(amount)) +
  geom_histogram(fill = dark2["purple"]) +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "Vermont Contracts Amount Distribution",
    caption = "Source: Dept. of Finance",
    x = "Amount",
    y = "Count"
  )
```

### Dates

We can add the calendar year from `date` with `lubridate::year()`

```{r date_year}
vtc <- mutate(vtc, year = year(quarter_ending))
```

```{r date_range}
min(vtc$quarter_ending)
sum(vtc$year < 2000)
max(vtc$quarter_ending)
sum(vtc$quarter_ending > today())
```

```{r bar_year, echo=FALSE}
vtc %>% 
  count(year) %>% 
  mutate(even = is_even(year)) %>% 
  ggplot(aes(x = year, y = n)) +
  geom_col(fill = dark2["purple"]) + 
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(breaks = 2009:2020) +
  theme(legend.position = "bottom") +
  labs(
    title = "Vermont Contracts by Year",
    caption = "Source: Dept. of Finance",
    x = "Year Made",
    y = "Count"
  )
```

## State

We can manually add the department state.

```{r state_add}
vtc <- mutate(vtc, dept_state = "VT", .after = department)
```

```{r state_canadian, echo=FALSE}
ca_url <- "https://www.ncbi.nlm.nih.gov/books/NBK7254/"
valid_state <- read_html(ca_url) %>% 
  html_node("table") %>% 
  html_table() %>% 
  as_tibble() %>% 
  pull(2) %>% 
  append(valid_state)
```

We can count the `state` abbreviation values that are not American or Canadian.

```{r state_out}
vtc %>% 
  filter(state %out% valid_state) %>% 
  count(state, sort = TRUE) %>% 
  print(n = Inf)
```

Those records with the `state` value of "CD" are Canadian cities with the
proper state/province abbreviation in the `city` name value.

```{r state_cad_count}
vtc %>% 
  filter(state == "CD") %>% 
  count(city, sort = TRUE)
```

```{r state_normal}
vtc <- mutate(
  .data = vtc,
  state_norm = normal_state(
    state = state,
    abbreviate = TRUE,
    na_rep = TRUE,
    valid = valid_state
  )
)
```

```{r state_progress}
progress_table(
  vtc$state, 
  vtc$state_norm,
  compare = valid_state
)
```

## City

Cities are the most difficult geographic variable to normalize, simply due to
the wide variety of valid cities and formats.

#### Normal

The `campfin::normal_city()` function is a good start, again converting case,
removing punctuation, but _expanding_ USPS abbreviations. We can also remove
`invalid_city` values.

```{r city_norm}
vtc <- vtc %>% 
  mutate(
    city_norm = normal_city(
      city = city, 
      abbs = usps_city,
      states = c("VT", "DC", "VERMONT"),
      na = invalid_city,
      na_rep = TRUE
    )
  )
```

#### Progress

```{r city_progress, echo=FALSE}
many_city <- c(valid_city, extra_city)
progress <- progress_table(
  str_to_upper(vtc$city),
  vtc$city_norm,
  compare = many_city
) %>% mutate(stage = as_factor(stage))
kable(progress, digits = 3)
```

You can see how the percentage of valid values increased with each stage.

```{r bar_progress, echo=FALSE}
raw_in <- percent(prop_in(vtc$city_raw, valid_city))
progress %>% 
  ggplot(aes(x = stage, y = prop_in)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = dark2["purple"]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Vermont City Normalization Progress",
    subtitle = glue("Raw at {raw_in} before conversion to uppercase"),
    x = "Stage",
    y = "Percent Valid"
  )
```

More importantly, the number of distinct values decreased each stage. We were
able to confidently change many distinct invalid values to their valid
equivalent.

```{r bar_distinct, echo=FALSE}
progress %>% 
  select(
    stage, 
    all = n_distinct,
    bad = n_diff
  ) %>% 
  mutate(good = all - bad) %>% 
  pivot_longer(c("good", "bad")) %>% 
  mutate(name = name == "good") %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = name)) +
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  scale_y_continuous(labels = comma) +
  theme(legend.position = "bottom") +
  labs(
    title = "Vermont City Normalization Progress",
    subtitle = "Distinct values, valid and invalid",
    x = "Stage",
    y = "Distinct Values",
    fill = "Valid"
  )
```

## Conclude

Before exporting, we can remove the intermediary normalization columns and
rename all added variables with the `_clean` suffix.

```{r clean_select}
vtc <- vtc %>% 
  rename_all(~str_replace(., "_norm", "_clean")) %>% 
  rename_all(~str_remove(., "_raw"))
```

```{r clean_glimpse}
glimpse(sample_n(vtc, 20))
```

1. There are `r comma(nrow(vtc))` records in the database.
1. There are `r comma(sum(vtc$dupe_flag))` duplicate records in the database.
1. The range and distribution of `amount` and `date` seem reasonable.
1. There are `r comma(sum(vtc$na_flag))` records missing key variables.
1. Consistency in geographic data has been improved with `campfin::normal_*()`.
1. The 4-digit `year` variable has been created with `lubridate::year()`.

## Export

Now the file can be saved on disk for upload to the Accountability server.

```{r clean_dir}
clean_dir <- dir_create(here("vt", "contracts", "data", "clean"))
clean_path <- path(clean_dir, "vt_contracts_clean.csv")
write_csv(vtc, clean_path, na = "")
file_size(clean_path)
file_encoding(clean_path)
```

## Upload

Using the [duckr] R package, we can wrap around the [duck] command line tool to
upload the file to the IRW server.

[duckr]: https://github.com/kiernann/duckr
[duck]: https://duck.sh/

```{r clean_upload, eval=FALSE}
# remotes::install_github("kiernann/duckr")
s3_dir <- "s3:/publicaccountability/csv/"
s3_path <- path(s3_dir, basename(clean_path))
if (require(duckr)) {
  duckr::duck_upload(clean_path, s3_path)
}
```

## Dictionary

The following table describes the variables in our final exported file:

```{r dict_make, echo=FALSE}
dict_raw <- tibble(
  var = md_code(names(vtc)),
  type = md_code(map_chr(vtc, typeof)),
  def = c(
    "End date of fiscal quarter made",
    "Spending department name",
    "Spending department state (VT)",
    "Department unit number",
    "Unique vendor number",
    "Full vendor name",
    "Vendor city",
    "Vendor state",
    "Department subdivision",
    "Department ID",
    "Contract/payment amount",
    "Spending account",
    "Source account number",
    "Fund name",
    "Fund number",
    "Flag indicating duplicate record",
    "Fiscal quarter calendar year",
    "Normalized vendor state",
    "Normalized vendor city"
  )
)
```

```{r dict_md, echo=FALSE}
(dict_md <- kable(
  x = dict_raw,
  format = "markdown",
  col.names = c("Column", "Type", "Definition")
))
```
