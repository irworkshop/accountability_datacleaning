---
title: "Vermont Expenditures"
author: "Kiernan Nicholls & Aarushi Sahejpal"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
```

## Objectives

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called ZIP5
1. Create a YEAR field from the transaction date
1. For campaign donation data, make sure there is both a donor AND recipient

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

```{r p_load, message=FALSE, warning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  snakecase, # change string case
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tidytext, # text analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  batman, # rep(NA, 8) Batman!
  refinr, # cluster and merge
  scales, # format strings
  knitr, # knit documents
  rvest, # scrape HTML pages
  vroom, # read files fast
  glue, # combine strings
  here, # relative storage
  httr, # http query
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory
of the more general, language-agnostic `irworkshop/accountability_datacleaning` 
[GitHub repository](https://github.com/irworkshop/accountability_datacleaning).

The `R_campfin` project uses the 
[RStudio projects](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects)
feature and should be run as such. The project also uses the dynamic 
[`here::here()`](https://github.com/jennybc/here_here) tool for
file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where was this document knit?
here::here()
```


## Data

> Definition of Expenditure - 17 V.S.A. 2901(7)
> 
> Expenditure means a payment, disbursement, distribution, advance deposit, loan, or gift of money,
or anything of value paid or promised to be paid for the purpose of influencing an election,
advocating a position on a public question, or supporting or opposing one or more candidates. As
used in this chapter, expenditure shall not include any of the following:
> 
> 1. A personal loan of money to a candidate from a lending institution made in the ordinary course
> of business;
> 2. Services provided without compensation by individuals volunteering their time on behalf of a
> candidate, political committee, or political party;
> 3. Unreimbursed travel expenses paid for by an individual for himself or herself, who volunteers
> personal services to a candidate; or
> 4. Unreimbursed campaign-related travel expenses, paid for by the candidate or the candidates
> spouse.

## Download

[cfs]: https://campaignfinance.vermont.gov/

Data can be downloaded from the [Vermont Campaign Finance System][cfs]. Under the Expenditure section, selection a transaction range that starts on Jan 1st, 1975 -- the earliest day in the digital system -- and then till the present day. We will save this exported text file locally.

## Read


```{r raw_dir}
raw_dir <- here("vt", "expends", "data", "raw")
dir_create(raw_dir)
```

```{r}
raw_csv <- path(raw_dir, "ViewExpendituresList.csv")
has_raw <- !file_exists(raw_csv)
```

```{r}
setwd("/Volumes/TAP/accountability_datacleaning/state/vt/expends/data/raw")
raw_csv <- read_csv("ViewExpenditureList.csv")
```

```{r read_csv}
vt <- 
  here("vt", "expends", "data", "raw", "ViewExpenditureList.csv") %>% 
  read_csv(
    col_types = cols(
      .default = col_character(),
      `Transaction Date` = col_date("%m/%d/%Y %H:%M:%S %p"),
      `Reporting Period` = col_date("%m/%d/%Y %H:%M:%S %p"),
      `Expenditure Amount` = col_number()
    )
  ) %>% 
  clean_names() %>% 
  remove_empty("rows") %>% 
  mutate_if(is.character, str_to_upper) %>% 
  rownames_to_column("id")
```

## Explore 


There are `r nrow(vt)` records of `r length(vt)` variables in the full database.

```{r glimpse}
glimpse(sample_frac(vt))
```

### Distinct

The variables range in their degree of distinctness.

```{r n_distinct}
glimpse_fun(vt, n_distinct)
```

We can use `ggplot2::geom_bar()` to explore the distribution of these least distinct nominal
values.

```{r plot_payee_type, echo=FALSE, fig.height=10}
ggplot(vt) + 
  geom_bar(aes(payee_type)) + 
  coord_flip() +
  labs(title = "VT Payee Type")
```

```{r plot_reg_type, echo=FALSE, fig.height=10}
ggplot(vt) + 
  geom_bar(aes(registrant_type)) + 
  coord_flip() +
  labs(title = "VT Registrant Type")
```

```{r plot_office, echo=FALSE, fig.height=10}
vt$office %>% 
  str_extract("(?:(?!-).)*") %>% 
  str_trim() %>% 
  enframe(NULL) %>%
  ggplot() +
  geom_bar(aes(value)) +
  coord_flip() +
  labs(title = "VT Office")
```

```{r plot_cycle, echo=FALSE, fig.height=10}
ggplot(vt) + 
  geom_bar(aes(election_cycle)) + 
  coord_flip() +
  labs(title = "VT Election Cycle")
```

```{r plot_expend_type, echo=FALSE}
ggplot(vt) + 
  geom_bar(aes(expenditure_type)) +
  labs(title = "VT Expenditure Type (Log)") +
  scale_y_log10()
```

```{r plot_expend_amt_type, echo=FALSE}
vt %>% 
  filter(!is.na(expenditure_type)) %>% 
  ggplot() +
  scale_y_continuous(labels = scales::dollar, trans = "log10") +
  labs(title = "VT Expenditure Amount by Type (Log)") +
  geom_boxplot(
    mapping = aes(
      x = expenditure_type,
      y = expenditure_amount
    )
  )
```

### Duplicate

There are a significant number of duplicate records.

```{r get_dupes}
vt <- flag_dupes(vt, -id)
sum(vt$dupe_flag)
percent(mean(vt$dupe_flag))
```

### Missing

The variables also vary in their degree of values that are `NA` (missing). Note that 68 rows were
removed using `janitor::remove_empty()` during our initial reading of the file. The remaining count
of missing values in each variable can be found below:

```{r count_na}
glimpse_fun(vt, count_na)
```

Most variables have zero `NA` values, aside from the supplemental `public_question` and `comments` 
variables. `NA` values in the `office` variable represent expenditures from non-candidate
registrants.

```{r office_na}
vt %>% 
  group_by(registrant_type) %>% 
  summarise(n_na = sum(is.na(office)))
```

### Ranges

The range of continuous variables will need to be checked for data integrity. There are only three
quasi-continuous variables, the `transaction_date`, `reporting_period`, and `expenditure_amount`.

The range for `trans_amount` seems reasonable enough.

```{r tran_amount_range}
summary(vt$expenditure_amount)
```

```{r plot_exp_amt_type, echo=FALSE}
vt %>% 
  ggplot(mapping = aes(expenditure_amount)) +
  geom_histogram() +
  scale_x_continuous(trans = "log10", labels = scales::dollar) +
  facet_wrap(~expenditure_type, scales = "free_y") +
  labs(
    title = "Distribution of VT Expenditures",
    x = "Expenditure Amount (USD)",
    y = "Number of Expenditures"
  )
```


```{r tran_date_range}
summary(vt$transaction_date)
```

```{r year_add}
vt <- mutate(vt, transaction_year = year(transaction_date))
```

```{r plot_exp_year}
vt %>% 
  group_by(transaction_year) %>% 
  ggplot(mapping = aes(transaction_year)) +
  geom_bar() +
  scale_x_continuous(breaks = seq(2014, 2023)) + 
  labs(
    title = "Number of Expenditures by Year",
    x = "Year",
    y = "Number of Expenditures"
  )
```

For some reason, the reporting period for expenditures begin in 2014 despite our data spanning
2008 to 2019.

```{r rep_per_range}
summary(vt$reporting_period)
```

## Wrangle

We can split the `payee_address` variable into it's base components in new variables using a
combination of `tidyr::separate()` and `tidyr::unite()`.

```{r address_separate}
vt <- vt %>% 
  separate(
    col = payee_address,
    into = c(glue("split_address{1:10}"), "city_sep", "state_zip"),
    sep = ",\\s",
    fill = "left",
    remove = FALSE,
  ) %>% 
  unite(
    starts_with("split_address"),
    col = "address_sep",
    sep = " ",
    na.rm = TRUE
  ) %>% 
  separate(
    col = state_zip,
    into = c("state_sep", "zip_sep"),
    sep = "\\s(?=\\d)"
  )
```

### Address

```{r address_normal}
packageVersion("tidyr")
vt <- vt %>% 
  mutate(
    address_norm = normal_address(
      address = address_sep,
      abbs = usps_street,
      na_rep = TRUE
    )
  )
```

```{r address_view}
vt %>% 
  select(starts_with("address")) %>% 
  distinct() %>% 
  sample_frac()
```

### ZIP

```{r zip_normal}
vt <- vt %>% 
  mutate(
    zip_norm = normal_zip(
      zip = zip_sep,
      na = c("", "NA"),
      na_rep = TRUE
    )
  )
```

```{r zip_progress}
progress_table(
  vt$zip_sep,
  vt$zip_norm,
  compare = valid_zip
)
```

### State

```{r state_normal}
vt <- vt %>% 
  mutate(
    state_norm = normal_state(
      state = state_sep,
      abbreviate = TRUE,
      na_rep = TRUE,
      valid = valid_state
    )
  )
```

```{r state_progress}
progress_table(
  vt$state_sep,
  vt$state_norm,
  compare = valid_state
)
```

### City

```{r normal_city}
vt <- vt %>% 
  mutate(
    city_norm = normal_city(
      city = city_sep, 
      na = invalid_city,
      na_rep = TRUE
    )
  )
```

```{r swap_city}
vt <- vt %>% 
  left_join(
    y = zipcodes,
    by = c(
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_abb = is_abbrev(city_norm, city_match),
    match_exp = is_abbrev(city_match, city_norm),
    match_dist = str_dist(city_norm, city_match),
    city_swap = if_else(
      condition = match_abb | match_exp | match_dist <= 2 | state_norm == city_norm,
      true = city_match,
      false = city_norm
    )
  )
```

```{r city_refine}
good_refine <- vt %>% 
  filter(state_norm == "VT") %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge() %>% 
      n_gram_merge(numgram = 1)
  ) %>% 
  filter(city_refine != city_swap) %>% 
  inner_join(
    y = zipcodes,
    by = c(
      "city_refine" = "city",
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  )
```

```{r city_join}
vt <- vt %>% 
  left_join(good_refine) %>% 
  mutate(city_refine = coalesce(city_refine, city_swap))
```

```{r city_edit}
vt$city_refine[str_which(vt$city_refine, "^VENLO LW$")] <- NA
```

### Progress

To check our progress, we will expand out `valid_city` vector using a list of towns taken directly
from the [vermont.gov](https://www.vermont.gov) website.

```{r city_scrape}
vt_city <- 
  read_html("https://www.vermont.gov/towns-and-Cities") %>% 
  html_node("select") %>% 
  html_nodes("option") %>% 
  html_text(trim = TRUE) 
```

```{r city_expand}
many_city <- unique(c(valid_city, extra_city, vt_city))
```

```{r city_view}
vt %>% 
  filter(city_refine %out% many_city) %>% 
  count(state_norm, zip_norm, city_refine, city_match, sort = TRUE) %>% 
  drop_na(city_refine)
```

```{r city_progress}
progress <- progress_table(
  vt$city_sep,
  vt$city_norm,
  vt$city_swap,
  vt$city_refine,
  compare = many_city
)

progress$stage <- as_factor(progress$stage)
```

```{r progress_print, echo=FALSE}
kable(progress, digits = 3)
```

You can see how the percentage of valid values increased with each stage.

```{r progress_bar, echo=FALSE}
progress %>% 
  ggplot(aes(x = stage, y = prop_in)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = RColorBrewer::brewer.pal(3, "Dark2")[3]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Vermont City Normalization Progress",
    x = "Stage",
    y = "Percent Valid"
  )
```

More importantly, the number of distinct values decreased each stage. We were
able to confidently change many distinct invalid values to their valid
equivilent.

```{r distinct_bar}
progress %>% 
  select(
    stage, 
    all = n_distinct,
    bad = n_diff
  ) %>% 
  mutate(good = all - bad) %>% 
  pivot_longer(c("good", "bad")) %>% 
  mutate(name = name == "good") %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = name)) +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  theme(legend.position = "bottom") +
  labs(
    title = "Vermont City Normalization Progress",
    subtitle = "Distinct values, valid and invalid",
    x = "Stage",
    y = "Distinct Values",
    fill = "Valid"
  )
```

## Conclude

1. There are `r nrow(vt)` records in the database
1. The `r sum(vt$dupe_flag)` duplicate records have been flagged with `dupe_flag`
1. Ranges for continuous variables have been checked and make sense
1. There are no important variables with blank or missing values
1. Consistency issues have been fixed with the `stringr` package
1. The geographic data has been `tidyr::separate()`'d and cleaned with `campfin::normal_*()`.
1. The `transaction_year` variable has been extracted from `transaction_date` with
`readr::col_date()` and `lubridate::year()`
1. There is both a registrant and payee for every record.

## Export

```{r create_proc_dir}
proc_dir <- here("vt", "expends", "data", "processed")
dir_create(proc_dir)
```

```{r clean_trim}
vt <- vt %>% 
  select(
    -address_sep,
    -zip_sep,
    -state_sep,
    -city_sep,
    -city_norm,
    -city_match,
    -match_abb,
    -match_dist,
    -city_swap
  ) %>%
  rename(
    address_clean = address_norm,
    zip_clean = zip_norm,
    state_clean = state_norm,
    city_clean = city_refine
  )
```

```{r clean_write, eval=FALSE}
vt %>% 
  write_csv(
    path = glue("{proc_dir}/vt_expends_20230520.csv"),
    na = ""
  )
```
