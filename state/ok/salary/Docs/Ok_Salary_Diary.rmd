---
title: "Oklahoma Salaries"
author: "Victor Brew"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
if (!interactive()) {
  options(width = 99)
  set.seed(5)
}
```

```{r create_docs_dir, eval=FALSE, echo=FALSE, include=FALSE}
doc_dir <- fs::dir_create(here::here("nc", "salary", "docs"))
```

## Project

The Accountability Project is an effort to cut across data silos and give
journalists, policy professionals, activists, and the public at large a simple
way to search across huge volumes of public data about people and organizations.

Our goal is to standardize public data on a few key fields by thinking of each
dataset row as a transaction. For each transaction there should be (at least) 3
variables:

1. All **parties** to a transaction.
2. The **date** of the transaction.
3. The **amount** of money involved.

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for entirely duplicated records.
1. Check ranges of continuous variables.
1. Is there anything blank or missing?
1. Check for consistency issues.
1. Create a five-digit ZIP Code called `zip`.
1. Create a `year` field from the transaction date.
1. Make sure there is data on both parties to a transaction.

## Packages

The following packages are needed to collect, manipulate, visualize, analyze,
and communicate these results. The `pacman` package will facilitate their
installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This
package contains functions custom made to help facilitate the processing of
campaign finance data.

```{r load_packages, message=FALSE, warning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  tidyverse, # data manipulation
  lubridate, # datetime strings
  gluedown, # printing markdown
  magrittr, # pipe operators
  janitor, # clean data frames
  refinr, # cluster and merge
  readxl, # read excel files
  scales, # format strings
  knitr, # knit documents
  vroom, # read files fast
  rvest, # html scraping
  glue, # combine strings
  here, # relative paths
  httr, # http requests
  fs # local storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a
sub-directory of the more general, language-agnostic
[`irworkshop/accountability_datacleaning`][tap] GitHub repository.

The `R_campfin` project uses the [RStudio projects][rproj] feature and should be
run as such. The project also uses the dynamic `here::here()` tool for file
paths relative to _your_ machine.

```{r where_here}
# where does this document knit?
here::here()
```

[tap]: https://github.com/irworkshop/accountability_datacleaning
[rproj]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

## Data

Salary data for the state of Oklahoma was obtained via a download from the state's data portal
from the [Office of Management and Enterprise Services] (https://data.ok.gov/tags/payroll/). 

## Extract
```{r}
raw_dir <- dir_create(here("ok", "salary", "data", "raw"))
raw_zip <- path(raw_dir,"Oklahoma_salaries.zip")
```

The ZIP archive itself contains a number of subsequent ZIP archives.

```{r zip_list}
(zip_files <- raw_zip %>% 
  unzip(list = TRUE) %>% 
  as_tibble(.name_repair = tolower) %>% 
  mutate(across(length, as_fs_bytes)) %>% 
  mutate(across(name, as_fs_path)))
```

We can extract only the file containing the latest salary data. #There might not be a flag in here for overwrite 
How do we incorporate a flag to ovewrite? Overwrite argument 

```{r zip_extract}
raw_path <- unzip(
  zipfile = raw_zip, 
  files = str_subset(zip_files$name,"^State"), 
  exdir = raw_dir,
  junkpaths = TRUE,
  overwrite = TRUE
)
```

```{r}
length(raw_path) #Run this before code to see length before changes 
```

```{r}
#
file_info(raw_path) %>% 
  group_by(size) %>% 
  slice(1)
```

```{r}
file_info(raw_path) %>% # make table of file into
  group_by(size) %>% # group them by file size
  slice(1) %>% # keep only ONE of the same size
  pull(path) -> raw_path # pull the path column as vector
```

```{r}
length(raw_path) #Run this after code to see if changes worked. 
```

## Read

The excel file containing salaries is a fixed-width file (FWF) with each column
found at unique line position. We can use the record layout Word document
provided by the OSC to define the columns and their positions.


We can use this information in `readr::read_fwf()` to parse the text file as a 
data frame for exploration.

```{r raw_read}
oks <- vroom(
  file = raw_path, 
  col_names = read_names(raw_path[1]),
  skip = 1,
  id = "source_file",
  .name_repair = make_clean_names,
  col_types = cols(
   # REPORTING_PERIOD = col_date("%m/%d/%y"),
    CHECK_DATE = col_date("%m/%d/%Y")
  )
  )
```

```{r}
oks <- oks %>%
  mutate(
  across(reporting_period,
         mdy)
)

```

## Explore

Here we have the top and bottom of the data frame, it appears as though the
entire file has been properly read.

```{r glimpse}
glimpse(oks)
tail(oks)
```

### Missing

Most columns are missing some amount of values; we can flag any records missing
one of the key variables needed to identify a transaction.

```{r na_count}
#Go across every column, count the number of NA rows per column and then calculate the % of NA 
col_stats(oks, count_na)
```

```{r na_flag}
oks <- oks %>% flag_na(check_date, last_name, gross_pay, hours, agency_name, job_descr) #Creates a new column with trues and falses and puts a true on any row that's missing a variable ex. .5% of rows is missing one of those values. .5 is normal though
mean(oks$na_flag) %>% #change this to sum to count the rows vs find %
  percent(0.01)
```

```{r na_view}
oks %>% #Take all the salaries and keep only the columns where NA is true and recount the values 
  filter(na_flag) %>% 
  select(check_date, last_name, gross_pay, hours, agency_name, job_descr) %>%
  col_stats(count_na)
```

`r percent(mean(is.na(oks$job_descr[oks$na_flag])))` of these
records missing a job description, but it's not entirely contained in a single employee
type.

```{r na_type}
oks %>% #of the missing job descriptions 96% of them are the U class 
  filter(na_flag) %>% 
  count(empl_class, sort = TRUE) %>% 
  add_prop()
```

### Duplicates

There are only a handful of entirely duplicated records.

```{r dupe_flag}
oks <- flag_dupes(oks,-source_file, .check = F) #create another logical column that puts true's next to each row with duplicates 
sum(oks$dupe_flag)
```

```{r dupe_view}
dups <- oks %>% 
  filter(dupe_flag) %>% 
  select(source_file, check_date, last_name, gross_pay, hours, agency_name, job_descr) %>% 
  arrange(check_date, last_name, gross_pay) %>%
  mutate(across(source_file, basename))
```

### Categorical

```{r distinct_count}
col_stats(oks, n_distinct)
```

```{r distinct_plots, echo=FALSE}
explore_plot(oks, account_descr) + scale_x_truncate()
explore_plot(oks, job_descr) + scale_x_truncate()
```

### Amounts

A significant amount of employees have a current salary less than or equal to
$1.

```{r amount_summary}
summary(oks$gross_pay)
mean(oks$gross_pay <= 1, na.rm = TRUE)
```

```{r}
#take the data and filter 
oks %>% 
  filter(gross_pay <= 1) %>%
  count(agency_name, sort = T) %>%
  add_prop()
```


Here is the employee with the highest salary.

```{r amount_max}
oks[which.max(oks$gross_pay), ] %>% 
  mutate(across(gross_pay, dollar)) %>% 
  glimpse()
```

```{r hist_amount, echo=FALSE}
oks %>%
  ggplot(aes(gross_pay)) +
  geom_histogram(fill = dark2["purple"], bins = 30) +
  scale_y_continuous(labels = comma, trans = "log10") +
  scale_x_continuous(labels = dollar, trans = "log10") +
  labs(
    title = "Oklahoma Salary Distribution",
    caption = "Source: NC DOA OCS",
    x = "Salary",
    y = "Count"
  )
```

### Dates

Over `r percent(prop_na(oks$date))` of all hire `date` values are missing.
Again, most of these missing values belong to members of the National Guard or
temporary employees,

We can add the calendar year from `date` with `lubridate::year()`

```{r date_year}
oks <- mutate(oks, year = year(check_date))
```

```{r date_range}
min(oks$check_date, na.rm = TRUE)
sum(oks$year < 2000, na.rm = TRUE)
max(oks$check_date, na.rm = TRUE)
sum(oks$check_date > today(), na.rm = TRUE)
```

State employees have hire dates going back to `r min(oks$date, na.rm = TRUE)`.

```{r bar_year, echo=FALSE}
oks %>% 
  filter(year <= 2020, year >=1965) %>% 
  count(year) %>% 
  mutate(even = is_even(year)) %>% 
  ggplot(aes(x = year, y = n)) +
  geom_col(fill = dark2["orange"]) + 
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(breaks = seq(1945, 2020, by = 5)) +
  theme(legend.position = "bottom") +
  labs(
    title = "Oklahoma Hired Employees by Year",
    caption = "Source: NC DOA OCS",
    x = "Year Hired",
    y = "Employee Count"
  )
```

## Wrangle

Before exporting, we will add the 2-letter state abbreviation. 

```{r}
mutate(oks, state = "OK", .before = "agency_name") -> oks
```

## Conclude

1. There are `r comma(nrow(oks))` records in the database.
1. There are `r comma(sum(oks$dupe_flag))` duplicate records in the database.
1. The range and distribution of `gross_pay` and `date` seem reasonable, aside from
the $1 salaries.
1. There are `r comma(sum(oks$na_flag))` records missing key variables.
1. There are no geographic variables in need of normalization.
1. The 4-digit `year` variable has been created with `lubridate::year()`.

## Export

Now the file can be saved on disk for upload to the Accountability server.

```{r clean_dir}
clean_dir <- dir_create(here("ok", "salary", "data", "clean"))
clean_path <- path(clean_dir, "ok_salary_clean.csv")
write_csv(oks, clean_path, na = "")
file_size(clean_path) 
 
```

## Upload

We can use the `aws.s3::put_object()` to upload the text file to the IRW server.

```{r aws_upload, eval=FALSE}
aws_path <- path("csv", basename(clean_path))
if (!object_exists(aws_path, "publicaccountability")) {
  put_object(
    file = clean_path,
    object = aws_path, 
    bucket = "publicaccountability",
    acl = "public-read",
    show_progress = TRUE,
    multipart = TRUE
  )
}
aws_head <- head_object(aws_path, "publicaccountability")
(aws_size <- as_fs_bytes(attr(aws_head, "content-length")))
unname(aws_size == clean_size)
```
