---
title: "South Carolina Lobbying Registration Data Diary"
author: "Yanqi Xu"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
set.seed(5)
```

```{r create_docs_dir, eval=FALSE, echo=FALSE, include=FALSE}
fs::dir_create(here::here("state","sc", "lobby", "docs"))
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This package contains
functions custom made to help facilitate the processing of campaign finance data.

```{r load_packages, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  httr, # interact with http responses
  tidyverse, # data manipulation
  lubridate, # datetime strings
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  scales, # format strings
  knitr, # knit documents
  vroom, # read files fast
  glue, # combine strings
  here, # relative storage
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here}
# where does this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"
[03]: https://apps.sc.gov/PublicReporting/Index.aspx
[04]: https://apps.sc.gov/LobbyingActivity/LAIndex.aspx

## Data

Lobbyist data is obtained from the [South Carolina State Ethics Commission][03].

> #### Welcome
> Registrations for both lobbyists and their respective lobbyist's principals are available online
for viewing. Disclosure for both lobbyists and their respective lobbyist's principals will also be
available at the conclusion of the first disclosure period, June 30, 2009, for the period, January
1, 2009 through May 31, 2009.

The [lobbying activity page][04], we can see the files that can be retrieved:


> #### Lobbying Activity
> Welcome to the State Ethics Commission Online Public Disclosure and Accountability Reporting
System for Lobbying Activity. Registrations for both lobbyists and their respective lobbyist's
principals are available online for viewing.
> 
> Disclosure for both lobbyists and their respective lobbyist's principals are available for the
period June 30, 2009 through the present.
> 
> These filings can be accessed by searching individual reports by lobbyist and lobbyist’s
principal names and by complete list of current lobbyist and lobbyist’s principal registrations.

> #### List Reports
View a list of lobbyists, lobbyists' principals or their contact information.
> 
> * [Lobbyists and Their Principals](https://apps.sc.gov/LobbyingActivity/SelectLobbyistGroup.aspx)
> * [Download Lobbyist Contacts (CSV file)](https://apps.sc.gov/LobbyingActivity/DisplayCsv.aspx)
> * [Individual Lobbyist Lookup](https://apps.sc.gov/LobbyingActivity/SearchLobbyistContact.aspx)
> * [Lobbyists' Principals and Their Lobbyists](https://apps.sc.gov/LobbyingActivity/SelectLobbyistPrincipalGroup.aspx)
> * [Download Lobbyist's Principal Contacts (CSV file)](https://apps.sc.gov/LobbyingActivity/DisplayCsv.aspx)
> * [Individual Lobbyist's Principal Lookup](https://apps.sc.gov/LobbyingActivity/SearchLPContact.aspx)
> * [Year End Compilation Report](https://apps.sc.gov/LobbyingActivity/CompilationReport.aspx)

First, we must download a report linking lobbyists to their principals. We will download the `Lobbyists and Their Principals` table. Go to [Public Disclosure][pd] > Lobbying Activity > List of Lobbyist > Type: All Lobbyist, and then hit Continue. A csv file is available for download. We'll name it `lob_prin.csv`.

Then we can download the `Lobbyists' Principals and Their Lobbyists`
Go to Public Disclosure > Lobbying Activity > List of Lobbyists' Principals > Type: All Lobbyists' Principals, and then hit Continue. A csv file is available for download. We'll name it `prin_lob.csv`.

Both tables are downloaded on March 18, 2020. 
[pd]:https://apps.sc.gov/LobbyingActivity/LAIndex.aspx

```{r raw_dir}
raw_dir <- here("state","sc", "lobby", "data", "raw", "reg")
dir_create(raw_dir)
```

```{r}
sclr <- read_csv(dir_ls(raw_dir))
```


### Import

Using these three files, we can create a single data frame listing lobbyists and those for whom
they lobby.

```{r read_lobs}
lobs <- 
  # read as string
  read_lines(file = path(raw_dir, "lob_prin.csv")) %>%
  extract(-2) %>% 
  # fix quote enclosure
  str_replace("\"Eye\"", "'Eye'") %>%
  # pass as delim file
  read_delim(
    delim = ",",
    escape_backslash = FALSE,
    escape_double = FALSE,
    na = c("", " ")
  ) %>%
  # clean shape
  remove_empty("cols") %>% 
  clean_names("snake")

# clarify col names
names(lobs) <- names(lobs) %>% 
  str_remove("_(.*)") %>% 
  str_remove("code$") %>% 
  str_c("lob", ., sep = "_")
```

```{r read_pris}
pris <- 
  read_delim(
    file = path(raw_dir, "prin_lob.csv"),
    delim = ",",
    escape_backslash = FALSE,
    escape_double = FALSE,
    na = c("", " ")
  ) %>% 
  remove_empty("cols") %>% 
  clean_names("snake")

names(pris) <- names(pris) %>% 
  str_remove("_(.*)") %>% 
  str_remove("code$") %>%
  str_replace("^lpname$", "name") %>% 
  str_c("pri", ., sep = "_")
```

```{r join_xref}
sclr <- lobs %>% 
  left_join(pris, by = c("lob_principal" = "pri_principal",
                         "lob_lastname" = "pri_last",
                         "lob_firstname" = "pri_first",
                         "lob_middle" = "pri_middle",
                         "lob_suffix" = "pri_suffix"))
```
By examining the count of `NA` before and after the join, we can see that all lobbyist records were accounted for from the `pris` dataframe.
```{r join stats}
col_stats(lobs, count_na)
col_stats(sclr, count_na)

prop_in(
  x = str_normal(paste(lobs$lob_firstname, lobs$lob_lastname)),
  y = str_normal(paste(sclr$lob_firstname, sclr$lob_lastname)),
)
```

## Explore

### Duplicaes
We can see that there's no duplicate rows in this dataset. 
```{r}
sclr %>% flag_dupes(dplyr::everything())
```



## Wrangle
To improve the searchability of the database, we will perform some consistent,
confident string normalization. For geographic variables like city names and
ZIP codes, the corresponding `campfin::normal_*()` functions are taylor made to
facilitate this process.


### Address

For the street `addresss` variable, the `campfin::normal_address()` function
will force consistence case, remove punctuation, and abbreviation official 
USPS suffixes.

```{r address_norm}
sclr <- sclr %>% 
  mutate_at(.vars = vars(ends_with('address')), 
            .funs = list(norm = ~ normal_address(.,
,abbs = usps_street,
      na_rep = TRUE)))
```

```{r address_view}
sclr %>% 
  select(contains("address")) %>% 
  distinct() %>% 
  sample_n(10) %>% 
  glimpse()
```

### ZIP

For ZIP codes, the `campfin::normal_zip()` function will attempt to create
valied _five_ digit codes by removing the ZIP+4 suffix and returning leading
zeroes dropped by other programs like Microsoft Excel.

```{r zip_norm}
sclr <- sclr %>% 
    mutate_at(.vars = vars(ends_with('zip')), .funs = list(norm = ~ normal_zip(.,na_rep = T))) %>% 
    rename(lob_zip5 = lob_zip_norm,
           pri_zip5 = pri_zip_norm)

```

```{r zip_progress}
progress_table(
  sclr$lob_zip,
  sclr$lob_zip5,
  sclr$pri_zip,
  sclr$pri_zip5,
  compare = valid_zip
)
```

### State

By examining the percentage of lobbyist_state that are considered valid, we can see that the `state` variable in both datasets doesn't need to be normalized.

```{r state_norm}
prop_in(sclr$lob_state, valid_state, na.rm = T)
prop_in(sclr$pri_state, valid_state, na.rm = T)
```

### City

Cities are the most difficult geographic variable to normalize, simply due to
the wide variety of valid cities and formats.
#### Normal

The `campfin::normal_city()` function is a good sclrart, again converting case,
removing punctuation, but _expanding_ USPS abbreviations. We can also remove
`invalid_city` values.

```{r city_norm}
sclr <- sclr %>% 
  mutate_at(.vars = vars(ends_with('city')), .funs = list(norm = ~ normal_city(.,
      abbs = usps_city,
      states = usps_state,
      na = invalid_city,
      na_rep = TRUE)))

prop_in(sclr$lob_city_norm, valid_city, na.rm = T)
prop_in(sclr$pri_city_norm, valid_city, na.rm = T)
```

#### Swap

We can further improve normalization by comparing our normalized value
against the _expected_ value for that record's state abbreviation and ZIP code.
If the normalized value is either an abbreviation for or very similar to the
expected value, we can confidently swap those two.

```{r city_swap lobbyist}
sclr <- sclr %>% 
  left_join(
    y = zipcodes,
    by = c(
      "lob_state" = "state",
      "lob_zip5" = "zip"
    )
  ) %>% 
  rename(lob_city_match = city) %>% 
  mutate(
    match_abb = is_abbrev(lob_city_norm, lob_city_match),
    match_dist = str_dist(lob_city_norm, lob_city_match),
    lob_city_swap = if_else(
      condition = !is.na(match_dist) & match_abb | match_dist == 1,
      true = lob_city_match,
      false = lob_city_norm
    )
  ) %>% 
  select(
    -lob_city_match,
    -match_dist,
    -match_abb
  )

sclr <- sclr %>% 
  left_join(
    y = zipcodes,
    by = c(
      "pri_state" = "state",
      "pri_zip5" = "zip"
    )
  ) %>% 
  rename(pri_city_match = city) %>% 
  mutate(
    match_abb = is_abbrev(pri_city_norm, pri_city_match),
    match_dist = str_dist(pri_city_norm, pri_city_match),
    pri_city_swap = if_else(
      condition = !is.na(match_dist) & match_abb | match_dist == 1,
      true = pri_city_match,
      false = pri_city_norm
    )
  ) %>% 
  select(
    -pri_city_match,
    -match_dist,
    -match_abb
  )
```

### Manual
There are still some remaining `pri_city_swap` fields that don't match our list of known cities.

```{r check_filter}
many_city <- c(valid_city, extra_city)

sclr_out <- sclr %>% 
  filter(pri_city_swap %out% many_city) %>% 
  count(pri_city_swap, pri_state, sort = TRUE) %>% 
  drop_na()
```

```{r}
sclr <- sclr %>% 
  mutate(pri_city_swap = str_replace(pri_city_swap,"^COLUMBIA SC$", "COLUMBIA"))
```

After the two normalization steps, the percentage of valid cities is close to 100% for both datasets. 

#### Progress

```{r city_progress, echo=FALSE}
progress <- progress_table(
  sclr$lob_city,
  sclr$lob_city_norm,
  sclr$lob_city_swap,
  sclr$pri_city,
  sclr$pri_city_norm,
  sclr$pri_city_swap,
  compare = many_city
) %>% mutate(stage = as_factor(stage))
```


```{r progress_print, echo=FALSE}
kable(progress, digits = 3, caption = 'SC Lobbyists Registration City Normalization Progress')
```

You can see how the percentage of valid values increased with each stage.

```{r progress_bar, echo=FALSE}
progress %>% 
  ggplot(aes(x = stage, y = prop_in)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = dark2["purple"]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "South Carolina Lobbyists Summary Normalization Progress",
    x = "stage",
    y = "Percent Valid"
  )

```

More importantly, the number of distinct values decreased each stage. We were
able to confidently change many distinct invalid values to their valid
equivalent.


## Conclude

```{r clean_glimpse}
glimpse(sample_n(sclr, 20))
```

1. There are `r nrow(sclr)` records in the database.
1. There're `r sum(sclr$dupe_flag)` duplicate records.
1. The range and distribution of `amount` and `date` seem reasonable.
1. There are `r sum(sclr$na_flag)` records missing either address or expenditure amount.
1. Consistency in goegraphic data has been improved with `campfin::normal_*()`.
1. This dataset doesn't contain `date` columns.

## Export

```{r clean_dir}
clean_dir <- dir_create(here("state","sc", "lobby", "data", "reg","clean"))
```

```{r write_clean}
write_csv(
  x = sclr %>% 
    select(-c(lob_city_norm, pri_city_norm)) %>% 
    rename( lob_city_clean = lob_city_swap,
                       pri_city_clean = pri_city_swap),
  path = path(clean_dir, "sc_lob_reg_clean.csv"),
  na = ""
)

```
