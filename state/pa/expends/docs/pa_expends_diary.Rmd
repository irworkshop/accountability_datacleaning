---
title: "Pennsylvania Campaign Expenditures Data Diary"
author: "Yanqi Xu"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

```{r p_load, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  tidyverse, # data manipulation
  readxl, # read excel files
  lubridate, # datetime strings
  tidytext, # string analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  knitr, # knit documents
  glue, # combine strings
  scales, #format strings
  here, # relative storage
  fs, # search storage 
  vroom #read deliminated files
)
```

```{r fix_fun, echo=FALSE, collapse = TRUE}
# fix conflict
here <- here::here
# custom utility functions
print_all <- function(df) df %>% print(n = nrow(.)) 
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data


[03]: https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Pages/FullCampaignFinanceExport.aspx "source"

### About

More information about the record layout can be found here https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Documents/readme.txt.

## Import

### Download

Download raw, **immutable** data file. Go to https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Pages/FullCampaignFinanceExport.aspx. We'll download the files from 2015 to 2019 (file format: zip file) with the script.

```{r raw_dir}
# create a directory for the raw data
raw_dir <- here("pa", "expends", "data", "raw")
dir_create(raw_dir)
```

Download all the file packages containing all campaign-finance-related files. 
```{r download to raw_dir, eval = FALSE}
#download the files into the directory
pa_exp_urls <- glue("https://www.dos.pa.gov//VotingElections/CandidatesCommittees/CampaignFinance/Resources/Documents/{2015:2019}.zip")

if (!all_files_new(raw_dir)) {
  for (url in pa_exp_urls) {
    download.file(
      url = url,
      destfile = glue("{raw_dir}/{basename(url)}")
    )
  }
}
```
### Read
Read individual csv files from the downloaded zip files
```{r read_many, eval = FALSE}

zip_files <- dir_ls(raw_dir, glob = "*.zip")

if (all_files_new(path = raw_dir, glob = "*.txt")) {
  for (i in seq_along(zip_files)) {
    unzip(
      zipfile = zip_files[i],
      #Matches the csv files that starts with expense, and trim the "./ " from directory names
      files = grep("expense.+", unzip(zip_files[i]), value = TRUE) %>% substring(3,),
      exdir = raw_dir
    )
  }
}
```
Read multiple csvs into R
```{r read multiple files}
#recursive set to true because 2016 and 2015 have subdirectories under "raw"
expense_files <- list.files(raw_dir, pattern = "expense.+", recursive = TRUE, full.names = TRUE)
#pa_lines <- list.files(raw_dir, pattern = ".txt", recursive = TRUE) %>% map(read_lines) %>% unlist()
pa_col_names <- c("FILERID","EYEAR","CYCLE","EXPNAME","ADDRESS1","ADDRESS2","CITY","STATE","ZIPCODE","EXPDATE","EXPAMT","EXPDESC")

pa <- expense_files %>% 
  map(read_delim, delim = ",", escape_double = FALSE,
      escape_backslash = FALSE, col_names = pa_col_names, 
      col_types = cols(.default = col_character())) %>% 
bind_rows()
```
There are `r nrow(pa %>% filter(nchar(STATE)!=2))` parsing failures.
We'll move along the information in rows that were read incorrectly due to double quotes in the address column. 
```{r}
nudge <- which(nchar(pa$STATE) > 2)
# However, the information in the last column EXPDESC for these columns has been lost at the time of data import.
for (index in nudge) {
  for (column in c(7:11)){
    pa[[index, column]] <- pa[[index, column+1]]
  }
}
```

The text fields contain both lower-case and upper-case letters. The for loop converts them to all upper-case letters unifies the encoding to "UTF-8", replaces the "&amp;", the HTML expression of "An ampersand". These strings are invalid and cannot be converted Converting the encoding may result in some NA values. But there're not too many of them based on counts of NAs before and after the encoding conversion.

```{r fix encoding}
col_stats(pa, count_na)

pa <- pa %>% mutate_all(.funs = iconv, to = "UTF-8") %>% 
  mutate_all(.funs = str_replace,"&AMP;", "&") %>% 
  mutate_if(is.character, str_to_upper)
# After the encoding, we'll see how many entries have NA fields for each column.
col_stats(pa, count_na)
```

```{r column types}
#All the fields are converted to strings. Convert to date and double.
pa$EXPDATE <- as.Date(pa$EXPDATE, "%Y%m%d")
pa$EXPAMT <- as.double(pa$EXPAMT)
pa$ADDRESS1 <- normal_address(pa$ADDRESS1)
pa$ADDRESS2 <- normal_address(pa$ADDRESS2)
```

## Explore

There are `nrow(pa)` records of `length(pa)` variables in the full database.

```{r glimpse}
head(pa)
tail(pa)
glimpse(pa)
```

### Distinct

The variables range in their degree of distinctness.

```{r n_distinct}
pa %>% col_stats(n_distinct)
```


### Missing

The variables also vary in their degree of values that are `NA` (missing).

```{r count_na}
pa %>% col_stats(count_na)
```

We will flag any records with missing values in the key variables used to identify an expenditure.
There are `r sum(pa$na_flag)` elements that are flagged as missing at least one value.
```{r na_flag}
pa <- pa %>% flag_na(EXPNAME, EXPDATE, EXPDESC, EXPAMT, CITY)
```

### Duplicates

```{r get_dupes, collapse=TRUE}
pa <- flag_dupes(pa, dplyr::everything())
sum(pa$dupe_flag)
```

### Ranges

#### Amounts

```{r, collapse = TRUE}
summary(pa$EXPAMT)
sum(pa$EXPAMT < 0 , na.rm = TRUE)
```

See how the campaign expenditures were distributed

```{r amount distribution, eval = TRUE}
pa %>% 
  ggplot(aes(x = EXPAMT)) + 
  geom_histogram() +
  scale_x_continuous(
    trans = "log10", labels = dollar)
```

Expenditures out of state
```{r}
sum(pa$STATE != "PA", na.rm = TRUE)
```

Top spending purposes
```{r eval = TRUE, echo = FALSE}
pa %>%   drop_na(EXPDESC) %>% 
  group_by(EXPDESC) %>% 
  summarize(total_spent = sum(EXPAMT)) %>% 
  arrange(desc(total_spent)) %>% 
  head(10) %>% 
  ggplot(aes(x = EXPDESC, y = total_spent)) +
  geom_col() +
  labs(title = "Pennsylvania Campaign Expenditures by Total Spending",
       caption = "Source: Pennsylvania Dept. of State") +
  scale_y_continuous(labels = scales::dollar) +
  coord_flip() +
  theme_minimal()

```

### Dates
Some of the dates are too far back and some are past the current dates. 
```{r}
summary(pa$EXPDATE)
```

### Year

Add a `year` variable from `date` after `col_date()` using `lubridate::year()`.

```{r add_year}
pa <- pa %>% mutate(year = year(EXPDATE), on_year = is_even(year))
```
Turn some year and date values to NAs. 
```{r year_flag}
pa <- pa %>% mutate(date_flag = year < 2000 | year > format(Sys.Date(), "%Y"), 
                    date_clean = ifelse(
                    date_flag, NA, EXPDATE),
                    year_clean = ifelse(
                    date_flag, NA, year))
```


```{r year_count_bar, eval = TRUE, echo=FALSE}

pa %>% 
  filter( 2014 < year & year < 2020) %>% 
  count(on_year, year) %>% 
  ggplot(aes(x = year, y = n)) +
  geom_col(aes(fill=on_year)) +
  scale_fill_brewer(
    type = "qual",
    palette = "Dark2",
    guide = FALSE
  ) +
  labs(
    title = "Pennsylvania Expenditure Counts per Year",
    caption = "Source: Pennsylvania Dept. of State",
    x = "Year",
    y = "Count"
  )
  
```

```{r amount_year_bar, eval = TRUE, echo=FALSE}
pa %>% 
  filter( 2014 < year & year < 2020) %>% 
  group_by(year, on_year) %>% 
  summarize(mean = mean(EXPAMT)) %>% 
  ggplot(aes(x = year, y = mean)) +
  geom_col(aes(fill = on_year)) +
  scale_fill_brewer(
    type = "qual",
    palette = "Dark2",
    guide = FALSE
  ) +
  scale_y_continuous(labels = scales::dollar) +
  labs(
    title = "Pennsylvania Expenditure Mean Amount per Year",
    caption = "Source: Pennsylvania Dept. of State",
    x = "Year",
    y = "Amount"
  ) 
```

```{r amount_month_line, echo = FALSE}
pa %>% 
  mutate(month = month(EXPDATE)) %>% 
  drop_na(month) %>% 
  group_by(on_year, month) %>% 
  summarize(mean = mean(EXPAMT)) %>% 
  ggplot(aes(month, mean)) +
  geom_line(aes(color = on_year), size = 2) +
  scale_y_continuous(labels = dollar) +
  scale_x_continuous(labels = month.abb, breaks = 1:12) +
  scale_color_brewer(
    type = "qual",
    palette = "Dark2"
  ) +
  labs(
    title = "Pennsylvania Expenditure Amount by Month",
    caption = "Source: Pennsylvania Dept. of State",
    color = "Election Year",
    x = "Month",
    y = "Amount"
  )
```

```{r}
pa %>% group_by(EXPDESC) %>% summarize(total = sum(EXPAMT)) %>% arrange(desc(total))
```

## Wrangle
The state column is now pretty clean, as all non-NA columns have two characters.

### Indexing
```{r}
pa <- tibble::rowid_to_column(pa, "index")
```


### Zipcode
The Zipcode column can range from 1 to 13 columns.

```{r collapse = TRUE}
table(nchar(pa$ZIPCODE))
```


```{r normalize zip, collapse = TRUE}
pa <- pa %>% 
  mutate(
    zip_clean = ZIPCODE %>% 
      normal_zip(na_rep = TRUE))
sample(pa$zip_clean, 10)
```


### State
View values in the STATE field is not a valid state abbreviation
```{r fix state abbrev, collapse = TRUE, echo=FALSE}
{pa$STATE[pa$STATE %out% valid_state]}[!is.na(pa$STATE[pa$STATE %out% valid_state])]

pa %>% filter(STATE == "CN")
```
These are expenditures in Canada, which we can leave in. 
### City

Cleaning city values is the most complicated. This process involves four steps:

1. Prepare raw city values by removing invalid data and reducing inconsistencies
1. Match prepared city values with the _actual_ city name of that record's ZIP code
1. swap prepared city values with the ZIP code match _if_ only 1 edit is needed
1. Refine swapped city values with key collision and n-gram fingerprints

#### Prep

`r sum(!is.na(pa$CITY))` distinct cities were in the original dataset in column 
```{r prep_city, collapse = TRUE}
pa <- pa %>% mutate(city_prep = normal_city(city = CITY,
                                            abbs = usps_city,
                                            states = c(valid_state),
                                            na = invalid_city,
                                            na_rep = TRUE))
n_distinct(pa$city_prep)
```

#### Match

```{r match_dist}
pa <- pa %>%
  left_join(
    y = zipcodes,
    by = c(
      "zip_clean" = "zip",
      "STATE" = "state"
    )
  ) %>% 
  rename(city_match = city)
```

#### Swap

To replace city names with expected city names from zipcode when the two variables are no more than two characters different
```{r }
pa <- pa %>% 
  mutate(
    match_dist = stringdist(city_prep, city_match),
    city_swap = if_else(condition = is.na(city_match) == FALSE,
                        if_else(
      condition = match_dist <= 2,
      true = city_match,
      false = city_prep
    ),
      false = city_prep
  ))


summary(pa$match_dist)
sum(pa$match_dist == 1, na.rm = TRUE)
n_distinct(pa$city_swap)
```

#### Refine

```{r valid_city}
valid_city <- campfin::valid_city
```
Use the OpenRefine algorithms to cluster similar values and merge them together. This can be done using the refinr::key_collision_merge() and refinr::n_gram_merge() functions on our prepared and swapped city data.
```{r view_refine}
pa_refined <- pa %>%
  filter(match_dist != 1) %>% 
  filter(STATE =="PA") %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge(dict = valid_city) %>% 
      n_gram_merge(numgram = 2),
    refined = (city_swap != city_refine)
  ) %>% 
  filter(refined) %>% 
  select(
    index,
    FILERID, 
    CITY,
    city_prep,
    city_match,
    city_swap,
    match_dist,
    city_refine,
    STATE, 
    ZIPCODE,
    zip_clean
  )

pa_refined %>% 
  count(city_swap, city_refine) %>% 
  arrange(desc(n))
```

```{r}
refined_values <- unique(pa_refined$city_refine)
count_refined <- tibble(
  city_refine = refined_values, 
  refine_count = NA
)

for (i in seq_along(refined_values)) {
  count_refined$refine_count[i] <- sum(str_detect(pa$city_swap, refined_values[i]), na.rm = TRUE)
}

swap_values <- unique(pa_refined$city_swap)
count_swap <- tibble(
  city_swap = swap_values, 
  swap_count = NA
)

for (i in seq_along(swap_values)) {
  count_swap$swap_count[i] <- sum(str_detect(pa$city_swap, swap_values[i]), na.rm = TRUE)
}

pa_refined %>% 
  left_join(count_swap) %>% 
  left_join(count_refined) %>%
  select(
    FILERID,
    city_match,
    city_swap,
    city_refine,
    swap_count,
    refine_count
  ) %>% 
  mutate(diff_count = refine_count - swap_count) %>%
  mutate(refine_dist = stringdist(city_swap, city_refine)) %>%
  distinct() %>%
  arrange(city_refine) %>% 
  print_all()
```

Manually change the city_refine fields due to overcorrection.


```{r revert overcorrected refine}
st_pattern <- str_c("\\s",unique(zipcodes$state), "$", collapse = "|")


pa_refined$city_refine <- pa_refined$city_refine %>% 
  str_replace("^DU BOIS$", "DUBOIS") %>% 
  str_replace("^PIT$", "PITTSBURGH") %>% 
  str_replace("^MCCBG$", "MCCONNELLSBURG") %>% 
  str_replace("^PLUM BORO$", "PLUM") %>% 
  str_replace("^GREENVILLE$", "EAST GREENVILLE") %>% 
  str_replace("^NON$", "ONO") %>% 
  str_replace("^FORD CLIFF$", "CLIFFORD") %>% 
  str_replace("^W\\sB$", "WILKES BARRE") 

  

refined_table <-pa_refined %>% 
  select(index, city_refine)
```
  

#### Merge 

```{r join_refine}
pa <- pa %>% 
  left_join(refined_table, by ="index") %>% 
  mutate(city = coalesce(city_refine, city_swap)) 

pa$city <- pa$city %>% 
  str_replace("^MT PLEASANT$", "MOUNT PLEASANT") %>% 
  str_replace("^ST\\s", "SAINT ") %>% 
  str_replace("^MT\\s", "MOUNT ") %>%  
  str_replace("^FT\\s", "FORT ") %>% 
  str_replace("^W\\sB$|WB", "WILKES BARRE") %>% 
  str_replace("\\sHTS$|\\sHGTS$", " HEIGHTS") %>% 
  str_replace("\\sSQ$", " SQUARE") %>% 
  str_replace("\\sSPGS$|\\sSPR$|\\sSPRG$", "  SPRINGS") %>% 
  str_replace("\\sJCT$", " JUNCTION") %>% 
  str_replace("^E\\s", "EAST ") %>% 
  str_replace("^N\\s", "NORTH ") %>% 
  str_replace("^W\\s", "WEST ") %>% 
  str_remove(valid_state) %>% 
  str_remove("^X+$")
```

```{r after lookup}
pa_match_table <- pa %>% 
  filter(str_sub(pa$city, 1,1) == str_sub(pa$city_match, 1,1)) %>% 
  filter(city %out% valid_city)  %>% 
  mutate(string_dis = stringdist(city, city_match)) %>% 
  select (index, zip_clean, STATE, city, city_match, string_dis) %>% 
  distinct() %>% 
  add_count(city_match) %>% 
  rename("sec_city_match" = "city_match")

# Manually change overcorrected city names to original 
pa_match_table$sec_city_match <- pa_match_table$sec_city_match %>% 
  str_replace("^ARLINGTON$", "ALEXANDRIA") %>% 
  str_replace("^BROWNSVILLE$", "BENTLEYVILLE") %>% 
  str_replace("^FEASTERVILLE\\sTREVOSE", "FEASTERVILLE") %>% 
  str_replace("LEES SUMMIT", "LAKE LOTAWANA") %>% 
  str_replace("HAZLETON", "HAZLE TOWNSHIP") %>% 
  str_replace("DANIA", "DANIA BEACH") %>% 
  str_replace("CRANBERRY TWP", "CRANBERRY TOWNSHIP")

pa_match_table[pa_match_table$city == "HOLLIDASBURG", "city_match"] <- "HOLLIDAYSBURG"
pa_match_table[pa_match_table$city == "PENN HELLE", "city_match"] <- "PENN HILLS"
pa_match_table[pa_match_table$city == "PHUM", "city_match"] <- "PLUM"
pa_match_table[pa_match_table$city == "CLARKSGREEN", "city_match"] <- "CLARKS GREEN"
pa_match_table[pa_match_table$city == "SANFRANCISCO", "city_match"] <- "SAN FRANCISCO"
pa_match_table[pa_match_table$city == "RIEFFTON", "city_match"] <- "REIFFTON"
pa_match_table[pa_match_table$city == "SHOREVILLE", "city_match"] <- "SHOREVIEW"
pa_match_table[pa_match_table$city == "PITTSBURGH PLUM", "city_match"] <- "PLUM"
pa_match_table[pa_match_table$city == "MOUNTVIEW", "city_match"] <- "MOUNT VIEW"
pa_match_table[pa_match_table$city == "PLUM BORO", "city_match"] <- "PLUM"
pa_match_table[pa_match_table$city == "HAZELTON CITY", "city_match"] <- "HAZLE TOWNSHIP"
pa_match_table[pa_match_table$city == "BARNSVILLE", "city_match"] <- "BARNESVILLE"

keep_original <- c( "SHOREVIEW" ,
   "CUYAHOGA" ,
   "MEDFORD LAKES" ,
   "WEST GOSHEN" ,
   "CLEVELAND HEIGHTS" ,
   "LAHORNE" ,
   "ROCHESTER HILLS" ,
   "PENLLYN" ,
   "SOUTHERN" ,
   "WEST DEPTFORD" ,
   "SEVEN FIELDS" ,
   "LORDS VALLEY" ,
   "WILDWOOD CREST" ,
   "BETHLEHEM TOWNSHIP" ,
   "MOON TOWNSHIP" ,
   "BELFONTE" ,
   "NEWPORT TOWNSHIP" , 
   "LINTHICUM" , 
   "WARRIOR RUN" ,
   "PRIMOS SECANE" ,
   "COOKPORT" , 
   "MANASSAS PARK" ,
   "MCMURRAY" ,
   "MOYLAN" ,
   "BELMONT HILLS" ,
   "THORNBURY" ,
   "HANOVER TOWNSHIP" ,
   "MIAMI SPRINGS" ,
   "BROOKLYN PARK" )

pa_match_table[pa_match_table$city %in% keep_original,"sec_city_match"] <-
  pa_match_table[pa_match_table$city %in% keep_original,"city"]


pa <-pa %>% 
  left_join(select(pa_match_table, index, sec_city_match), by = "index") %>% mutate(city_clean = coalesce(sec_city_match, city))

pa$city_clean <- pa$city_clean %>% str_replace("\\sTWP$", " TOWNSHIP")

n_distinct(pa$city_clean[pa$city_clean %out% valid_city])
```

```{r}
valid_city <- unique(c(valid_city, extra_city))

pa_city_lookup <- read_csv(file = here("pa", "expends", "data", "raw", "pa_city_lookup.csv"), col_names = c("city", "city_lookup", "changed", "count"), skip = 1)

pa_out <- pa %>% 
  count( city_clean, sort = TRUE) %>% 
  filter(city_clean %out% valid_city) 

pa_out <- pa_out %>% left_join(pa_city_lookup, by = c("city_clean" = "city")) %>% filter(city_clean != city_lookup | is.na(city_lookup)) %>% drop_na(city_clean)

pa <- pa %>% left_join(pa_out, by = "city_clean") %>% mutate(city_after_lookup = ifelse(
  is.na(city_lookup) & city_clean %out% valid_city,
  NA,
  coalesce(city_lookup, city_clean))) 

pa$city_after_lookup <- pa$city_after_lookup %>% str_replace("^\\sTWP$", " TOWNSHIP")

pa[pa$index == which(pa$city_after_lookup == "MA"),8:9] <- c("LEXINGTON", "MA")
pa[pa$index == which(pa$city_after_lookup == "L"),8] <- c("LOS ANGELES")
pa[pa$index %in% which(pa$city_after_lookup %in% c("PA", "NJ")), 8] <- ""
pa[pa$index == "319505", 8] <- "HARRISBURG"

#pa_final_lookup <- read_excel(glue("{raw_dir}/pa_final_fixes.xlsx"))
pa_final_lookup <- read_excel(glue("{raw_dir}/pa_final_fixes.xlsx"), col_types = "text")


pa <- pa %>% left_join(pa_final_lookup, by = c("city_after_lookup" = "city_clean")) %>% 
mutate(city_output = if_else(condition = is.na(fix) & city_after_lookup %out% valid_city, NA_character_,
                               coalesce(fix,city_after_lookup)))
```



Each process also increases the percent of valid city names.

```{r city_progress, collapse=TRUE}
prop_in(pa$CITY, valid_city, na.rm = TRUE)
prop_in(pa$city_prep, valid_city, na.rm = TRUE)
prop_in(pa$city_swap, valid_city, na.rm = TRUE)
prop_in(pa$city, valid_city, na.rm = TRUE)
prop_in(pa$city_clean, valid_city, na.rm = TRUE)
prop_in(pa$city_after_lookup, valid_city, na.rm = TRUE)
prop_in(pa$city_output, valid_city, na.rm = TRUE)

progress_table <- tibble(
  stage = c("raw", "norm", "swap", "refine","second match", "lookup", "final lookup"),
  prop_good = c(
    prop_in(str_to_upper(pa$CITY), valid_city, na.rm = TRUE),
    prop_in(pa$city_prep, valid_city, na.rm = TRUE),
    prop_in(pa$city_swap, valid_city, na.rm = TRUE),
    prop_in(pa$city, valid_city, na.rm = TRUE),
    prop_in(pa$city_clean, valid_city, na.rm = TRUE),
    prop_in(pa$city_after_lookup, valid_city, na.rm = TRUE),
    prop_in(pa$city_output, valid_city, na.rm = TRUE)
  ),
  total_distinct = c(
    n_distinct(str_to_upper(pa$CITY)),
    n_distinct(pa$city_prep),
    n_distinct(pa$city_swap),
    n_distinct(pa$city),
    n_distinct(pa$city_clean),
    n_distinct(pa$city_after_lookup),
    n_distinct(pa$city_output)
  ),
  unique_bad = c(
    length(setdiff(str_to_upper(pa$CITY), valid_city)),
    length(setdiff(pa$city_prep, valid_city)),
    length(setdiff(pa$city_swap, valid_city)),
    length(setdiff(pa$city, valid_city)),
    length(setdiff(pa$city_clean, valid_city)),
    length(setdiff(pa$city_after_lookup, valid_city)),
    length(setdiff(pa$city_output, valid_city))
  )
)

diff_change <- progress_table$unique_bad[1]-progress_table$unique_bad[4]
prop_change <- diff_change/progress_table$unique_bad[1]
```

Each step of the cleaning process reduces the number of distinct city values.
There are `r sum(!is.na(pa$CITY))` with `r n_distinct(pa$CITY)` distinct values, after the swap and refine processes, there are `r sum(!is.na(pa$city_output))` entries with `r n_distinct(pa$city_output)` distinct values. 

```{r print_progress, echo=FALSE}
kable(
  x = progress_table,
  format = "markdown", 
  digits = 4,
  col.names = c("Normalization Stage", "Percent Valid", "Total Distinct", "Unique Invalid")
)
```

```{r wrangle_bar_prop, echo=FALSE}
progress_table %>% 
  mutate(stage = as_factor(stage)) %>% 
  ggplot(aes(x = stage, y = prop_good)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = "#D95F02") +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Pennsylvania Expenditures Payee City Progress",
    subtitle = "Percent of total values contained in pre-defined list of cities",
    caption = "Source: Pennsylvania Dept. of State",
    x = "Wrangling Stage",
    y = "Proportion Valid Cities"
  )
```

```{r clean address, echo = TRUE, execute = FALSE}
pa <- pa %>%   
  unite(
    ADDRESS1, ADDRESS2,
    col = address_clean,
    sep = " ",
    remove = FALSE,
    na.rm = TRUE
  ) %>% 
  mutate(address_clean = normal_address(
      address = address_clean,
      abbs = usps_city,
      na_rep = TRUE
    ))
```

We also need to pull up the processed filer table to join back to the `FILERID`, `EYEAR` and `CYCLE` field. 

```{r read filer table}
clean_dir <- here("pa", "expends", "data", "processed")
pa_filer <- read_csv(glue("{clean_dir}/pa_filers_clean.csv"), 
                     col_types = cols(.default = col_character())) %>% 
  rename_at(vars(contains("clean")), list(~str_c("filer_",.)))

pa_filer <- pa_filer %>% flag_dupes(FILERID,EYEAR,CYCLE)

pa_filer <-  pa_filer %>% 
  filter(!dupe_flag) %>% 
  select(
    FILERID,
    EYEAR,
    CYCLE,
    FILERNAME,
    FILERTYPE,
    filer_state,
    ends_with("clean")
  )
```

```{r join with filer}
pa <- pa %>% left_join(pa_filer, by = c("FILERID", "EYEAR", "CYCLE"))
```


## Conclude

1. There are `r nrow(pa)` records in the database
1. There are `r sum(pa$dupe_flag)` records with suspected duplicate filerID, recipient, date, _and_ amount
(flagged with `dupe_flag`)
1. The ranges for dates and amounts are reasonable
1. Consistency has been improved with `stringr` package and custom `normal_*()` functions.
1. The five-digit `zip_clean` variable has been created with `zipcode::clean.zipcode()`
1. The `year` variable has been created with `lubridate::year()`
1. There are `r count_na(pa$CITY)` records with missing `city` values and `r count_na(pa$EXPNAME)` records with missing `payee` values (both flagged with the `na_flag`).

## Export

```{r write_clean}
clean_dir <- here("pa", "expends", "data", "processed")
dir_create(clean_dir)
pa %>% 
  select(
    -index,
    -city_prep,
    -on_year,
    -city_match,
    -match_dist,
    -city_swap,
    -city_refine,
    -city_clean,
    -city_lookup,
    -city_after_lookup,
    -sec_city_match,
    -n,
    -fix,
    -changed,
    -city,
    -count
  ) %>% 
    rename (zip5 = zip_clean,
          filer_zip5 = filer_zip_clean,
          city_clean = city_output) %>% 
  write_csv(
    path = glue("{clean_dir}/pa_expends_clean.csv"),
    na = ""
  )
```

