---
title: "Alabama Lobbyists"
author: "First Last"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
set.seed(5)
```

```{r create_docs_dir, eval=FALSE, echo=FALSE, include=FALSE}
fs::dir_create(here::here("al", "lobby", "docs"))
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This package contains
functions custom made to help facilitate the processing of campaign finance data.

```{r load_packages, message=FALSE, warning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  tidyverse, # data manipulation
  lubridate, # datetime strings
  magrittr, # pipe opperators
  pdftools, # process pdf text
  janitor, # dataframe clean
  refinr, # cluster and merge
  scales, # format strings
  knitr, # knit documents
  vroom, # read files fast
  glue, # combine strings
  here, # relative storage
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here}
# where does this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

Data is obtained from the [Alabama Ethics Commission (AEC)][03].

[03]: http://ethics.alabama.gov/

> The Alabama Ethics Commission was created by the Alabama Legislature in 1973
by Act No. 1056. The mission of this Commission is to ensure that public
officials are independent and impartial; that decisions and policies are made in
the proper governmental channels; that public office is not used for private
gain; and, most importantly, that there is public confidence in the integrity of
government.

On the [lobbyist section][04] of the AEC website, the requirments for lobbyist
registration are given.

>  Registration as a Lobbyist is now required if your duties include promoting
or attempting to influence the awarding of a grant or contract with any
department or agency of the Executive, Legislative or Judicial Branch of state
government.

[04]: http://ethics.alabama.gov/lobbyists.aspx

Per [Section 36-25-1(20)][05]:

> Lobby or Lobbying is: â€œThe practice of promoting, opposing, or in any manner
influencing or attempting to influence the introduction, defeat, or enactment of
legislation before any legislative body; opposing or in any manner influencing
the executive approval, veto, or amendment of legislation; or the practice of
promoting, opposing, or in any manner influencing or attempting to influence the
enactment, promulgation, modification, or deletion of regulations before any
regulatory body. The term does not include providing public testimony before a
legislative body or regulatory body or any committee thereof."

[05]: http://ethics.alabama.gov/docs/WhatisLobbyingREVISEDDec2012.pdf

## Import

While the AEC _does_ provide two Excel files listing [registered lobbyists][06] 
and [registered principal clients][07] for 2020, these two files do not show
the relationship between each lobbyist and those entites for which they lobby.

Instead, that relationship is documents on annual filings for each individual
lobbyist. These reports are given as PDF documents and can be searched from the
[AEC search page][08].

The PDF statements can be then be viewed one at a time. Each yearly PDF has a 
unique lobbyist ID (`lid`), which can be passed to an `httr::GET()` request to 
save the PDF.

```{r get, eval=FALSE}
GET(
  url = "http://ethics.alabama.gov/search/ViewReports.aspx",
  write_disk(path, overwrite = TRUE),
  query = list(
    lid = 21,
    rpt = "rptLobbyistRegistration"
  )
)
```

[06]: https://ethics-form.alabama.gov/entity/FileUpload2015/RegisteredLobbyist/WebDataForExcel_2010.aspx
[07]: https://ethics-form.alabama.gov/entity/FileUpload2015/RegisteredLobbyist/rptPrincipalsListing_Excel.aspx
[08]: http://ethics.alabama.gov/search/PublicEmployeeSearch.aspx
### Download

Opening random PDF's from 2008 to 2020, it seems as though their are valid 
lobbyist ID's from 1 to 11,000 (with roughly 25% inbetween leading to "empty"
files without any information).

This takes **hours**, but we can loop through each ID and write the file to
disk.

```{r raw_dir, eval=TRUE}
raw_dir <- dir_create(here("al", "lobby", "data", "raw"))
```

```{r download, eval=FALSE}
n <- 11100
start_time <- Sys.time()
if (length(dir_ls(raw_dir)) < 5000) {
  for (i in seq(min, n)) {
    path <- glue("{raw_dir}/reg_{str_pad(i, nchar(n), pad = '0')}.pdf")
    loop_start <- Sys.time()
    # make get request
    GET(
      url = "http://ethics.alabama.gov/search/ViewReports.aspx",
      write_disk(path, overwrite = TRUE),
      query = list(
        lid = i,
        rpt = "rptLobbyistRegistration"
      )
    )
    # delete if empty pdf
    if (file_size(path) == 55714) {
      file_delete(path)
      deleted <- TRUE
    } else {
      deleted <- FALSE
    }
    # track progress
    loop_time <- Sys.time() - loop_start
    loop_time <- paste(round(loop_time, 2), attributes(loop_time)$units)
    total_time <- Sys.time() - start_time
    total_time <- paste(round(total_time, 2), attributes(total_time)$units)
    message(glue(
      "{i} done in {str_pad(loop_time, 2)}",
      "running for {str_pad(total_time, 2)}",
      "({percent(i/n)})",
      deleted,
      .sep = " / "
    ))
    # rand sleep
    Sys.sleep(time = runif(n = 1, min = 0, max = 3))
  }
}
```

### Read

Once we have downloaded all 7,500 PDF files to the same directory, we can
write some generic functions that use the `pdftools::pdf_text()` and
`stringr::str_extract()` functions to scan the embeded text of each page and
extract the bits of information we want.

The overall technic is to create 1 data rame row with lobbyist information per
document and an individual row per principal client. We can then combine those
two data frames to produce a single row per lobbyist-principal relationship.

This `str_get()` function is just a simple way to look for the line containing
the information we want (e.g., Lobbyist Name) and extract the relevant text
from that line using regular expressions. Each page has the exact same layout,
so we can then use this function to get each bit of text from every page.

```{r str_get}
# extract first from which contains
str_get <- function(string, pattern, n = 1) {
  got <- str_trim(str_extract(str_subset(string, pattern), pattern)[[n]])
  if (nchar(got) == 0) {
    got <- NA_character_
  }
  return(got)
}
```

This `frame_lob()` function uses `str_get()` to locate each piece of information
and turn it into the column of a single row tibble.

```{r frame_lob}
frame_lob <- function(x) {
    # find email line index
    which_email <- str_which(x, "E-Mail")
    # check for no address after email
    if (str_detect(x[which_email + 1], "Address", negate = TRUE)) {
      # collapse two lines
      x[which_email] <- str_c(x[which_email], x[which_email + 1], collapse = "")
      # remove overflow line
      x <- x[-(which_email + 1)]
    }
    # extract content from lines of text
    tibble(
      lob_year = as.integer(str_get(x, "(?<=Year:)(.*)")),
      lob_date = mdy(str_get(x[str_which(x, "I certify that") + 1], "(?<=Date:)(.*)")),
      lob_name = str_get(x, "(?<=Lobbyist:)(.*)(?=Business Phone:)"),
      lob_phone = str_get(x, "(?<=Business Phone:)(.*)"),
      lob_addr1 = str_get(x, "(?<=Business)(.*)(?=E-Mail)"),
      lob_addr2 = str_get(x, "(?<=Address:)(.*)"),
      lob_city = str_get(x, "(?<=City/State/Zip:)(.*)"),
      lob_public = str_get(x, "(?<=Public Employee\\?)(.*)"),
      # combine all lines between these
      lob_subjects = str_c(x[seq(
        str_which(x, "Categories of legislation") + 1,
        str_which(x, "List Business Entities") - 1
      )], collapse = " "
      )
    )
  }
```

This `frame_pri()` function does a similar thing for each principal section of
the document.

```{r frame_pri}
# extract content from lines of text
frame_pri <- function(section) {
    a <- section$text
    tibble(
      pri_name = str_get(a, "(?<=Principal Name:\\s)(.*)(?=\\sPhone)"),
      pri_phone = str_get(a, "(?<=Phone:)(.*)"),
      pri_addr = str_get(a, "(?<=Address:)(.*)"),
      pri_start = mdy(str_get(a, "(?<=Effective Date:)(.*)(?=\\s)")),
      pri_end_date = mdy(str_get(a, "(?<=Termination Date:)(.*)")),
      pri_sign = str_get(a, "(?<=Principal:)(.*)"),
      pri_behalf = a[str_which(a, "If your activity") + 1]
    )
  }
```

The final `frame_pdf()` function reads the PDF document and appropriately 
formats the text before calling `frame_lob()` and `frame_pri()` to return a
single combined data frame.

```{r frame_pdf}
frame_pdf <- function(file) {
  id <- str_extract(file, "\\d+")

  # read text of single file
  text <-
    # read lines of text
    pdf_text(pdf = file) %>%
    # concat pages of text
    str_c(collapse = "\n") %>%
    # split by newline
    str_split(pattern = "\n") %>%
    pluck(1) %>%
    # reduce whitespace
    str_squish() %>%
    # remove header, footer, empty
    str_subset("^Page \\d+ of \\d+$", negate = TRUE) %>%
    str_subset("^\\d{1,2}/\\d{1,2}/\\d{4}$", negate = TRUE) %>%
    str_subset("^$", negate = TRUE)
  
  lob <-
    frame_lob(x = text) %>%
    mutate(id) %>%
    select(id, everything())

  # keep only pri lines
  pri <- text[seq(
    str_which(text, "List Business Entities") + 1,
    str_which(text, "I certify that") - 1
  )]

  pri <- pri %>%
    enframe(name = "line", value = "text") %>%
    # count pri section
    mutate(section = cumsum(str_detect(text, "Principal Name:"))) %>%
    # split into list
    group_split(section)
  
  # frame every section
  pri <- map_df(pri, frame_pri)

  # rep lob by col bind
  as_tibble(cbind(lob, pri))
}
```

We can then apply this function to every PDF downloaded and combine the results
of each into a single giant data frame.

```{r}
allr <- map_df(
  .x = dir_ls(raw_dir),
  .f = frame_pdf
)
```

## Explore

```{r glimpse}
head(allr)
tail(allr)
glimpse(sample_frac(allr))
```

```{r glimpse_na}
col_stats(allr, count_na)
```

```{r glimpse_distinct}
col_stats(allr, n_distinct)
```

```{r plot_year}
ggplot(data = allr) +
  geom_bar(mapping = aes(x = lob_year))
```

## Wrangle

Now we can separate some of the lobbyist information into distinct columns.

```{r sep_lob}
allr <- allr %>%
  mutate_all(str_to_upper) %>%
  separate(
    col = lob_name,
    into = c("lob_last", "lob_first"),
    sep = ",\\s",
    extra = "merge",
    fill = "right"
  ) %>%
  separate(
    col = lob_city,
    into = c("lob_city", "lob_state"),
    sep = ",\\s(?=[:upper:])",
    extra = "merge"
  ) %>%
  mutate_at(
    .vars = vars(lob_state),
    .funs = str_remove,
    pattern = "(.*,\\s)(?=[:upper:])"
  ) %>%
  separate(
    col = lob_state,
    into = c("lob_state", "lob_zip"),
    sep = "\\s(?=\\d+)"
  )
```

```{r echo=FALSE}
allr %>% 
  select(starts_with("lob")) %>% 
  distinct() %>% 
  sample_frac()
```

And we can do the same for principal clients.

```{r sep_pri}
allr <- allr %>%
  separate(
    col = pri_addr,
    into = c(
      glue("pri_addr{1:10}"),
      "pri_city",
      "pri_state"
    ),
    sep = ",\\s+",
    extra = "merge",
    fill = "left"
  ) %>%
  unite(
    starts_with("pri_addr"),
    col = pri_addr,
    sep = ", ",
    na.rm = TRUE
  ) %>%
  separate(
    col = pri_state,
    into = c("pri_state", "pri_zip"),
    sep = "\\s(?=\\d+)",
    extra = "merge",
    fill = "right"
  ) %>%
  mutate_if(
    .predicate = is_character,
    .funs = str_trim
  ) %>%
  na_if("")
```

```{r echo=FALSE}
allr %>% 
  select(starts_with("pri")) %>% 
  distinct() %>% 
  sample_frac()
```

## Normalize

### Address

```{r lob_address_norm}
allr <- allr %>% 
  # combine street addr
  unite(
    starts_with("lob_addr"),
    col = lob_addr_full,
    sep = " ",
    remove = FALSE,
    na.rm = TRUE
  ) %>% 
  # normalize combined addr
  mutate(
    lob_addr_norm = normal_address(
      address = lob_addr_full,
      abbs = usps_street,
      na_rep = TRUE
    )
  ) %>% 
  select(-lob_addr_full)
```

```{r pri_address_norm}
allr <- allr %>% 
  mutate(
    pri_addr_norm = normal_address(
      address = pri_addr,
      abbs = usps_street,
      na_rep = TRUE
    )
  )
```

```{r address_view}
allr %>% 
  select(contains("lob_addr")) %>% 
  distinct() %>% 
  sample_frac()
```

### ZIP

```{r zip_norm}
allr <- mutate_at(
  .tbl = allr,
  .vars = vars(ends_with("_zip")),
  .funs = list(norm = normal_zip),
  na_rep = TRUE
)
```

```{r zip_progress}
progress_table(
  allr$lob_zip,
  allr$lob_zip_norm,
  allr$pri_zip,
  allr$pri_zip_norm,
  compare = valid_zip
)
```

### State

```{r state_norm}
allr <- mutate_at(
  .tbl = allr,
  .vars = vars(ends_with("_state")),
  .funs = list(norm = normal_state),
  na_rep = TRUE
)
```

```{r state_progress}
progress_table(
  allr$lob_state,
  allr$lob_state_norm,
  allr$pri_state,
  allr$pri_state_norm,
  compare = valid_state
)
```

### City

```{r city_norm}
allr <- allr %>% 
  mutate_at(
    .vars = vars(ends_with("_city")),
    .funs = list(norm = normal_city),
    abbs = usps_city,
    states = c("AL", "ALA", "ALABAMA", "DC"),
    na = invalid_city,
    na_rep = TRUE
  )
```

```{r lob_city_swap}
allr <- allr %>% 
  left_join(
    y = zipcodes,
    by = c(
      "lob_state_norm" = "state",
      "lob_zip_norm" = "zip"
    )
  ) %>% 
  rename(lob_city_match = city) %>% 
  mutate(
    lob_match_abb = is_abbrev(lob_city_norm, lob_city_match),
    lob_match_dist = str_dist(lob_city_norm, lob_city_match),
    lob_city_swap = if_else(
      condition = lob_match_abb | lob_match_dist == 1,
      true = lob_city_match,
      false = lob_city_norm
    )
  ) %>% 
  select(
    -lob_city_match,
    -lob_match_abb,
    -lob_match_dist
  )
```

```{r pri_city_swap}
allr <- allr %>% 
  left_join(
    y = zipcodes,
    by = c(
      "pri_state_norm" = "state",
      "pri_zip_norm" = "zip"
    )
  ) %>% 
  rename(pri_city_match = city) %>% 
  mutate(
    pri_match_abb = is_abbrev(pri_city_norm, pri_city_match),
    pri_match_dist = str_dist(pri_city_norm, pri_city_match),
    pri_city_swap = if_else(
      condition = pri_match_abb | pri_match_dist == 1,
      true = pri_city_match,
      false = pri_city_norm
    )
  ) %>% 
  select(
    -pri_city_match,
    -pri_match_abb,
    -pri_match_dist
  )
```

```{r city_progress}
progress_table(
  allr$lob_city,
  allr$lob_city_norm,
  allr$lob_city_swap,
  allr$pri_city,
  allr$pri_city_norm,
  allr$pri_city_swap,
  compare = valid_city
)
```

## Export

```{r create_proc_dir}
proc_dir <- dir_create(here("al", "lobby", "data", "processed"))
```

```{r write_clean}
allr %>% 
  select(
    -lob_city_norm,
    -pri_city_norm
  ) %>% 
  rename(
    lob_city_norm = lob_city_swap,
    pri_city_norm = pri_city_swap,
  ) %>% 
  write_csv(
    path = glue("{proc_dir}/al_lobbyists.csv"),
    na = ""
  )
```

