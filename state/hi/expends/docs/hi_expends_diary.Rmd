---
title: "Hawaii Expenditures"
author: "Kiernan Nicholls"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

```{r load_packages, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  tidyverse, # data manipulation
  lubridate, # datetime strings
  magrittr, # pipe operators
  gluedown, # print markdown
  janitor, # dataframe clean
  refinr, # cluster and merge
  aws.s3, # aws cloud storage
  scales, # format strings
  rvest, # read html pages
  RSocrata, # read SODA API
  knitr, # knit documents
  vroom, # read files fast
  glue, # combine strings
  here, # relative storage
  fs # search storage 
)
```

The IRW's `campfin` package will also have to be installed from GitHub. This package contains
functions custom made to help facilitate the processing of campaign finance data.

```{r load_campfin}
pacman::p_load_gh("irworkshop/campfin")
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

Data is provided by the Hawaii Campaign Spending Committee's Socrata open data portal. From the
[Primer metadata page][03], we can find more informat. The Records Reporting System RSN is 38832.
The file was created on February 26, 2013 and last updated May 15, 2019.

[03]: https://data.hawaii.gov/Community/Expenditures-Made-By-Hawaii-State-and-County-Candi/3maa-4fgr

## Import

The data can be directly read using `RSocrata::read.socrata()`.

```{r raw dir}
raw_dir <- dir_create(here("state","hi", "expends", "data", "raw"))
prev_dir <- dir_create(here("state","hi", "expends", "data", "previous"))
```


```{r download, eval=FALSE}
#hi <- as_tibble(read.socrata("https://data.hawaii.gov/resource/smzs-eax2.json"))
raw_url <- "https://opendata.hawaii.gov/dataset/1dfb86f4-629c-426f-abef-3029889aa0f7/resource/cbc222c7-42dd-47cf-bea3-1c2dda29acd4/download/ccschb.csv"
raw_path <- path(raw_dir,basename(raw_url))

if (this_file_new(raw_path)) {
  download.file(raw_url, raw_path, method = "libcurl")
}
```

Then we can make some general changes to the structure and format of the data frame.

```{r read}
hie <- read_csv(raw_path) %>% clean_names()
hi_prev <- read_csv(dir_ls(here("state","hi","expends","data","previous")))
hi_prev <- hi_prev %>% select(names(hie))

%>%
  mutate_if(
    is_character, 
    str_to_upper
  ) %>% 
  separate(
    col = location_1.coordinates,
    into = c("lat", "lon"),
    sep = ",\\s"
  ) %>% 
  mutate(
    amount = as.double(amount),
    lat = as.double(str_remove(lat, "c\\(")),
    lon = as.double(str_remove(lon, "\\)")),
    in_state = equals(inoutstate, "HI")
  ) %>% 
  select(
    -starts_with("location_1")
  )
```

## Explore

```{r glimpse}
head(hi)
tail(hi)
glimpse(hi)
```

### Distinct

```{r n_distinct}
glimpse_fun(hi, n_distinct)
```

We can use `campfin::explore_plot()` and/or `ggplot2::geom_bar()` to explore the distribution of
distinct categorical variables.

```{r vendor_bar, echo=FALSE}
explore_plot(
  data = hi,
  var = vendor_type,
  title = "HI Expend Vendor Types",
  caption = "Source: HI CRS"
)
```

```{r category_bar, echo=FALSE, fig.height=10}
hi %>%
  count(expenditure_category, sort = TRUE) %>%
  mutate(p = n/sum(n)) %>%
  ggplot(aes(reorder(expenditure_category, desc(p)), p)) +
  geom_col(aes(fill = expenditure_category)) +
  scale_fill_discrete(guide = FALSE) +
  scale_y_continuous(labels = percent) +
  coord_flip() +
  labs(
    title = "HI Expend Types",
    caption = "Source: HI CRS",
    x = "Expenditure Type",
    y = "Percent"
  )
```

```{r office_bar, echo=FALSE}
hi %>%
  count(office, sort = TRUE) %>%
  mutate(p = n/sum(n)) %>%
  ggplot(aes(reorder(office, desc(p)), p)) +
  geom_col(aes(fill = office)) +
  scale_fill_discrete(guide = FALSE) +
  scale_y_continuous(labels = percent) +
  coord_flip() +
  labs(
    title = "HI Expend Office",
    caption = "Source: HI CRS",
    x = "Office Sought",
    y = "Percent"
  )
```

```{r party_bar, echo=FALSE}
hi %>%
  count(party, sort = TRUE) %>%
  mutate(p = n/sum(n)) %>%
  ggplot(aes(reorder(party, desc(p)), p)) +
  geom_col(aes(fill = party)) +
  scale_fill_manual(
    guide = FALSE,
    values = c(
      "dodgerblue",
      "grey10",
      "red2",
      "mediumpurple",
      "forestgreen",
      "orange"
    )
  ) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "HI Expend Party",
    caption = "Source: HI CRS",
    x = "Political Party",
    y = "Percent"
  )
```

```{r state_bar, echo=FALSE}
explore_plot(
  data = hi,
  var = in_state,
  title = "HI Expends In State",
  caption = "Source: HI CRS"
)
```

```{r county_bar, echo=FALSE}
explore_plot(
  data = filter(hi, !is.na(county)),
  var = county,
  title = "HI Expends County",
  caption = "Source: HI CRS"
)
```

```{r authorized_bar, echo=FALSE}
explore_plot(
  data = filter(hi, !is.na(authorized_use)),
  var = authorized_use,
  flip = TRUE,
  title = "HI Expends Use",
  caption = "Source: HI CRS"
) 
```

```{r words_bar, echo=FALSE, fig.height=10}
hi %>% 
  unnest_tokens(word, purpose_of_expenditure) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE) %>% 
  head(30) %>% 
  ggplot(
    mapping = aes(
      x = reorder(word, n),
      y = n
    )
  ) +
  geom_col() +
  coord_flip() +
  labs(
    title = "HI Expend Purpose",
    caption = "Source: HI CRS",
    x = "Word",
    y = "Percent"
  )
```

### Duplicates

We can use `janitor::get_dupes()` to create a separate table of duplicate rows, then flag those
rows on the original data frame.

```{r get_dupes, collapse=TRUE}
hi_dupes <- distinct(get_dupes(hi))

nrow(hi_dupes)
sum(hi_dupes$dupe_count)
n_distinct(hi_dupes$candidate_name)
```

```{r join_dupes, collapse=TRUE}
hi <- hi %>% 
  left_join(hi_dupes) %>% 
  mutate(dupe_flag = !is.na(dupe_count)) %>% 
  select(-dupe_count)

rm(hi_dupes)
sum(hi$dupe_flag)
```

### Missing

There are relatively few missing values. There are no missing values that need to be flagged.

```{r count_na}
glimpse_fun(hi, count_na)
```

### Ranges

#### Amount

```{r summary_amount, collapse=TRUE}
summary(hi$amount)
sum(hi$amount < 0)
```

```{r amount_hist, echo=FALSE}
hi %>% 
  ggplot(aes(amount)) +
  geom_histogram() +
  scale_x_continuous(
    trans = "log10",
    labels = dollar
  ) +
  labs(
    title = "HI Expend Amount",
    caption = "Source: HI CRS",
    x = "Amount",
    y = "Count"
  )
```

```{r amount_hist_party, echo=FALSE}
hi %>% 
  filter(party %out% c("GREEN", "LIBERTARIAN")) %>% 
  ggplot(aes(amount)) +
  geom_histogram(aes(fill = party)) +
  scale_x_continuous(
    trans = "log10",
    labels = dollar
  ) +
  scale_fill_manual(
    guide = FALSE,
    values = c(
      "dodgerblue",
      "mediumpurple",
      "grey10",
      "red2"
    )
  ) +
  labs(
    title = "HI Expend Amount",
    caption = "Source: HI CRS",
    x = "Amount",
    y = "Count"
  ) +
  facet_wrap(~party)
```

```{r amount_box_party, echo=FALSE}
hi %>% 
  filter(party %out% c("GREEN", "LIBERTARIAN")) %>% 
  ggplot(
    mapping = aes(
      x = party,
      y = amount
    )
  ) +
  geom_boxplot(
    mapping = aes(fill = party),
    varwidth = TRUE,
    outlier.alpha = 0.01
  ) +
  scale_y_continuous(
    trans = "log10",
    labels = dollar
  ) +
  scale_fill_manual(
    guide = FALSE,
    values = c(
      "dodgerblue",
      "mediumpurple",
      "grey10",
      "red2"
    )
  ) +
  labs(
    title = "HI Expend Amount",
    caption = "Source: HI CRS",
    x = "Amount",
    y = "Count"
  )
```

```{r amount_line_time, echo=FALSE}
hi %>% 
  group_by(party) %>% 
  arrange(date) %>% 
  mutate(total = cumsum(amount)) %>% 
  ggplot(
    mapping = aes(
      x = date,
      y = total
    )
  ) +
  geom_line(
    size = 2,
    mapping = aes(
      color = party
    )
  ) +
  scale_y_continuous(labels = dollar) +
  scale_color_manual(
    values = c(
      "dodgerblue",
      "forestgreen",
      "mediumpurple",
      "orange",
      "grey10",
      "red2"
    )
  ) +
  labs(
    title = "HI Expend Amount",
    caption = "Source: HI CRS",
    x = "Time",
    y = "Total Amount"
  )
```

```{r amount_cat_box, echo=FALSE}
top_category <- hi %>% 
  count(expenditure_category, sort = TRUE) %>% 
  mutate(p = n/sum(n)) %>% 
  filter(p > 0.05) %>% 
  pull(expenditure_category)

hi %>% 
  filter(expenditure_category %in% top_category) %>% 
  ggplot(
    mapping = aes(
      x = reorder(expenditure_category, amount),
      y = amount
    )
  ) +
  geom_boxplot(
    mapping = aes(fill = expenditure_category),
    varwidth = TRUE,
    outlier.alpha = 0.01
  ) +
  scale_fill_brewer(
    type = "qual",
    palette = "Set1",
    guide = FALSE
  ) +
  scale_y_continuous(
    trans = "log10",
    labels = dollar
  ) +
  coord_flip() +
  labs(
    title = "HI Expend Amount",
    caption = "Source: HI CRS",
    x = "Amount",
    y = "Count"
  )
```

#### Date

There are no dates before `r as_date(min(hi$date))` and `r sum(hi$date > today())` dates past the
creation of this document.

```{r date_range, collapse=TRUE}
min(hi$date)
max(hi$date)
sum(hi$date > today())
```

To better explore the distribution of dates and track expendtures, we will create a `year_clean`
variable from `date` using `lubridate::year()`.

```{r add_year}
hi <- mutate(hi, year_clean = year(date))
```

We can see the expenditures naturally increase in frequency every other year, during the elections.

```{r year_count}
hi %>%
  ggplot(aes(year_clean)) +
  geom_bar() +
  labs(
    title = "HI Expends per Year",
    caption = "Source: HI CRS",
    x = "Year",
    y = "Count"
  )
```

```{r year_amount}
is_even <- function(x) x %% 2 == 0
hi %>% 
  mutate(election_year = is_even(year_clean)) %>% 
  group_by(year_clean, election_year) %>% 
  summarize(mean = mean(amount)) %>% 
  ggplot(aes(year_clean, mean)) +
  geom_col(aes(fill = election_year)) +
  scale_y_continuous(labels = dollar) +
  labs(
    title = "HI Expend Mean Amount per Year",
    caption = "Source: HI CRS",
    fill = "Election Year",
    x = "Amount",
    y = "Mean Amount"
  )
```

```{r}
hi %>% 
  mutate(
    month = month(date),
    election_year = is_even(year_clean)
  ) %>%
  group_by(month, election_year) %>% 
  summarize(mean = mean(amount)) %>% 
  ggplot(aes(month, mean)) +
  scale_y_continuous(labels = dollar) +
  geom_line(size = 2, aes(color = election_year)) +
  labs(
    title = "HI Expend Mean Amount over Year",
    caption = "Source: HI CRS",
    fill = "Election Year",
    x = "Amount",
    y = "Mean Amount"
  )
```

## Wrangle

To improve the searchability of the database, we can perform some functional data cleaning and
text normalization, using the `campfin::normal_*()` functions, which wrap around `stringr::str_*()`
functions.

### Address

```{r norm_address}
hi <- hi %>% 
  unite(
    col = address_combine,
    address_1, address_2,
    sep = " ",
    remove = FALSE,
    na.rm = TRUE
  ) %>% 
  mutate(
    address_norm = normal_address(
      address = address_combine,
      add_abbs = usps_street,
      na_rep = TRUE
    )
  )
```

```{r view_address}
hi %>% 
  select(
    address_1,
    address_2,
    address_norm
  )
```

### ZIP

```{r pre_zip, collapse=TRUE}
n_distinct(hi$zip_code)
prop_in(hi$zip_code, valid_zip)
sum(hi$zip_code %out% valid_zip)
```

```{r normal_zip}
hi <- hi %>% 
  mutate(
    zip_norm = normal_zip(
      zip = zip_code,
      na_rep = TRUE
    )
  )
```

```{r post_zip, collapse=TRUE}
n_distinct(hi$zip_norm)
prop_in(hi$zip_norm, valid_zip)
sum(hi$zip_norm %out% valid_zip)
```

### State

`r percent(prop_in(hi$state, valid_state))` of `state` values are valid.

```{r pre_state, collapse=TRUE}
n_distinct(hi$state)
prop_in(hi$state, valid_state)
sum(hi$state %out% valid_state)
```

### City

#### Normal

```{r pre_city, collapse=TRUE}
n_distinct(hi$city)
prop_in(hi$city, valid_city)
sum(unique(hi$city) %out% valid_city)
```

```{r view_city}
hi %>% 
  count(city, sort = TRUE) %>% 
  filter(city %out% valid_city)
```

```{r}
hi <- hi %>% 
  mutate(
    city_norm = normal_city(
      city = city,
      geo_abbs = usps_city,
      st_abbs = c("HI", "HAWAII", "DC"),
      na = invalid_city,
      na_rep = TRUE
    )
  )

n_distinct(hi$city_norm)
prop_in(hi$city_norm, valid_city)
sum(unique(hi$city_norm) %out% valid_city)
```

#### Swap

```{r swap_city, collapse=TRUE}
hi <- hi %>% 
  rename(city_raw = city) %>% 
  left_join(
    y = zipcodes,
    by = c(
      "zip_norm" = "zip", 
      "state" = "state"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_dist = stringdist(city_norm, city_match),
    city_swap = if_else(
      condition = match_dist == 1,
      true = city_match,
      false = city_norm
    )
  )

mean(hi$match_dist, na.rm = TRUE)
max(hi$match_dist, na.rm = TRUE)
sum(hi$match_dist == 1, na.rm = TRUE)
n_distinct(hi$city_swap)
prop_in(hi$city_swap, valid_city)
sum(unique(hi$city_swap) %out% valid_city)
```

```{r view_swap}
hi %>% 
  count(state, city_swap, sort = TRUE) %>% 
  filter(city_swap %out% valid_city) %>% 
  drop_na()
```

```{r fix_city}
hi$city_swap <- hi$city_swap %>% 
  str_replace("HON", "HONOLULU") %>% 
  na_if("HI")
```

## Conclude

1. There are `r comma(nrow(hi))` records in the database.
1. There are `r sum(hi$dupe_flag)` duplicate records, flagged with `dupe_flag`.
1. Ranges for `amount` and `date` are both reasonable.
1. There are no missing records of importance.
1. Consistency issues in geographic values have been improved.
1. The 5-digit `zip_norm` variable has been created with `campfin::normal_zip(hi$zip_code)`
1. The 4-gitit `year_clean` variable has been created with `lubridate::year(hi$date)`
1. Every record has a payer, payee, date, and amount.

## Export

```{r}
proc_dir <- here("hi", "expends", "data", "processed")
dir_create(proc_dir)

hi <- hi %>% 
  select(
    -inoutstate,
    -zip_code,
    -city_raw,
    -address_1,
    -address_2,
    -address_combine,
    -city_norm,
    -city_match,
    -match_dist
  )
```

## Lookup

```{r}
lookup <- read_csv("hi/expends/data/hi_city_lookup.csv") %>% select(1:2)
hi <- left_join(hi, lookup)
progress_table(hi$city_swap, hi$city_clean, compare = valid_city)
write_csv(
  x = hi,
  path = glue("{proc_dir}/hi_expends_clean.csv"),
  na = ""
)
```

