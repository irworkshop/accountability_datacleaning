---
title: "Tennessee Expenditures"
author: "Kiernan Nicholls"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
set.seed(5)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This package contains
functions custom made to help facilitate the processing of campaign finance data.

```{r load_packages, message=FALSE, warning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  tidyverse, # data manipulation
  lubridate, # datetime strings
  magrittr, # pipe opperators
  janitor, # dataframe clean
  batman, # convert logical
  refinr, # cluster and merge
  scales, # format strings
  knitr, # knit documents
  vroom, # read files fast
  glue, # combine strings
  here, # relative storage
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where does this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Import

Data is obtained from the [Tennessee Bureau of Ethics and Campaign Finance (BECF)][becf].

Data can be searched on the [BECF portal][portal] by year and downloaded locally.

[becf]: https://www.tn.gov/tref.html
[portal]: https://apps.tn.gov/tncamp-app/public/ceresults.htm

```{r create_raw_dir}
raw_dir <- here("tn", "expends", "data", "raw")
dir_create(raw_dir)
```

### Read

```{r read_raw}
tn <- map_df(
  .x = dir_ls(raw_dir),
  .f = read_delim,
  delim = ",",
  escape_double = FALSE,
  escape_backslash = FALSE,
  col_types = cols(
    .default = col_character(),
    Amount = col_number(),
    # date should be col_date_usa()
    # a couple hundred use two digit year
    Date = col_character(), 
    `Election Year` = col_integer()
  )
)

tn <- clean_names(tn, "snake")
tn <- mutate(tn, adj = to_logical(adj), support_lgl = equals(s_o, "S"))
```

## Explore

```{r glimpse}
head(tn)
tail(tn)
glimpse(sample_frac(tn))
```

### Missing

```{r glimpse_na}
glimpse_fun(tn, count_na)
```

```{r flag_na}
tn <- tn %>% flag_na(amount, date, candidate_pac_name, vendor_name)
sum(tn$na_flag)
mean(tn$na_flag)
```

### Duplicates

```{r flag_dupes}
tn <- flag_dupes(tn, everything())
sum(tn$dupe_flag)
mean(tn$dupe_flag)
```

### Categorical

```{r glimpse_distinct}
glimpse_fun(tn, n_distinct)
```

### Continuous

#### Amounts

```{r summary_amount}
summary(tn$amount)
```

```{r amount_histogram, echo=FALSE}
tn %>%
  ggplot(aes(amount)) +
  geom_histogram(fill = RColorBrewer::brewer.pal(3, "Dark2")[3]) +
  geom_vline(xintercept = median(tn$amount, na.rm = TRUE), linetype = 2) +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "Tennessee Expenditure Amount Distribution",
    caption = "Source: Tennessee Registry of Election Finance",
    x = "Amount",
    y = "Count"
  )
```

#### Dates

```{r}
date_split <- str_split(tn$date, "/")
date_join <- rep(NA_character_, length(tn$date))
for (i in seq_along(date_split)) {
  year <- date_split[[i]][3]
  if (nchar(year) == 2 & grepl("^(0|1)\\d", year)) {
    date_split[[i]][3] <- paste0("20", year)
  }
  date_join[i] <- str_c(date_split[[i]][3], date_split[[i]][1], date_split[[i]][2], sep = "-")
}
prop_na(tn$date)
prop_na(date_join)
tn <- mutate(tn, date_fix = as.Date(date_join))
```

```{r add_year}
tn <- mutate(tn, year = year(date_fix))
```

```{r}
count(tn, year)
```

```{r date_range, collapse=TRUE}
min(tn$date_fix, na.rm = TRUE)
sum(tn$year < 2008, na.rm = TRUE)
max(tn$date_fix, na.rm = TRUE)
sum(tn$date_fix > today(), na.rm = TRUE)
```

```{r}
tn <- tn %>% 
  mutate(
    date_flag = year < 2008 | date_fix > today(),
    date_clean = as_date(ifelse(date_flag, NA, date_fix)),
    year_clean = year(date_clean)
  )
```

```{r amount_hist, echo=FALSE}
tn %>% 
  count(year_clean) %>% 
  mutate(on = is_even(year_clean)) %>% 
  ggplot(aes(x = year_clean, y = n)) +
  geom_col(aes(fill = on)) +
  scale_fill_brewer(palette = "Dark2") +
  labs(
    title = "Tennessee Expenditure Yearly Count",
    caption = "Source: Tennessee Registry of Election Finance",
    x = "Year Made",
    fill = "Election Year",
    y = "Count"
  )
```

## Wrangle

```{r address_separate}
tn <- tn %>% 
  separate(
    col = vendor_address,
    into = c(glue("addr_split{1:5}"), "city_sep", "state_sep", "zip_sep"),
    sep = ",\\s",
    remove = FALSE,
    fill = "left"
  ) %>%
  unite(
    starts_with("addr_split"),
    col = "address_sep",
    sep = " ",
    na.rm = TRUE
  ) %>% 
  mutate_at(vars(ends_with("sep")), str_trim)
```

```{r address_spli_view, echo=FALSE}
tn %>% 
  select(
    vendor_address,
    address_sep,
    city_sep,
    state_sep,
    zip_sep    
  ) %>% 
  distinct() %>% 
  sample_frac()
```

```{r}

```

### Address

```{r address_norm}
tn <- tn %>% 
  mutate(
    address_norm = normal_address(
      address = address_sep,
      add_abbs = usps_street,
      na_rep = TRUE
    )
  )
```

```{r address_view}
tn %>% 
  select(starts_with("address")) %>%
  distinct() %>% 
  sample_frac()
```

### ZIP

```{r zip_norm}
tn <- tn %>% 
  mutate(
    zip_norm = normal_zip(
      zip = zip_sep,
      na_rep = TRUE
    )
  )
```

```{r zip_view}
tn %>% 
  select(starts_with("zip")) %>% 
  filter(zip_sep != zip_norm) %>% 
  distinct() %>% 
  sample_frac()
```

```{r zip_progress}
progress_table(
  tn$zip_sep,
  tn$zip_norm,
  compare = valid_zip
)
```

## State

```{r state_view_bad}
tn %>% 
  drop_na(state_sep) %>% 
  filter(state_sep %out% valid_state) %>% 
  count(state_sep, sort = TRUE)
```

```{r state_shift}
# shift these all left 1
city_states <- which(tn$state_sep %in% zipcodes$city[zipcodes$state == "TN"])
tn$address_sep[city_states] <- tn$city_sep[city_states]
tn$city_sep[city_states] <- tn$state_sep[city_states]
tn$state_sep[city_states] <- "TN"
```

```{r state_norm}
tn <- tn %>% 
  mutate(
    state_norm = normal_state(
      state = state_sep,
      abbreviate = TRUE,
      na_rep = TRUE,
      valid = valid_state
    )
  )
```

```{r state_progress}
progress_table(
  tn$state_sep,
  tn$state_norm,
  compare = valid_state
)
```

## City

```{r city_norm_match_swap}
tn <- tn %>% 
  mutate(
    city_norm = normal_city(
      city = city_sep,
      geo_abbs = usps_city,
      st_abbs = c("TN", "DC"),
      na = invalid_city,
      na_rep = TRUE
    )
  ) %>% 
  left_join(
    zipcodes, 
    by = c(
      "zip_norm" = "zip",
      "state_norm" = "state"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_dist = str_dist(city_norm, city_match),
    match_abb = is_abbrev(city_norm, city_match),
    city_swap = if_else(
      condition = match_abb | match_dist == 1,
      true = city_match,
      false = city_norm
    )
  )
```

```{r city_progress}
progress_table(
  tn$city_sep,
  tn$city_norm,
  tn$city_swap,
  compare = valid_city
)
```

## Conclude

1. There are `r nrow(tn)` records in the database.
1. There are `r sum(tn$dupe_flag)` duplicate records in the database (`dupe_flag`).
1. The range and distribution of `amount` is reasonable.
1. The range of `date` has been cleaned by removing `r sum(tn$date_flag, na.rm = T)` values in a new `date_clean` variable.
1. There are `r sum(tn$na_flag)` records missing either `vendor_name` or `date` (`na_flag`).
1. Consistency in geographic data has been improved with `campfin::normal_*()`.
1. The 5-digit `zip_norm` variable has been created with `campfin::normal_zip()`.
1. The 4-digit `year_clean` variable has been created with `lubridate::year()`.

## Export

```{r create_proc_dir}
proc_dir <- here("tn", "expends", "data", "processed")
dir_create(proc_dir)
```

```{r clean_trim}
tn <- tn %>%
  select(
    -year,
    -date_fix,
    -address_sep,
    -zip_sep,
    -state_sep,
    -city_sep,
    -city_norm,
    -city_match,
    -match_dist,
    -match_abb
  ) %>% 
  rename(
    address_clean = address_norm,
    zip_clean = zip_norm,
    state_clean = state_norm,
    city_clean = city_swap,
  )

```

```{r clean_write, eval=FALSE}
tn %>% 
  write_csv(
    path = glue("{proc_dir}/tn_expends_clean.csv"),
    na = ""
  )
```

## Lookup

```{r lookup}
lookup_file <- here("tn", "expends", "data", "tn_city_lookup.csv")
if (file_exists(lookup_file)) {
  lookup <- read_csv(lookup_file) %>% select(1:2)
  tn <- left_join(tn, lookup, by = "city_clean")
}
```

```{r lookup_progress}
progress_table(
  tn$city_clean,
  tn$city_clean_new,
  compare = valid_city
)
```

```{r lookup_write}
tn %>% 
  select(-city_clean) %>% 
  rename(city_clean = city_clean_new) %>% 
  write_csv(
    path = glue("{proc_dir}/tn_expends_clean.csv"),
    na = ""
  )
```
