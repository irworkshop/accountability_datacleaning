---
title: "New Mexico Expenditures"
author: "Kiernan Nicholls"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

```{r load_packages, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_current_gh("kiernann/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  snakecase, # change string case
  RSelenium, # remote browser
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tidytext, # text analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  batman, # rep(NA, 8) Batman!
  refinr, # cluster and merge
  scales, # format strings
  knitr, # knit documents
  vroom, # read files fast
  glue, # combine strings
  here, # relative storage
  httr, # http query
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

Data is obtained from the [New Mexico Campaign Finance Information System (CFIS)][04].

From the [CFIS Data Download page][05], we can retrieve bulk data.

> The files available will be downloaded in a CSV file format. To download a file, select the type
of CFIS data and corresponding filters and click Download Data. The file will be downloaded and
should appear in the lower left corner of your browser window. If you do not see the file, please
check your browserâ€™s pop-up blocker settings.

The transaction files are not separated by contribution or expenditure.

> Download a listing of all contributions and expenditures for one or all filing periods for
candidates, PACs, and Lobbyists. The CFIS data available for download is updated daily at 12:00AM
and 12:00PM MST.

[04]: https://www.cfis.state.nm.us/
[05]: https://www.cfis.state.nm.us/media/CFIS_Data_Download.aspx

### Download

The form on the data download page must be manually filled out to download a file. We can automate
this process with the RSelenium package.

```{r raw_dir}
raw_dir <- here("nm", "expends", "data", "raw")
dir_create(raw_dir)
```

```{r remote_download, eval=FALSE}
# open the driver with auto download options
remote_driver <- rsDriver(
  port = 4444L,
  browser = "firefox",
  extraCapabilities = makeFirefoxProfile(
    list(
      browser.download.dir = raw_dir,
      browser.download.folderList = 2L,
      browser.helperApps.neverAsk.saveToDisk = "text/csv"
    )
  )
)

# navigate to the FL DOE download site
remote_browser <- remote_driver$client
remote_browser$navigate("https://www.cfis.state.nm.us/media/CFIS_Data_Download.aspx")

# chose "All" from elections list
type_menu <- "/html/body/form/div[3]/div[2]/div[2]/select/option[3]"
remote_browser$findElement("xpath", type_menu)$clickElement()

# find download button
download_button <- '//*[@id="ctl00_ContentPlaceHolder1_header1_Button1"]'

# download candidate trans
cand_menu <- "/html/body/form/div[3]/div[2]/div[2]/div[2]/table/tbody/tr/td[2]/select/option[2]"
remote_browser$findElement("xpath", cand_menu)$clickElement()
remote_browser$findElement("xpath", download_button)$clickElement()

# download committee trans
comm_menu <- "/html/body/form/div[3]/div[2]/div[2]/div[2]/table/tbody/tr/td[2]/select/option[3]"
remote_browser$findElement("xpath", comm_menu)$clickElement()
remote_browser$findElement("xpath", download_button)$clickElement()

# close the browser and driver
remote_browser$close()
remote_driver$server$stop()
```

## Read

We can read in both the files for Candidates and Committee transactions separated.

```{r}
nm_cands <- vroom(
  file = glue("{raw_dir}/CandidateTransactions.csv"),
  .name_repair = make_clean_names,
  col_types = cols(
    .default = col_character(),
    IsContribution = col_logical(),
    IsAnonymous = col_logical(),
    Amount = col_double(),
    `Date Contribution` = col_datetime(),
    `Date Added` = col_datetime()
  )
) %>% rename(
  cand_first = first_name,
  cand_last = last_name
)
```

```{r}
nm_comms <- vroom(
  file = glue("{raw_dir}/PACTransactions.csv"),
  .name_repair = make_clean_names,
  col_types = cols(
    .default = col_character(),
    IsContribution = col_logical(),
    IsAnonymous = col_logical(),
    Amount = col_double(),
    `Date Contribution` = col_datetime(),
    `Date Added` = col_datetime()
  )
)
```

The data frames can then be joined together and contributions can be removed.

```{r bind_rows}
nm <- 
  bind_rows(
    nm_cands,
    nm_comms
  ) %>% 
  filter(!is_contribution) %>% 
  mutate_if(is_character, str_to_upper) %>% 
  rename(type = description)

names(nm) <- str_remove(names(nm), "contrib_expenditure_")
```

```{r rm_sub, echo=FALSE}
# remove to save memory
rm(nm_cands, nm_comms)
```

## Explore

```{r}
head(nm)
tail(nm)
glimpse(sample_frac(nm))
```

### Missing

There are zero records without both a payer (either `cand_last` or `pac_name`), a date, _and_ an
amount.

```{r count_na}
glimpse_fun(nm, count_na)
```

### Duplicates

A huge number of records in this database are complete duplicates. We can find them using
`janitor::get_dupes()`.

```{r get_dupes}
nm <- flag_dupes(nm, everything())
sum(nm$dupe_flag)
percent(mean(nm$dupe_flag))
```

### Categorical

For categorical variables, we should explore the distribution of distinct/frequent values.

```{r n_distinct}
glimpse_fun(nm, n_distinct)
```

From this, we can see that our database only contains "MONETARY EXPENDITURE" records. There is no
completely distinct unique identifier.

### Continuous

For continuous variables, we should explore the range and distribution of the values.

#### Amounts

```{r range_amount, collapse=TRUE}
summary(nm$amount)
sum(nm$amount <= 0)
```

There are very few negative values (typically corrections). We can also see that the largest 
"expenditure" is really simply a transfer of funds.

```{r glimpse_max_amount}
nm %>% 
  filter(amount == max(amount)) %>% 
  glimpse()
```

```{r amount_hist, echo=FALSE}
nm %>% 
  ggplot(aes(amount)) +
  geom_histogram() +
  scale_x_continuous(
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "New Mexico Expenditure Amount Distribution",
    caption = "Source: NM CFIS",
    x = "Amount",
    y = "Count"
  )
```

#### Dates

```{r range_date, collapse=TRUE}
min(nm$date_contribution)
max(nm$date_contribution)
sum(nm$date_contribution > today())
```

```{r add_year}
nm <- mutate(nm, year = year(date_contribution))
```

```{r print_years}
print(count(nm, year), n = 26)
```

```{r clean_date}
nm <- mutate(nm, date_clean = date_contribution)
nm$date_clean[which(nm$date_clean > today() | nm$year < 2003)] <- NA
nm <- mutate(nm, year = year(date_clean))
```

```{r year_bar, echo=FALSE}
nm %>%
  mutate(on = is_even(year)) %>% 
  count(on, year) %>% 
  ggplot(aes(year, n)) +
  geom_col(aes(fill = on)) +
  scale_fill_brewer(
    type = "qual",
    palette = "Dark2"
  ) +
  labs(
    title = "New Mexico Expenditure Count by Year",
    caption = "Source: NM CFIS",
    fill = "Election Year",
    x = "Amount",
    y = "Count"
  )
```

## Wrangle

### Address

```{r normal_address}
nm <- nm %>% 
  mutate(
    address_norm = normal_address(
      address = address,
      add_abbs = usps_street,
      na_rep = TRUE
    )
  )
```

```{r sample_address_changes}
nm %>% 
  select(
    address,
    address_norm
  ) %>% 
  sample_n(10)
```

### ZIP

```{r zip_pre, collapse=TRUE}
n_distinct(nm$zip)
prop_in(nm$zip, valid_zip, na.rm = TRUE)
length(setdiff(nm$zip, valid_zip))
```

```{r normal_zip}
nm <- nm %>% 
  mutate(
    zip_norm = normal_zip(
      zip = zip,
      na_rep = TRUE
    )
  )
```

```{r zip_post, collapse=TRUE}
n_distinct(nm$zip_norm)
prop_in(nm$zip_norm, valid_zip, na.rm = TRUE)
length(setdiff(nm$zip_norm, valid_zip))
```

### State

```{r state_pre, collapse=TRUE}
n_distinct(nm$state)
prop_in(nm$state, valid_state, na.rm = TRUE)
length(setdiff(nm$state, valid_state))
```

```{r normal_state}
nm <- nm %>% 
  mutate(
    state_norm = normal_state(
      abbreviate = TRUE,
      valid = valid_state,
      na_rep = TRUE,
      state = state %>% 
        str_replace("^NEWMEXICO$", "NM") %>% 
        str_replace("^DISTRICTOFCOLUMBIA$", "DC") %>% 
        str_replace("^NEWHAMPSHIRE$", "NH") %>%
        str_replace("^NEWJERSEY$", "NJ") %>% 
        str_replace("^NEWYORK$", "NY") %>% 
        str_replace("^S DAKOTA$", "SD") %>% 
        str_replace("^W VA$", "WV") %>% 
        str_replace("^DEL$", "DE") %>% 
        str_replace("^COLO$", "CO") %>% 
        str_replace("^ILL$", "IL") %>% 
        str_replace("^ILINOIS$", "IL") %>% 
        str_replace("^CORADO$", "CO") %>% 
        str_replace("^ONTARIO$", "OT") %>% 
        str_replace("^S CAROLINA$", "SC") %>% 
        str_replace("^WA DC$", "DC") %>% 
        str_replace("^WASH DC$", "DC") %>% 
        str_replace("^MASS$", "MA") %>% 
        str_replace("^N CAROLINA$", "NC") %>% 
        str_replace("^DEAWARE$", "DE") %>% 
        str_replace("^NEW JERSERY$", "NJ") %>% 
        str_replace("^NEW MEXCO$", "NM") %>% 
        str_replace("^TENNESEE$", "TN") %>% 
        str_replace("^TEX$", "TX") %>% 
        str_replace("^DEAWARE$", "DE")
    )
  )
```

```{r state_post, collapse=TRUE}
n_distinct(nm$state_norm)
prop_in(nm$state_norm, valid_state, na.rm = TRUE)
```

### City

```{r city_pre, collapse=TRUE}
n_distinct(nm$city)
prop_in(nm$city, valid_city, na.rm = TRUE)
length(setdiff(nm$city, valid_city))
```

```{r rename_city}
nm <- rename(nm, city_raw = city)
```

#### Normalize

```{r normal_city}
nm <- nm %>%
  mutate(
    city_norm = normal_city(
      na = invalid_city,
      na_rep = TRUE,
      geo_abbs = usps_city,
      st_abbs = c("NM", "DC", "NEW MEXICO"),
      city = 
        str_trim(city_raw) %>% 
        str_replace("^SF$", "SANTA FE") %>% 
        str_replace("^LC$", "LAS CRUCES") %>% 
        str_replace("^AL$", "ALBUQUERQUE") %>% 
        str_replace("^ABQ$", "ALBUQUERQUE") %>% 
        str_replace("^ALB$", "ALBUQUERQUE") %>% 
        str_replace("^ALBQ$", "ALBUQUERQUE") %>% 
        str_replace("^ALBU$", "ALBUQUERQUE") %>% 
        str_replace("^ALBUQ$", "ALBUQUERQUE") %>% 
        str_replace("^ALBQU$", "ALBUQUERQUE") %>% 
        str_replace("^ALBUEQ$", "ALBUQUERQUE") %>% 
        str_replace("^ALBURQUE$", "ALBUQUERQUE") %>% 
        str_replace("^ALBUQUQUE$", "ALBUQUERQUE") %>% 
        str_replace("^ALBUQIERQUE$", "ALBUQUERQUE") %>% 
        str_replace("^ALBUQUERQUENM$", "ALBUQUERQUE") %>% 
        str_replace("^ALBUQUEWRWQUE$", "ALBUQUERQUE") %>% 
        str_replace("^T OR C$", "TRUTH OR CONSEQUENCES")
    )
  )
```

```{r city_post_norm, collapse=TRUE}
n_distinct(nm$city_norm)
prop_in(nm$city_norm, valid_city, na.rm = TRUE)
length(setdiff(nm$city_norm, valid_city))
```

#### Swap

```{r swap_city}
nm <- nm %>%
  left_join(
    y = zipcodes,
    by = c(
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_dist = stringdist(city_norm, city_match),
    city_swap = if_else(
      condition = match_dist == 1,
      true = city_match,
      false = city_norm
    )
  )

mean(nm$match_dist, na.rm = TRUE)
sum(nm$match_dist == 1, na.rm = TRUE)
```

```{r city_post_swap, collapse=TRUE}
n_distinct(nm$city_swap)
prop_in(nm$city_swap, valid_city, na.rm = TRUE)
length(setdiff(nm$city_swap, valid_city))
```

```{r count_city_post_swap}
nm %>% 
  filter(city_swap %out% valid_city) %>% 
  count(city_swap, sort = TRUE)
```

#### Refine

```{r city_refine}
nm_refined <- nm %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge(dict = valid_city[valid_state == "NM"]) %>% 
      n_gram_merge(numgram = 2)
  )
```

```{r join_refine}
nm <- nm %>% 
  left_join(nm_refined) %>% 
  mutate(city_clean = coalesce(city_swap, city_refine))
```

```{r city_post_refine, collapse=TRUE}
n_distinct(nm_refined$city_refine)
prop_in(nm_refined$city_refine, valid_city, na.rm = TRUE)
length(setdiff(nm_refined$city_refine, valid_city))
```

## Conclude

```{r conclude, echo=FALSE}
n_dupes <- sum(nm$dupe_flag, na.rm = T)
p_dupes <- percent(mean(nm$dupe_flag))
```

1. There are `r nrow(nm)` records in the database.
1. There are `r n_dupes` records (`r p_dupes` of the database) flagged with `dupe_flag`.
1. The range and distribution of `amount` and `date` seem reasonable.
1. There are zero records with unexpected missing values.
1. Consistency in goegraphic data has been improved with `campfin::normal_*()`.
1. The 5-digit `zip_norm` variable has been created with `campfin::normal_zip(nm$zip)`.
1. The 4-digit `year` variable has been created with `lubridate::year(nm$date_clean)`.

## Export

```{r proc_dir}
proc_dir <- here("nm", "expends", "data", "processed")
dir_create(proc_dir)
```

```{r write_csv}
nm <- nm %>% 
  select(
    -city_match,
    -city_norm,
    -match_dist,
    -city_swap,
    -city_refine
  )
```

## Lookup

```{r lookup_city}
lookup <- read_csv("nm/expends/data/nm_city_lookup.csv") %>% select(1:2)
nm <- left_join(nm, lookup)
progress_table(
  nm$city_raw,
  nm$city_clean, 
  nm$city_clean2, 
  compare = valid_city
)
write_csv(
  x = nm,
  path = glue("{proc_dir}/nm_expends_clean.csv"),
  na = ""
)
```

