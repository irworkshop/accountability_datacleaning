---
title: "Maryland Expenditures"
author: "Kiernan Nicholls"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  # it's nice to un-collapse df print
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
```

## Project

The Accountability Project is an effort to cut across data silos and give
journalists, policy professionals, activists, and the public at large a simple
way to search across huge volumes of public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each
dataset row as a transaction. For each transaction there should be (at least) 3
variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze,
and communicate these results. The `pacman` package will facilitate their
installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This
package contains functions custom made to help facilitate the processing of
campaign finance data.

```{r load_packages, message=FALSE, warning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  RSelenium, # remote browser
  tidyverse, # data manipulation
  lubridate, # datetime strings
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  scales, # format strings
  knitr, # knit documents
  vroom, # read files fast
  glue, # combine strings
  here, # relative storage
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a
sub-directory of the more general, language-agnostic
[`irworkshop/accountability_datacleaning`][01] GitHub repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be
run as such. The project also uses the dynamic `here::here()` tool for file
paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

## Data

Data is obtained from the [Maryland Campaign Reporting Information System][03].

As explained by this [CRIS help page][04]:

> ## General Information on Expenditures and Outstanding Obligations
>
> An ***expenditure*** is defined as a gift, transfer, disbursement, or promise
of money or valuable thing by or on behalf of a political committee to promote
or assist in promoting the success or defeat of a candidate, political party, or
question at  an election.
> 
> Expenditures must be election related; that is, they must enhance the
candidates election chances, such that they would not have been incurred if
there had been no candidacy. Furthermore, expenditures, including loans, may not
be for the personal use of the candidate or any other individual.
> 
> An outstanding obligation is any unpaid debt that the committee has incurred
at the end of a reporting period.

[03]: https://campaignfinance.maryland.gov/Home/Logout
[04]: https://campaignfinance.maryland.gov/home/viewpage?title=View%20Expenditures%20/%20Outstanding%20Obligations&link=Public/ViewExpenses

## Import

### Download

The data must be exported from the results page of an [expenditure search][05]. We can automate
this process using the RSelenium package.

```{r create_raw_dir}
raw_dir <- here("md", "expends", "data", "raw")
dir_create(raw_dir)
```

```{r download_raw, warning=FALSE, error=FALSE, message=FALSE, collapse=TRUE, eval=FALSE}
remote_driver <- rsDriver(browser = "firefox")
remote_browser <- remote_driver$client
remote_browser$navigate("https://campaignfinance.maryland.gov/Public/ViewExpenses")
remote_browser$findElement("css", "#dtStartDate")$sendKeysToElement(list("01/01/2008"))
end_date <- format(today(), "%m/%d/%Y")
remote_browser$findElement("css", "#dtEndDate")$sendKeysToElement(list(end_date))
remote_browser$findElement("css", "#btnSearch")$clickElement()
remote_browser$findElement("css", "a.t-button:nth-child(1)")$clickElement()
remote_driver$server$stop()
```

```{r find_raw_file}
raw_file <- dir_ls(raw_dir)
```

[05]: https://campaignfinance.maryland.gov/Public/ViewExpenses

### Read

```{r read_raw}
md <- read_delim(
  file = raw_file,
  delim = ",",
  escape_double = FALSE,
  escape_backslash = FALSE,
  col_types = cols(
    .default = col_character(),
    `Expenditure Date` = col_date_usa(),
    `Amount($)` = col_double()
  )
)
```

Our search result above indicated that 503,676 records were available for download, yet we were
only able to correctly read `r scales::comma(nrow(md))`. From `readr::problems()` we can see the 
types of issues found when reading the file.

```{r read_problems}
md_probs <- problems(md)
nrow(md_probs)
distinct(select(md_probs, -row, -file))
```

These issues almost assuredly stem from extraneous or erroneous commas or quotation marks. We are
going to remove rows with these problems (for now).

```{r filter_bad_rows}
# identify bad rows
bad_rows <- unique(md_probs$row)
# bad rows make up small part
percent(length(bad_rows)/nrow(md))
# filter out bad rows and cols
md <- md %>% 
  extract(-bad_rows, ) %>% 
  remove_empty("cols") %>% 
  clean_names("snake")
```

## Wrangle 

To better understand the database, we will first have to perform some rudimentary wrangling.

### Separate

The `address` feild contains many not only the street address, but also the city, state, and ZIP
code. Each element of the address is separated by _two_ space characters (`\\s`). We can use the
[`tidyr::separate()`][sep] function to split this single column into four new columns. There are a
few rows containing more than four instances of double spaces; The first half of the address is the
most troublesome, sometimes containing a name or additional address information. The city, state,
and ZIP code are always the last 3 elements, so we can set `extra = "merge"` and `fill = "left"`
to push ensure our data is pushed to the rightmost column.

```{r sep_address}
md %>% separate(
  col    = address,
  into   = c("address1", "address2", "city_sep", "state_zip"),
  sep    = "\\s{2}",
  remove = FALSE,
  extra  = "merge",
  fill   = "left"
) -> md
```

Since we `fill = "left"`, any record without a secondary address unit (e.g., Apartment number)
will have their primary street address shifted into the new rightmost `address2` column. We do not
need the complex street address separated, so we can use [`tidyr::unite()`][unite] to combine them
into a single `address_sep` column.

```{r unite_address}
md %>% unite(
  address1, address2,
  col    = "address_sep",
  sep    = " ",
  remove = TRUE,
  na.rm  = TRUE
) -> md
```

Unlike the rest of the original `address` string, the state and ZIP code are separated by only _one_
space character. We can again use `tidyr::separate()`, this time only splitting the two elements
by the single space preceding the fist digit (`\\d`) of the ZIP code.

```{r separate_state_zip}
md %>% separate(
  col    = state_zip,
  into   = c("state_sep", "zip_sep"),
  sep    = "\\s{1,}(?=\\d)",
  remove = TRUE,
  extra  = "merge"
) -> md
```

Below, you can see how this makes wrangling these individual components much easier. Now we can
more easily search the database for a certain state or ZIP code.

```{r show_sep, echo=FALSE}
ex_sep <- sample_n(md, 10)
select(ex_sep, address)
select(ex_sep, ends_with("sep"))
rm(ex_sep)
```

[sep]: https://tidyr.tidyverse.org/reference/separate.html
[unite]: https://tidyr.tidyverse.org/reference/unite.html

### Normalize

Now that each geographic element in a separate column, we can use the `campfin::normal_*()` 
functions to improve the searchability of the database.

#### ZIP

```{r zip_pre}
progress_table(
  md$zip_sep,
  compare = valid_zip
)
```

The `campfin::normal_zip()` function is used to form a valid 5-digit US ZIP code from messy data.
It removes the unnecessary ZIP+4 suffix, pads the left of short strings with zeroes, and removes
any values which are a single repeating digit (e.g., 00000, XXXXX).

```{r normal_zip}
md <- mutate(md, zip_norm = normal_zip(zip_sep, na_rep = TRUE))
```

This process puts our percentage of valid `zip_norm` values well over 99%.

```{r zip_post}
progress_table(
  md$zip_sep,
  md$zip_norm,
  compare = valid_zip
)
```

#### Address

We can perform similar normalization on the `address_sep` variable, although it's impractical to
compare these values against a fixed set of known valid addresses. The primary function of
`campfin::normal_address()` is to replace all USPS abbreviations with their full string equivalent.
This improves consistency in our database and makes addresses more searchable.

```{r normal_address}
md <- md %>% 
  mutate(
    address_norm = normal_address(
      address = address_sep,
      add_abbs = usps_street,
      na_rep = TRUE
    )
  )
```

```{r show_address_norm, echo=FALSE}
ex_add <- sample_n(md, 10)
select(ex_add, address_sep, address_norm)
rm(ex_add)
```

#### State

```{r state_pre}
progress_table(
  md$state_sep,
  compare = valid_state
)
```

The `campfin::normal_state()` function aims to form a valid 2-letter state abbreviation from our
`state_sep` variable.

```{r normal_state}
md <- md %>% 
  mutate(
    state_norm =
      normal_state(
      state = state_sep,
      abbreviate = TRUE,
      na_rep = TRUE
    )
  )
```

There are still a fair number of invalid `state_norm` values that we can try and fix more manually,
although they account for less than `r percent(prop_out(md$state_norm, valid_state))` of the total
values.

We will create a list of all `state_norm` values not found in our comprehensive `valid_state` 
vector. Many of these are longer strings which contain a valid abbreviation or name.

```{r manual_abbrev_check}
bad_states <- md$state_norm[which(md$state_norm %out% valid_state)]
count_vec(bad_states)
```

We can abbreviate these full strings and attempt to extract a valid abbreviation from the end of
the invalid string.

```{r abbrev_bad_states}
bad_states <- abbrev_full(bad_states, full = valid_name, rep = valid_state)
fixed_states <- if_else(
  condition = str_extract(bad_states, "\\b[:upper:]{2}$") %in% valid_state,
  true = str_extract(bad_states, "\\b[:upper:]{2}$"),
  false = bad_states
)
```

```{r view_bad_state_fixes, echo=FALSE}
tibble(
  orig = md$state_norm[which(md$state_norm %out% valid_state)],
  new = fixed_states
) %>% 
  mutate(fixed = new %in% valid_state) %>%
  count(orig, new, fixed, sort = TRUE)
```

```{r replace_bad_states}
md <- mutate(md, state_clean = state_norm)
md$state_clean[which(md$state_clean %out% valid_state)] <- fixed_states
```

```{r state_post}
progress_table(
  md$state_sep,
  md$state_norm,
  md$state_clean,
  compare = valid_state
)
```

```{r count_bad_state}
md %>% 
  filter(state_clean %out% valid_state) %>% 
  count(state_clean, sort = TRUE)
```

#### City

The `city_sep` portion of the `address` is the hardest to normalize. The sheer potential variation
makes it difficult to assess how clean the data is. There is a five-step process we use to make
_confident_ improvements to the data.

1. Normalize with `campfin::normal_city()`.
1. Match against an _expected_ city for that ZIP code.
1. Compare that expected city against our normalized city.
1. Swap for our expected value if it meets our criteria.
1. Refine by cluster and merging remaining similar values.

```{r}
progress_table(
  md$city_sep,
  compare = valid_city
)
```

##### Normalize

First, we will use `campfin::normal_city()` to force consistent capitalization, abbreviations,
punctuation, and remove known invalid values.

```{r normal_city}
md <- md %>% 
  mutate(
    city_norm = normal_city(
      city = city_sep,
      geo_abbs = usps_city,
      st_abbs = c("MD", "DC", "MARYLAND"),
      na = c(invalid_city, ""),
      na_rep = TRUE
    )
  )
```

##### Match

Then, we can use `dplyr::left_join()` to perform a relational join with the `campfin::zipcodes`
dataframe, matching each `zip_norm` and `state_norm` value with their _expect_ city.

```{r match_city}
md <- md %>% 
  left_join(
    y = zipcodes,
    by = c(
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city)
```

```{r prop_match_na}
# no matching value
percent(prop_na(md$city_match)- prop_na(md$city_norm))
```

##### Compare

By using `campfin::str_dist()` and `campfin::is_abbrev()` we can compare our `city_norm` value
against the expected `city_match` value. These two functions are programmatic methods of checking
for fixed consistency issues.

```{r compare_city}
md <- md %>% 
  mutate(
    match_dist = str_dist(city_norm, city_match),
    match_abb = is_abbrev(city_norm, city_match)
  )
```

```{r compare_stats}
summary(md$match_dist)
sum(md$match_abb, na.rm = TRUE)
```

Here, we can see `city_norm` values that appear to be abbreviations of their expected `city_match`.

```{r view_match_abbs}
md %>% 
  filter(match_abb) %>% 
  count(city_norm, city_match, sort = TRUE)
```

Here, we can see `city_norm` values that are only 1 character change different than their 
`city_match` counterpart.

```{r view_match_dist}
md %>% 
  filter(match_dist == 1) %>% 
  count(city_norm, city_match, sort = TRUE)
```

##### Swap

If `city_norm` is _either_ an abbreviation for `city_match` or only 1 edit away, we can 
confidently use the expected value in place of the typo/abbreviation.

```{r swap_city}
md <- md %>% 
  mutate(
    city_swap = if_else(
      condition = match_abb | match_dist == 1,
      true = city_match,
      false = city_norm
    )
  )
```

##### Refine

One further step to mass check for inconsistencies is a cluster and merge method relying on the
OpenRefine algorithms to match similar values and rely on the most common. We will use 
`dplyr::inner_join()` to only keep any values where a _correct_ refine was made.

```{r refine_city}
good_refine <- md %>% 
  filter(state_norm == "MD") %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge() %>% 
      n_gram_merge(numgram = 1)
  ) %>%
  filter(city_refine != city_swap) %>% 
  inner_join(
    y = zipcodes,
    by = c(
      "city_refine" = "city",
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  )
```

In this dataset, not many additional changes we made.

```{r count_refine}
good_refine %>% 
  count(city_swap, city_refine, sort = TRUE)
```

Nevertheless, it's still an improvement and we can add these changes to our data.

```{r join_refine}
md <- md %>% 
  left_join(good_refine) %>% 
  mutate(city_refine = coalesce(city_refine, city_swap))
```

##### Evaluate

Maryland is a state with a large number of residents living in 
[census designated places (CDP)][cdp]. We can get a list of Census Designated Places in Maryland
from the [Maryland OpenData portal][mdod]. This is a shape file, but we are only interested in the
_names_ of each CDP. These are names people will often use when listing their residence.

> Places always nest within a State, but may extend across county and county subdivision
boundaries. An incorporated place usually is a city, town, village, or borough, but can have other
legal descriptions. CDPs are delineated for the decennial census as the statistical counterparts of
incorporated places. CDPs are delineated to provide data for settled concentrations of population
that are identifiable by name, but are not legally incorporated under the laws of the State in
which they are located.

[cdp]: https://en.wikipedia.org/wiki/Census-designated_place
[mdod]: https://data.imap.maryland.gov/datasets/008cbfc9d1d34644864b6b0110f318ab_1

```{r read_designated_places}
cdp <- read_csv("https://opendata.arcgis.com/datasets/008cbfc9d1d34644864b6b0110f318ab_1.csv")
designated_place <- normal_city(cdp$NAME10, geo_abbs = usps_city)
```

```{r combine_valid_city}
valid_city2 <- c(valid_city, designated_place)
```

We will add the four most common invalid city names. These have been checked and are valid city
names, primarily small towns in Maryland or CDPs in other states that appear frequently in the
dataset.

```{r extend_valid_city2}
valid_city3 <- c(valid_city2, "CHEVERLY", "PRINCE GEORGES" ,"BERWYN HEIGHTS", "SYMMES TOWNSHIP")
```

We will fix four typos manually in a new `city_clean` variable.

```{r manual_fix_city}
md <- md %>% 
  mutate(
    city_clean = city_refine %>% 
      str_replace("^BALTO$", "BALTIMORE") %>% 
      str_replace("^LAVALE$", "LA VALE") %>% 
      str_replace("^SYMMES$", "SYMMES TOWNSHIP") %>% 
      str_replace("^BALTIMORE CITY$", "BALTIMORE")
  )
```

There are still many `city_clean` variables not contained in our list of valid cities and census
designated places. Many of them are actually valid, but some are still errors that could be
corrected with more effort.

```{r view_final_bad}
md %>%
  filter(city_clean %out% valid_city3) %>% 
  count(city_clean, state_clean, city_match, sort = TRUE) %>% 
  drop_na(city_clean)
```

However, this five-step process has allowed us to go from 
`r percent(prop_in(str_to_upper(md$city_sep), valid_city3))` of our (capitalized) `city_sep` values
being recognized as valid, to over `r percent(prop_in(str_to_upper(md$city_clean), valid_city3))`.

```{r city_prog_final}
progress_table(
  str_to_upper(md$city_sep),
  md$city_norm,
  md$city_swap,
  md$city_refine,
  md$city_clean,
  compare = valid_city3
) -> progress_table
```

```{r fix_prog_stage, echo=FALSE}
progress_table$stage <- str_remove(progress_table$stage, "\\)")
```

```{r print_progress, echo=FALSE}
progress_table %>% 
  mutate_at(vars(2, 4), percent) %>% 
  mutate_at(vars(3, 5, 6), comma) %>% 
  kable(
    format = "markdown", 
    digits = 4,
    col.names = c(
      "Stage", 
      "Percent Valid",
      "Total Distinct", 
      "Prop NA",
      "Total Invalid",
      "Unique Invalid"
    )
  )
```

```{r wrangle_bar_prop, echo=FALSE}
brewer_dark2 <- RColorBrewer::brewer.pal(8, "Dark2")
progress_table %>% 
  mutate(stage = as_factor(stage)) %>% 
  ggplot(aes(x = stage, y = prop_in)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = brewer_dark2[2]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Maryland Expenditures Payee City Progress",
    subtitle = "Percent of total values contained in pre-defined list of cities",
    caption = "Source: Maryland Campaign Reporting Information System",
    x = "Wrangling Stage",
    y = "Proportion Valid Cities"
  )
```

```{r wrangle_bar_distinct, echo=FALSE}
progress_table %>% 
  mutate(stage = as_factor(stage)) %>% 
  select(-prop_in, -prop_na, -n_out) %>% 
  rename(
    All = n_distinct,
    Invalid = n_diff
  ) %>% 
  gather(
    -stage,
    key = "key",
    value = "value"
  ) %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = key)) +
  scale_fill_brewer(type = "qual", palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  labs(
    title = "Maryland Expenditures Payee City Progress",
    subtitle = "Total distinct number of city values",
    caption = "Source: Maryland Campaign Reporting Information System",
    fill = "Distinct Values",
    x = "Wrangling Stage",
    y = "Number of Expenditures"
  )
```

## Explore

Now that the data is sufficiently wrangled and normalized, we should explore it a little for
consistency issues.

```{r glimpse}
head(md)
tail(md)
glimpse(sample_frac(md))
```

### Missing

There are a number of records missing one of the four key variables we need to identify a unique
transaction (primarily `payee_name`).

```{r glimpse_na}
glimpse_fun(md, count_na)
```

We will flag these records with a new `na_flag` variable using the `campfin::flag_na()` function.

```{r flag_na}
md <- md %>% flag_na(expenditure_date, amount, payee_name, committee_name)
sum(md$na_flag)
percent(mean(md$na_flag))
```

### Duplicates

There are also a number of records that are complete duplicates of another row. It's possible that
a campaign made multiple valid expenditures for the same amount, to the same vendor, on the same
day. However, we will flag these two variables just to be safe. We can create a new `dupe_flag`
variable using the `campfin::flag_dupes()` function.

```{r flag_dupes}
md <- flag_dupes(md, everything())
sum(md$dupe_flag)
percent(mean(md$dupe_flag))
```

```{r view_dupes}
md %>% 
  filter(dupe_flag) %>% 
  select(expenditure_date, amount, payee_name, committee_name) %>% 
  arrange(expenditure_date)
```

### Categorical

```{r glimpse_distinct}
glimpse_fun(md, n_distinct)
```

```{r payee_type_bar, echo=FALSE}
explore_plot(
  data = drop_na(md, payee_type),
  var = payee_type,
  title = "Maryland Expenditure Count by Payee Type",
  caption = "Source: Maryland Campaign Reporting Information System",
  x = "Amount",
  y = "Count"
)
```

```{r expense_category_bar, echo=FALSE}
explore_plot(
  data = drop_na(md, expense_category),
  var = expense_category,
  title = "Maryland Expenditure Count by Expense Category",
  caption = "Source: Maryland Campaign Reporting Information System",
  x = "Amount",
  y = "Count"
) + theme(axis.text.x = element_text(angle = 10, vjust = 1, hjust = 1))
```

```{r expense_method_bar, echo=FALSE}
explore_plot(
  data = drop_na(md, expense_method),
  var = expense_method,
  title = "Maryland Expenditure Count by Expense Methods",
  caption = "Source: Maryland Campaign Reporting Information System",
  x = "Amount",
  y = "Count"
)
```

### Continuous

#### Amounts

```{r summary_amount}
summary(md$amount)
sum(md$amount <= 0)
sum(md$amount > 1000000)
```

```{r amount_histogram, echo=FALSE}
md %>%
  ggplot(aes(amount)) +
  geom_histogram(fill = brewer_dark2[1]) +
  geom_vline(xintercept = median(md$amount)) +
  scale_x_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "Maryland Expenditure Amount Distribution",
    caption = "Source: Maryland Campaign Reporting Information System",
    x = "Amount",
    y = "Count"
  )
```

```{r amount_violin_category, echo=FALSE}
md %>%
  filter(expense_category %in% most_common(expense_category)) %>% 
  ggplot(
    mapping = aes(
      x = reorder(expense_category, amount), 
      y = amount
    )
  ) +
  geom_violin(
    mapping = aes(fill = expense_category),
    scale = "width"
  ) +
  scale_fill_brewer(palette = "Dark2", guide = FALSE) +
  scale_y_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "Maryland Expenditure Amount Distribution",
    caption = "Source: Maryland Campaign Reporting Information System",
    x = "Amount",
    y = "Count"
  ) + 
  theme(axis.text.x = element_text(angle = 10, vjust = 1, hjust = 1))
```

#### Dates

The `expenditure_date` is very clean, given that our data was downloaded from a search result 
bounded by 2008-01-01 and `lubridate::today()`.

```{r date_range, collapse=TRUE}
min(md$expenditure_date)
max(md$expenditure_date)
sum(md$expenditure_date > today())
```

We can use `lubridate::year()` to create an `expenditure_year` variable from the 
`expenditure_date`.

```{r add_year}
md <- mutate(md, expenditure_year = year(expenditure_date))
```

```{r year_bar_count, echo=FALSE}
md %>% 
  count(expenditure_year) %>% 
  mutate(even = is_even(expenditure_year)) %>% 
  ggplot(aes(x = expenditure_year, y = n)) +
  geom_col(aes(fill = even)) +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(breaks = 2008:2019) +
  theme(legend.position = "bottom") +
  labs(
    title = "Maryland Expenditures Count by Year",
    caption = "Source: Maryland Campaign Reporting Information System",
    fill = "Election Year",
    x = "Year Made",
    y = "Number of Expenditures"
  )
```

```{r year_bar_median, echo=FALSE}
md %>% 
  group_by(expenditure_year) %>% 
  summarize(median = median(amount)) %>% 
  mutate(even = is_even(expenditure_year)) %>% 
  ggplot(aes(x = expenditure_year, y = median)) +
  geom_col(aes(fill = even)) +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = dollar) +
  scale_x_continuous(breaks = 2008:2019) +
  theme(legend.position = "bottom") +
  labs(
    title = "Maryland Expenditures Median Amount by Year",
    caption = "Source: Maryland Campaign Reporting Information System",
    fill = "Election Year",
    x = "Year Made",
    y = "Median Expenditure"
  )
```

```{r year_bar_sum, echo=FALSE}
md %>% 
  group_by(expenditure_year) %>% 
  summarize(sum = sum(amount)) %>% 
  mutate(even = is_even(expenditure_year)) %>% 
  ggplot(aes(x = expenditure_year, y = sum)) +
  geom_col(aes(fill = even)) +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = dollar) +
  scale_x_continuous(breaks = 2008:2019) +
  theme(legend.position = "bottom") +
  labs(
    title = "Maryland Expenditures Total Amount by Year",
    caption = "Source: Maryland Campaign Reporting Information System",
    fill = "Election Year",
    x = "Year Made",
    y = "Median Expenditure"
  )
```

```{r year_line_sum, echo=FALSE}
md %>% 
  mutate(
    month = month(expenditure_date),
    even = is_even(expenditure_year)
  ) %>% 
  group_by(month, even) %>% 
  summarize(sum = sum(amount)) %>% 
  ggplot(aes(x = month, y = sum)) +
  geom_line(aes(color = even), size = 2) +
  scale_color_brewer(palette = "Dark2") +
  scale_y_continuous(labels = dollar) +
  scale_x_continuous(breaks = 1:12, labels = month.abb) +
  theme(legend.position = "bottom") +
  labs(
    title = "Maryland Expenditures Total Amount by Year",
    caption = "Source: Maryland Campaign Reporting Information System",
    color = "Election Year",
    x = "Year Made",
    y = "Median Expenditure"
  )
```

## Conclude

1. There are `r nrow(md)` records in the database.
1. There are `r sum(md$dupe_flag)` duplicate records in the database.
1. The range and distribution of `amount` and `date` seem reasonable.
1. There are `r sum(md$na_flag)` records missing a `payee_name`.
1. Consistency in geographic data has been improved with `campfin::normal_*()`.
1. The 5-digit `zip_norm` variable has been created with `campfin::normal_zip()`.
1. The 4-digit `expenditure_year` variable has been created with `lubridate::year()`.

## Export

```{r create_proc_dir}
proc_dir <- here("md", "expends", "data", "processed")
dir_create(proc_dir)
```

```{r write_clean}
md %>% 
  select(
    -address_sep,
    -zip_sep,
    -state_sep,
    -state_norm,
    -city_sep,
    -city_norm,
    -city_match,
    -city_swap,
    -city_swap,
    -match_dist,
    -match_abb,
    -city_refine,
  ) %>% 
  write_csv(
    path = glue("{proc_dir}/md_expends_clean.csv"),
    na = ""
  )
```
