---
title: "Florida Expenditures"
author: "Kienan Nicholls"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This package contains
functions custom made to help facilitate the processing of campaign finance data.

```{r load_packages, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_current_gh("irworkshop/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  RSelenium, # remote browser
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tidytext, # string analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  scales, # format strings
  knitr, # knit documents
  vroom, # read files fast
  glue, # combine strings
  here, # relative storage
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

Data is obtained from the Florida Division of Elections.

As the [agency home page][03] explains:

> By Florida law, campaigns, committees, and electioneering communications organizations are
required to disclose detailed financial records of campaign contributions and expenditures. Chapter
106, Florida Statutes, regulates campaign financing for all candidates, including judicial
candidates, political committees, electioneering communication organizations, affiliated party
committees, and political parties. It does not regulate campaign financing for candidates for
federal office.

[03]: https://dos.myflorida.com/elections/candidates-committees/campaign-finance/ "source"

### About

A more detailed description of available data can be found on the [Campaign Finance page][04]:

[04]: https://dos.myflorida.com/elections/candidates-committees/campaign-finance/campaign-finance-database/

> #### Quality of Data
> 
> The information presented in the campaign finance database is an accurate representation of the reports filed with the Florida Division of Elections.
> 
> Some of the information in the campaign finance database was submitted in electronic form, and
some of the information was key-entered from paper reports. Sometimes items which are not
consistent with filing requirements, such as incorrect codes or incorrectly formatted or blank
items, are present in the results of a query. They are incorrect in the database because they were
incorrect on reports submitted to the division.

> #### What does the Database Contain?
> 
> By law candidates and committees are required to disclose detailed financial records of
contributions received and expenditures made. For committees, the campaign finance database
contains all contributions and expenditures reported to the Florida Division of Elections since
January 1, 1996. For candidates, the campaign finance database contains all contributions and
expenditures reported to the Division since the candidacy was announced, beginning with the 1996
election.

> #### Whose Records are Included?
> 
> Included are campaign finance reports which have been filed by candidates for any multi-county
office, with the exception of U.S. Senator and U.S. Representative, and by organizations that
receive contributions or make expenditures of more than $500 in a calendar year to support or
oppose any multi-county candidate, issue, or party. To obtain reports from local county or
municipal candidates and committees, contact county or city filing offices.

> #### When are the Records Available?
> 
> Campaign finance reports are posted to the database as they are received from the candidates and
committees. Our data is as current as possible, consistent with the reporting requirements of
Florida law.

## Import

### Download

We will use the [Expenditure Records][05] querey form to download three separate files covering all
campaign expenditures. [The previous page][04] lists instructions on how to download the desired
files:

> #### How to Use the Campaign Finance Database
> 
> 1. Specify a subset of the [Expenditure]...
> 2. Select an election year entry from the list box
> 3. Select a candidate/committee option: 
> 4. Select contribution criteria (for Detail report only): 
> 5. Select how you would like the records sorted. 
> 6. Select the format in which you would like the data returned. 
> 7. Limit the number of records to return. 
> 8. Click on the Submit Query button.

To get all files covering all expenditures:

1. Select "All" from the **Election Year** drop down menu
1. In the **From Date Range** text box, enter "01/01/2008"
1. Delete "500" from the **Limit Records** text box
1. Select "Return Results in a Tab Delimited Text File" **Retrieval Format** option
1. Save to the `/fl/expends/data/raw` directory

We can automate this process using the `RSelenium` package:

```{r raw_dir}
# create a directory for the raw data
raw_dir <- here("fl", "expends", "data", "raw")
dir_create(raw_dir)
```

```{r download_raw, warning=FALSE, error=FALSE, message=FALSE, collapse=TRUE, eval=FALSE}
# open the driver with auto download options
remote_driver <- rsDriver(
  port = 4444L,
  browser = "firefox",
  extraCapabilities = makeFirefoxProfile(
    list(
      browser.download.dir = raw_dir,
      browser.download.folderList = 2L,
      browser.helperApps.neverAsk.saveToDisk = "text/txt"
    )
  )
)

# navigate to the FL DOE download site
remote_browser <- remote_driver$client
expends_url <- "https://dos.elections.myflorida.com/campaign-finance/expenditures/"
remote_browser$navigate(expends_url)

# chose "All" from elections list
year_menu <- "/html/body/div/div[1]/div/div/div/div/div/div/div/div/form/select[1]/option[@value = 'All']"
remote_browser$findElement("xpath", year_menu)$clickElement()

# remove the records limit text of 500
limit_box <- "div.marginBot:nth-child(64) > input:nth-child(1)"
remote_browser$findElement("css", limit_box)$clearElement()

# enter Jan 1 2008 as start date
date_box <- "div.indent:nth-child(2) > input:nth-child(1)"
remote_browser$findElement("css", date_box)$sendKeysToElement(list("01/01/2008"))

# chose "txt" as export option
txt_button <- "ul.noBullet:nth-child(70) > li:nth-child(2) > input:nth-child(1)"
remote_browser$findElement("css", txt_button)$clickElement()

# click the submit button
submit_button <- "#rightContent > form:nth-child(6) > div:nth-child(71) > input:nth-child(2)"
remote_browser$findElement("css", submit_button)$clickElement()

# close the browser and driver
remote_browser$close()
remote_driver$server$stop()
```

[05]: https://dos.elections.myflorida.com/campaign-finance/expenditures/

### Read

```{r read_raw}
fl <- 
  read_delim(
    file = dir_ls(path = raw_dir),
    delim = "\t",
    escape_double = FALSE,
    escape_backslash = FALSE,
    trim_ws = TRUE,
    col_types = cols(
      .default = col_character(),
      Date = col_date("%m/%d/%Y"),
      Amount = col_double()
    )
  ) %>% 
  select(-starts_with("X")) %>% 
  clean_names() %>% 
  mutate_if(is_character, str_to_upper)
```

## Explore

```{r glimpse}
head(fl)
tail(fl)
glimpse(fl)
```

### Categorical

We can explore the least distinct variables with `ggplot::geom_bar()` or perform tidytext analysis
on complex character strings.

```{r n_distinct}
glimpse_fun(fl, n_distinct)
```

```{r type_bar, echo=FALSE}
fl %>% 
  count(type, sort = TRUE) %>% 
  ggplot(aes(reorder(type, n), n)) +
  geom_col(fill = rep(RColorBrewer::brewer.pal(8, "Dark2"), length.out = 19)) +
  scale_y_log10() +
  coord_flip() +
  labs(
    title = "Florida Expenditures by Type",
    x = "Type",
    y = "Count",
    caption = "Source: Florida Dept. of State"
  )
```

```{r purpose_bar, echo=FALSE}
fl %>% 
  unnest_tokens(word, purpose) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE) %>% 
  head(20) %>% 
  ggplot(aes(reorder(word, n), n)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Florida Expenditure Purpose by Word Count",
    x = "Word",
    y = "Count",
    caption = "Source: Florida Dept. of State"
  )
```

### Continuous

```{r}
fl <- mutate(fl, year = year(date))
```

```{r amount_hist, echo=FALSE}
fl %>% 
  ggplot(aes(amount)) +
  geom_histogram(fill = RColorBrewer::brewer.pal(3, "Dark2")[1]) +
  scale_x_continuous(labels = scales::dollar, trans = "log10") +
  labs(
    title = "Florida Expenditures by Amount",
    x = "Amount",
    y = "Count",
    caption = "Source: Florida Dept. of State"
  )
```

```{r year_bar, echo=FALSE}
fl %>% 
  filter(year > 2008, date < today()) %>% 
  count(year) %>% 
  mutate(even = is_even(year)) %>% 
  ggplot(aes(year, n)) +
  geom_col(aes(fill = even)) +
  scale_fill_brewer(palette = "Dark2") +
  scale_x_continuous(breaks = 2008:2019) +
  theme(legend.position = "bottom") +
  labs(
    title = "Florida Expenditures by Year",
    x = "Year",
    y = "Count",
    caption = "Source: Florida Dept. of State",
    fill = "Election Year"
  )
```

```{r month_line, echo=FALSE}
fl %>% 
  mutate(even = is_even(year)) %>% 
  group_by(even, month = month(date)) %>% 
  filter(year > 2008, year < 2019) %>% 
  summarise(mean = mean(amount)) %>% 
  ggplot(mapping = aes(x = month, y = mean)) +
  geom_line(aes(color = even), size = 2) +
  scale_x_continuous(breaks = 1:12, labels = month.abb) +
  scale_color_brewer(palette = "Dark2") +
  theme(legend.position = "bottom") +
  labs(
    title = "Florida Expenditures by Month",
    x = "Month",
    y = "Count",
    caption = "Source: Florida Dept. of State",
    color = "Election Year"
  )
```

### Duplicates

The `flag_dupes()` function can flag records with duplicate values across every variable.

```{r get_dupes, collapse=TRUE}
fl <- flag_dupes(fl, everything())
sum(fl$dupe_flag)
mean(fl$dupe_flag)
```

### Missing

There are a number of rows missing key information.

```{r count_na}
glimpse_fun(fl, count_na)
```

The `flag_na()` function can flag records missing values key values in any key variable.

```{r flag_na}
fl <- flag_na(fl, payee_name, candidate_committee, date, amount)
sum(fl$na_flag)
```

## Clean

We need to separate the `city_state_zip` variable into their respective variables. Then we can
clean each part.

```{r separate_geo}
fl <- fl %>% 
  separate(
    col = city_state_zip,
    into = c("city_sep", "state_zip"),
    sep = ",\\s",
    remove = FALSE
  ) %>% 
  separate(
    col = state_zip,
    into = c("state_sep", "zip_sep"),
    sep = "\\s",
    remove = TRUE
  )
```

### Address

The database seems to use repeating astricks characters as `NA` values. We can remove any value
with a single repeating character.

```{r norm_address}
fl <- fl %>% 
  mutate(
    address_clean = normal_address(
      address = address,
      add_abbs = usps_street,
      na_rep = TRUE
    )
  )
```

### Zip

```{r bad_zip, collapse=TRUE}
sample(fl$zip_sep[which(nchar(fl$zip_sep) != 5)], 10)
```

```{r norm_zip}
fl <- fl %>% mutate(zip_clean = normal_zip(zip_sep, na_rep = TRUE))
```

```{r zip_progress}
progress_table(
  fl$zip_sep,
  fl$zip_clean,
  compare = valid_zip
)
```

### State

```{r norm_state}
fl <- fl %>% mutate(
  state_clean = normal_state(
    state = state_sep,
    abbreviate = TRUE,
    valid = NULL
  )
)
```

```{r state_progress}
progress_table(
  fl$state_sep,
  fl$state_clean,
  compare = valid_state
)
```

```{r view_state}
fl %>% 
  filter(state_clean %out% valid_state) %>% 
  count(state_clean, sort = TRUE)
```

```{r manual_state}
fl$state_clean <- str_replace(fl$state_clean, "^F$", "FL")
fl$state_clean <- na_out(fl$state_clean, valid_state)
```

### City

```{r city_normal}
fl <- fl %>% 
  mutate(
    city_norm = normal_city(
      city = city_sep,
      na = invalid_city,
      st_abbs = c("FL", "DC"),
      geo_abbs = usps_city,
      na_rep = TRUE
    )
  )

n_distinct(fl$city_norm)
```

```{r city_match}
fl <- fl %>% 
  left_join(
    y = zipcodes, 
    by = c(
      "zip_clean" = "zip", 
      "state_clean" = "state"
    )
  ) %>% 
  rename(city_match = city)
```

```{r city_swap}
fl <- fl %>% 
  mutate(
    match_abb = is_abbrev(city_norm, city_match),
    match_dist = str_dist(city_norm, city_match),
    city_swap = if_else(
      condition = match_abb | match_dist <= 2,
      true = city_match,
      false = city_norm
    )
  )

summary(fl$match_dist)
sum(fl$match_dist == 1, na.rm = TRUE)
```

```{r city_refine}
fl_refine <- fl %>% 
  filter(state_clean == "FL") %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge() %>% 
      n_gram_merge()
  ) %>% 
  filter(city_refine != city_swap) %>% 
  inner_join(
    y = zipcodes, 
    by = c(
      "city_swap" = "city",
      "zip_clean" = "zip", 
      "state_clean" = "state"
    )
  )

fl_refine %>% 
  count(
    city_swap, 
    city_refine,
    sort = TRUE
  )
```

```{r city_join}
fl <- fl %>% 
  left_join(fl_refine) %>% 
  mutate(city_clean = coalesce(city_refine, city_swap))
```

We will check a few remaining cities by hand. The 20 most common cities not in
`valid_city` are actually valid cities.

```{r city_bad}
bad_city <- fl$city_clean[which(fl$city_clean %out% valid_city)]
more_city <- most_common(bad_city, n = 20)
print(more_city)
```

```{r city_progress}
progress <-
  progress_table(
    fl$city_sep,
    fl$city_norm,
    fl$city_swap,
    fl$city_clean,
    compare = c(valid_city, more_city)
  ) %>% 
  mutate(stage = as_factor(stage))
```

```{r progress_print, echo=FALSE}
print(progress)
```

You can see how the percentage of valid values increased with each stage.

```{r progress_bar, echo=FALSE}
progress %>% 
  ggplot(aes(x = stage, y = prop_in)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = RColorBrewer::brewer.pal(3, "Dark2")[3]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Florida City Normalization Progress",
    x = "Stage",
    y = "Percent Valid",
    caption = "Source: Florida Dept. of State"
  )
```

More importantly, the number of distinct values decreased each stage. We were
able to confidently change many distinct invalid values to their valid
equivilent.

```{r distinct_bar}
progress %>% 
  select(
    stage, 
    all = n_distinct,
    bad = n_diff
  ) %>% 
  mutate(good = all - bad) %>% 
  pivot_longer(c("good", "bad")) %>% 
  mutate(name = name == "good") %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = name)) +
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  theme(legend.position = "bottom") +
  labs(
    title = "Florida City Normalization Progress",
    subtitle = "Distinct values, valid and invalid",
    x = "Stage",
    y = "Percent Valid",
    caption = "Source: Florida Dept. of State",
    fill = "Valid"
  )
  
```

## Lookup

If there is a lookup file, we can add that stage too.

```{r lookup, eval=TRUE}
lookup_file <- here("fl", "expends", "data", "fl_expends_city_lookup.csv")
if (file.exists(lookup_file)) {
  lookup <- read_csv(lookup_file) %>% clean_names()
  fl <- left_join(fl, select(lookup, 1, 2))
  progress_table(
    fl$city_sep,
    fl$city_norm,
    fl$city_swap,
    fl$city_clean,
    fl$city_clean2,
    compare = c(valid_city, more_city)
  ) 
}
```

## Export

```{r write_clean}
clean_dir <- here("fl", "expends", "data", "processed")
dir_create(clean_dir)
fl %>% 
  select(
    -city_state_zip,
    -city_sep,
    -state_sep,
    -zip_sep,
    -city_norm,
    -city_match,
    -match_dist,
    -match_abb,
    -city_swap,
    -city_refine
  ) %>% 
  write_csv(
    path = glue("{clean_dir}/fl_expends_clean.csv"),
    na = ""
  )
```



