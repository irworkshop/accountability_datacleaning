---
title: "Minnesota Contributions"
author: "Kiernan Nicholls"
date: "`r date()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 3
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
if (!interactive()) {
  options(width = 120)
  set.seed(5)
}
```

```{r create_docs_dir, eval=FALSE, echo=FALSE, include=FALSE}
doc_dir <- fs::dir_create(here::here("mn", "contribs", "docs"))
```

## Project

The Accountability Project is an effort to cut across data silos and give
journalists, policy professionals, activists, and the public at large a simple
way to search across huge volumes of public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each
dataset row as a transaction. For each transaction there should be (at least) 3
variables:

1. All **parties** to a transaction.
2. The **date** of the transaction.
3. The **amount** of money involved.

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for entirely duplicated records.
1. Check ranges of continuous variables.
1. Is there anything blank or missing?
1. Check for consistency issues.
1. Create a five-digit ZIP Code called `zip`.
1. Create a `year` field from the transaction date.
1. Make sure there is data on both parties to a transaction.

## Packages

The following packages are needed to collect, manipulate, visualize, analyze,
and communicate these results. The `pacman` package will facilitate their
installation and attachment.

```{r load_packages, message=FALSE, warning=FALSE, error=FALSE}
if (!require("pacman")) {
  install.packages("pacman")
}
pacman::p_load(
  tidyverse, # data manipulation
  lubridate, # datetime strings
  gluedown, # printing markdown
  janitor, # clean data frames
  campfin, # custom irw tools
  aws.s3, # aws cloud storage
  refinr, # cluster & merge
  scales, # format strings
  knitr, # knit documents
  vroom, # fast reading
  rvest, # scrape html
  glue, # code strings
  here, # project paths
  httr, # http requests
  fs # local storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a
sub-directory of the more general, language-agnostic
[`irworkshop/accountability_datacleaning`][tap] GitHub repository.

The `R_campfin` project uses the [RStudio projects][rproj] feature and should be
run as such. The project also uses the dynamic `here::here()` tool for file
paths relative to _your_ machine.

```{r where_here}
# where does this document knit?
here::i_am("mn/contribs/docs/mn_contribs_diary.Rmd")
```

[tap]: https://github.com/irworkshop/accountability_datacleaning
[rproj]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

## Data

The data is obtained from the [Minnesota Campaign Finance Board (CFB)][cfb].

> The Campaign Finance and Public Disclosure Board was established by the state
legislature in 1974 and is charged with the administration of Minnesota
Statutes, Chapter 10A, the Campaign Finance and Public Disclosure Act, as well
as portions of Chapter 211B, the Fair Campaign Practices act.

> The Board's four major programs are campaign finance registration and
disclosure, public subsidy administration, lobbyist registration and disclosure,
and economic interest disclosure by public officials. The Board has six members,
appointed by the Governor on a bi-partisan basis for staggered four-year terms.
The appointments must be confirmed by a three-fifths vote of the members of each
house of the legislature.

[cfb]: https://cfb.mn.gov/

The CFB provides [direct data download][data] for all campaign finance data.

[data]: https://cfb.mn.gov/reports-and-data/self-help/data-downloads/campaign-finance/

## Import

To import the file for processing, we will first have save the file locally
and then read the flat file.

### Download

We can download the file to disk with the `httr::GET()` and `httr::write_disk()`
functions. These functions make the HTTP requests one would make when clicking
on the download link on the CFB page. 

```{r raw_download}
raw_dir <- dir_create(here("mn", "contribs", "data", "raw"))
raw_file <- path(raw_dir, "all_contribs.csv")
if (!file_exists(raw_file)) {
  GET(
    url = "https://cfb.mn.gov/",
    path = c("reports-and-data", "self-help", "data-downloads", "campaign-finance"),
    query = list(download = -2113865252),
    write_disk(raw_file, overwrite = TRUE),
  )
}
```

### Read

We can read this flat file with the `vroom::vroom()` function.

```{r raw_read}
mnc <- vroom(
  file = raw_file,
  .name_repair = make_clean_names,
  col_types = cols(
    .default = col_character(),
    `Recipient reg num` = col_integer(),
    Amount = col_double(),
    `Receipt date` = col_date_usa(),
    Year = col_integer(),
    `Contributor ID` = col_integer(),
    `Contrib Reg Num` = col_integer(),
    `Contrib employer ID` = col_integer()
  )
)
```

```{r raw_rename, echo=FALSE}
mnc <- mnc %>% 
  rename(
    rec_num = recipient_reg_num,
    rec_name = recipient,
    rec_type = recipient_type,
    rec_sub = recipient_subtype,
    date = receipt_date,
    year = year,
    con_name = contributor,
    con_id = contributor_id,
    con_reg = contrib_reg_num,
    con_type = contrib_type,
    receipt = receipt_type,
    in_kind = in_kind,
    in_kind_desc = in_kind_descr,
    con_zip = contrib_zip,
    con_emp_id = contrib_employer_id,
    con_emp_name = contrib_employer_name
  )%>% 
  mutate(
    in_kind = "Yes" == in_kind
  ) %>% 
  remove_empty("cols")
```

## Explore

The file has `r comma(nrow(mnc))` records of `r ncol(mnc)` variables.

```{r glimpse}
head(mnc)
tail(mnc)
glimpse(sample_n(mnc, 20))
```

### Missing

We should flag any variable missing the key variables needed to identify a 
unique contribution.

```{r na_count}
col_stats(mnc, count_na)
```

```{r na_flag}
mnc <- mnc %>% flag_na(rec_name, con_name, date, amount)
sum(mnc$na_flag)
```

```{r na_view}
mnc %>% 
  filter(na_flag) %>% 
  select(rec_name, con_name, date, amount) %>% 
  sample_frac()
```

### Duplicates

Similarly, we can flag all records that are duplicated at least one other time.

```{r dupe_flag}
mnc <- flag_dupes(mnc, everything())
sum(mnc$dupe_flag)
```

```{r dupe_view}
mnc %>% 
  filter(dupe_flag) %>% 
  select(rec_name, con_name, date, amount) %>% 
  arrange(rec_name)
```

### Categorical

We can explore the distribution of categorical variables.

```{r distinct_count}
col_stats(mnc, n_distinct)
```

```{r bar_rec_type, echo=FALSE}
explore_plot(
  data = filter(mnc, !is.na(rec_type)),
  var = rec_type
)
```

```{r bar_rec_sub, echo=FALSE}
explore_plot(
  data = filter(mnc, !is.na(rec_sub)),
  var = rec_sub
)
```

```{r bar_con_type, echo=FALSE}
explore_plot(
  data = filter(mnc, !is.na(con_type)),
  var = con_type
)
```

```{r cat_count, echo=FALSE}
count(mnc, receipt)
count(mnc, in_kind)
```

### Continuous

The range of continuous variables should be checked to identify any egregious
outliers or strange distributions.

#### Amounts

The range of the `amount` variable is reasonable, with very few contributions
at or less than zero dollars.

```{r ammount_summary}
summary(mnc$amount)
sum(mnc$amount <= 0)
```

As we'd expect, the contribution `amount` are log-normally distributed around 
the median value of `r dollar(median(mnc$amount))`.

```{r hist_amount, echo=FALSE}
mnc %>%
  ggplot(aes(amount)) +
  geom_histogram(fill = dark2["purple"]) +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "Minnesota Contributions Amount Distribution",
    caption = "Source: MC CFB",
    x = "Amount",
    y = "Count"
  )
```

#### Dates

Since the `year` variable already exists, there is no need to create one. Any of
these which do not match seems to fall near beginning of the year.

```{r date_year}
mean(mnc$year == year(mnc$date))
mnc %>% 
  filter(year != year(date)) %>% 
  count(month = month(date))
```

No further cleaning of the date variable is needed.

```{r date_range}
min(mnc$date)
sum(mnc$year < 2000)
max(mnc$date)
sum(mnc$date > today())
```

```{r bar_year, echo=FALSE}
mnc %>% 
  count(year) %>% 
  mutate(even = is_even(year)) %>% 
  ggplot(aes(x = year, y = n)) +
  geom_col(aes(fill = even)) + 
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(breaks = seq(2000, 2020, by = 2)) +
  theme(legend.position = "bottom") +
  labs(
    title = "Minnesota Contributions by Year",
    caption = "Source: {source}",
    fill = "Election Year",
    x = "Year Made",
    y = "Count"
  )
```

## Wrangle

The database does _not_ contain the full range of geographic variables we'd
expect. There is only a ZIP code. We can use this `zip` variable to add the
`city` and `state` variables, but not an `address`. These variables will _not_
be accurate to the data provided by the state.

```{r zip_join}
mnc <- mnc %>% 
  left_join(zipcodes, by = c("con_zip" = "zip")) %>% 
  rename_at(vars(19:20), ~str_replace(., "(.*)$", "cont_\\1_match"))
```

## Conclude

```{r clean_glimpse}
glimpse(sample_n(mnc, 100))
```

1. There are `r comma(nrow(mnc))` records in the database.
1. There are `r comma(sum(mnc$dupe_flag))` duplicate records in the database.
1. The range and distribution of `amount` and `date` seem reasonable.
1. There are `r comma(sum(mnc$na_flag))` records missing ....
1. Consistency in geographic data has been improved with `campfin::normal_*()`.
1. The 4-digit `year` variable has been created with `lubridate::year()`.

## Export

Now the file can be saved on disk for upload to the Accountability server.

```{r clean_dir}
clean_dir <- dir_create(here("mn", "contribs", "data", "clean"))
clean_path <- path(clean_dir, "mn_contribs_clean.csv")
write_csv(mnc, clean_path, na = "")
(clean_size <- file_size(clean_path))
```

## Upload

We can use the `aws.s3::put_object()` to upload the text file to the IRW server.

```{r aws_upload, eval=FALSE}
aws_path <- path("csv", basename(clean_path))
if (!object_exists(aws_path, "publicaccountability")) {
  put_object(
    file = clean_path,
    object = aws_path, 
    bucket = "publicaccountability",
    acl = "public-read",
    show_progress = TRUE,
    multipart = TRUE
  )
}
aws_head <- head_object(aws_path, "publicaccountability")
(aws_size <- as_fs_bytes(attr(aws_head, "content-length")))
unname(aws_size == clean_size)
```

## Dictionary

The following table describes the variables in our final exported file:

```{r dict_make, echo=FALSE}
dict_raw <- tibble(
  var = md_code(names(mnc)),
  type = md_code(map_chr(mnc, typeof)),
  def = c(
    "Recipient ID",
    "**Recipient name**",
    "Recipeint type",
    "Recipient sub-type",
    "**Amount** of contribution",
    "**Date** contribution made",
    "**Year** contribution made",
    "**Contributor name**",
    "Contributor ID",
    "Contributor registration",
    "Contributor type",
    "Receipt type",
    "Flag indicating in-kind contribution",
    "Description of in-kind contribution",
    "Contributor ZIP code",
    "Contributor employer name",
    "Flag indicating missing value",
    "Flag indicating duplicate record",
    "City name from _matched_ ZIP code",
    "State abbreviation from _matched_ ZIP code"
  )
)
```

```{r dict_md, echo=FALSE}
(dict_md <- kable(
  x = dict_raw,
  format = "markdown",
  col.names = c("Column", "Type", "Definition")
))
