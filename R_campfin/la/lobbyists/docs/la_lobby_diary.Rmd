---
title: "Louisiana Lobbying Registration Data Diary"
author: "Yanqi Xu"
date: "`r format(Sys.time())`"
output:
   github_document:    
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
set.seed(5)
```

```{r create_docs_dir, eval=FALSE, echo=FALSE, include=FALSE}
fs::dir_create(here::here("la", "lobbyists", "docs"))
```
## Project


The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.


Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:


1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved


## Objectives


This document describes the process used to complete the following objectives:


1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction


## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

```{r p_load, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  tidyverse, # data manipulation
  readxl, # import excel files
  lubridate, # datetime strings
  tidytext, # string analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  knitr, # knit documents
  glue, # combine strings
  scales, #format strings
  here, # relative storage
  fs, # search storage 
  vroom, #read deliminated files
  readxl #read excel files
)
```

```{r fix_fun, echo=FALSE, collapse = TRUE}
# fix conflict
here <- here::here
```
This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.


The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.


```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

## Download
Set the data directory first.
```{r raw_dir}
# create a directory for the raw data
reg_dir <- here("la", "lobbyists", "data", "raw", "reg")
dir_create(reg_dir)
```
The [Louisiana Board of Ethics] [03] makes available a listing of all current lobbyists (2019). For more detailed representation data, the Accountability Project obtained records of previous years through a public record request.

[03]: http://ethics.la.gov/LobbyistLists.aspx
[05]: http://ethics.la.gov/Pub/Laws/Title24LegislativeLobbying.pdf
[06]: http://ethics.la.gov/LobbyistData/ResultsByFinancialDisclosure.aspx?SearchParams=PSGrp.%7b1%7d&OrderBy=1


[La. Stat. Ann. ยง 24:53.][05] regulates lobbyists registration as such.
> Each lobbyist shall register with the board as soon as possible after employment as a lobbyist or after
the first action requiring his registration as a lobbyist, whichever occurs first, and in any event not later than
five days after employment as a lobbyist or not later than five days after the first action requiring his
registration as a lobbyist, whichever occurs first. 


# Reading
```{r read csv}
la_reg <- read_delim(glue("{reg_dir}/lobby_reg.csv"), delim = ",", col_types = cols(.default = col_character()),escape_double = FALSE, escape_backslash = FALSE) %>% 
  clean_names() 
```

We will notice that some rows were delimited incorrectly, as a supposedly single rows is separated into two lines with the first row of the overflow line an invalid forward slash.
```{r}
reg_dir <- dir_create(here("la", "lobbyists", "data", "raw", "reg"))

la_lines <- read_lines(glue("{reg_dir}/lobby_reg.csv"))
la_cols <- str_split(la_lines[1], ",", simplify = TRUE)
la_lines <- la_lines[-1]

sum(str_detect(la_lines, "^\\D"))
#> 1092

for (i in rev(seq_along(la_lines))) {
  if (is.na(la_lines[i])) {
    next()
  }
  if (str_detect(la_lines[i], "^\\D")) {
    la_lines[i - 1] <- str_c(la_lines[i - 1], la_lines[i], collapse = ",")
    la_lines[i] <- NA_character_
  }
}

la_lines <- na.omit(la_lines)

la <- read_csv(la_lines, col_names = la_cols)
```

## Duplicates

We'll use the `flag_dupes()` function to see if there are records identical to one another and flag the duplicates. A new variable `dupe_flag` will be created.

```{r flag dupe}
la_reg <- flag_dupes(la_reg, dplyr::everything())
```

## Missing
```{r glimpse_na, collapse=T}
col_stats(la_reg, count_na)
```
## Explore

```{r}
summary(la_reg$earliest_registration_date)
summary(la_reg$latest_termination_date)
```

### Year
```{r bar reg year, echo=F}
la_reg %>% 
  ggplot(aes(x=year_registered)) +
  geom_bar(fill = RColorBrewer::brewer.pal(3, "Dark2")[3]) +
  labs(
    title = "Louisiana Lobbyists Registered by Year",
    caption = "Source: Louisiana Ethics Administration Program"
  ) +
  theme_minimal()
```


## Wrangling

### ZIP 
Running the following commands tells us the zipcode fields are mostly clean.
```{r client normal zip}
prop_in(la_reg$mailing_zip_string , valid_zip, na.rm = TRUE) %>% percent()
prop_in(la_reg$employer_zip_string , valid_zip, na.rm = TRUE) %>% percent()
la_reg <- la_reg %>% 
  mutate_at(
    .vars = vars(contains("zip")),
    .fun = list(norm = normal_zip),
    na_rep = TRUE
  )

progress_table(la_reg$mailing_zip_string,
               la_reg$mailing_zip_string_norm,
               la_reg$employer_zip_string,
               la_reg$employer_zip_string_norm,
               compare = valid_zip)
```

### State
The state fields use the regular spelling of states, and we'll use `normal_state` to transform them into two-letter abbreviations.
```{r clean state}
la_reg <- la_reg %>% 
    mutate_at(
    .vars = vars(ends_with("state")),
    .fun = list(norm = normal_state),
    na_rep = TRUE
  )

progress_table(la_reg$mailing_state,
               la_reg$mailing_state_norm,
               la_reg$employer_state,
               la_reg$employer_state_norm,
               compare = valid_state)
```

### City

#### Prep
```{r prep_city, collapse = TRUE}
la_reg <- la_reg %>% mutate_at(
    .vars = vars(ends_with("city")),
    .fun = list(norm = normal_city),
    st_abbs = usps_state,
    geo_abbs = usps_city,
    na = invalid_city,
    na_rep = TRUE
  )

```

#### Swap
Then, we will compare these normalized `city_norm` values to the _expected_ city value for that
vendor's ZIP code. If the [levenshtein distance][09] is less than 3, we can confidently swap these
two values.

[09]: https://en.wikipedia.org/wiki/Levenshtein_distance

```{r swap_city}
la_reg <- la_reg %>% 
  left_join(
    y = zipcodes,
    by = c(
      "employer_state_norm" = "state",
      "employer_zip_string_norm" = "zip"
    )
  ) %>% 
  rename(employer_city_match = city) %>% 
  mutate(
  match_abb = is_abbrev(employer_city_norm, employer_city_match),
    match_dist = str_dist(employer_city_norm, employer_city_match),
    employer_city_swap = if_else(
      condition = match_abb | match_dist == 1,
      true = employer_city_match,
      false = employer_city_norm
    )
  ) %>% 
  select(
    -employer_city_match,
    -match_dist,
    -match_abb
  )

prop_in(la_reg$employer_city_swap, valid_city, na.rm = TRUE)

```

```{r}
la_reg <- la_reg %>% 
  left_join(
    y = zipcodes,
    by = c(
      "mailing_state_norm" = "state",
      "mailing_zip_string_norm" = "zip"
    )
  ) %>% 
  rename(mailing_city_match = city) %>% 
  mutate(
  match_abb = is_abbrev(mailing_city_norm, mailing_city_match),
    match_dist = str_dist(mailing_city_norm, mailing_city_match),
    mailing_city_swap = if_else(
      condition = match_abb | match_dist == 1,
      true = mailing_city_match,
      false = mailing_city_norm
    )
  ) %>% 
  select(
    -mailing_city_match,
    -match_dist,
    -match_abb
  )
prop_in(la_reg$mailing_city_swap, valid_city, na.rm = TRUE)
```

This is a very fast way to increase the valid proportion of modified `mailing_city` to
`r percent(prop_in(la_reg$mailing_city_swap, valid_city, na.rm = TRUE))` and reduce the number of distinct
_invalid_ values from `r length(setdiff(la_reg$mailing_city_norm, valid_city))` to only
`r length(setdiff(la_reg$mailing_city_swap, valid_city))`


#### Check

```{r check mail}
api_key <- Sys.getenv("GEOCODING_API")

valid_place <-  c(valid_city, extra_city) %>% unique()

la_mail_check <- la_reg %>% 
  filter(mailing_city_swap %out% valid_place) %>% 
  drop_na(mailing_city_swap, mailing_state_norm) %>% 
  count(mailing_city_swap, mailing_state_norm)

la_mail_check_result <- 
  pmap_dfr(.l = list(city = la_mail_check$mailing_city_swap, state = la_mail_check$mailing_state_norm), .f = check_city, key = api_key, guess = T)

la_mail_check <- la_mail_check %>% 
  left_join(la_mail_check_result %>% select(-original_zip), by = c("mailing_city_swap" = "original_city", "mailing_state_norm" = "original_state"))
```

```{r emp_check}
la_emp_check <- la_reg %>% 
  filter(employer_city_swap %out% valid_place) %>% 
  drop_na(employer_city_swap, employer_state_norm) %>% 
  count(employer_city_swap, employer_state_norm)

la_emp_check_result <- 
  pmap_dfr(.l = list(city = la_emp_check$employer_city_swap, state = la_emp_check$employer_state_norm), .f = check_city, key = api_key, guess = T)

la_emp_check <- la_emp_check %>% 
  left_join(la_emp_check_result %>% select(-original_zip), by = c("employer_city_swap" = "original_city", "employer_state_norm" = "original_state"))
```


```{r add to extra_city, eval=FALSE}
extra_city_gs <- gs_title("extra_city")

extra_city_gs <- extra_city_gs %>% 
  gs_add_row(ws = 1, input = la_mail_check %>% filter(check_city_flag) %>% select(mailing_city_swap)) %>% 
  gs_add_row(ws = 1, input = la_emp_check %>% filter(check_city_flag) %>% select(mailing_city_swap))
```

```{r join check mail, echo=TRUE}
valid_place <-  c(valid_place,la_mail_check$mailing_city_swap[la_mail_check$check_city_flag]) %>% unique()

la_reg <- la_mail_check %>% select(mailing_city_swap, mailing_state_norm, guess_place) %>% 
  right_join(la_reg, by = c("mailing_city_swap","mailing_state_norm"))
  
la_reg <-  la_reg %>% mutate(mailing_city_clean = coalesce(guess_place, mailing_city_swap)) %>% select(-guess_place)
```

```{r join emp check, echo=TRUE}
valid_place <-  c(valid_place,la_emp_check$employer_city_swap[la_emp_check$check_city_flag]) %>% unique()

la_reg <- la_emp_check %>% select(employer_city_swap, employer_state_norm, guess_place) %>% 
  right_join(la_reg, by = c("employer_city_swap","employer_state_norm"))
  
la_reg <-  la_reg %>% mutate(employer_city_clean = coalesce(guess_place, employer_city_swap)) %>% select(-guess_place)
```

We've now increased the percentage of clean city names in valid city names.
```{r}
prop_in(la_reg$mailing_city_clean, valid_place, na.rm = TRUE)
prop_in(la_reg$employer_city_clean, valid_place, na.rm = TRUE)
```

## Address
```{r norm address}
la_reg <- la_reg %>% 
  mutate_at(
    .vars = vars(ends_with("street")),
    .fun = list(norm = normal_address),
    abbs = usps_street,
    na_rep = TRUE
  )
```


## Export

```{r write clean}
clean_reg_dir <- here("la", "lobbyists", "data", "processed", "reg")
dir_create(clean_reg_dir)
la_reg %>%
  select(-c(mailing_city_norm,
            employer_city_norm,
            mailing_city_swap,
            employer_city_swap)) %>%
  write_csv(path = glue("{clean_reg_dir}/la_reg_clean.csv"),
            na = "")
```

