---
title: "Maine Expenditures"
author: "Kiernan Nicholls"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This package contains
functions custom made to help facilitate the processing of campaign finance data.

```{r load_packages, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_current_gh("kiernann/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  snakecase, # change string case
  RSelenium, # remote browser
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tidytext, # text analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  batman, # rep(NA, 8) Batman!
  refinr, # cluster and merge
  scales, # format strings
  knitr, # knit documents
  vroom, # read files fast
  glue, # combine strings
  here, # relative storage
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

Data is obtained from the [Maine Ethics Commission][03]

> This page provides comma separated value (CSV) downloads of contribution, expenditure, and loan
data for each reporting year in a zipped file format. These files can be downloaded and imported
into other applications (Microsoft Excel, Microsoft Access, etc.) This data is extracted from the
Maine Ethics Commission database as it existed as of 08/12/2019 12:34 PM

The Ethics Commission also provides a [PDF file layout key][04] that can be used to identify the
role of each variable.

* `ORG ID` is the unique ID of the paying candidate or committee
* `LEGACY ID` is the unique ID of the recipient candidate of committee, for candidates or
committees that existed prior to 7/1/2018 (Old System prior to 7/1/2018)
* `LAST NAME` is the Last Name of Payee (entity paid), if an individual person. If not an
individual, the entity full name will be in LAST NAME field.

[03]: https://mainecampaignfinance.com/#/index
[04]: https://mainecampaignfinance.com/Template/KeyDownloads/ME%20Expenditures%20File%20Layout.pdf

### Download

Expenditures data is separated into annual files. The files can only be downloaded by clicking on 
the Data Download link.

```{r raw_dir}
raw_dir <- here("me", "expends", "data", "raw")
dir_create(raw_dir)
```

```{r download_raw, warning=FALSE, error=FALSE, message=FALSE, collapse=TRUE, eval=FALSE}
# open the driver with auto download options
remote_driver <- rsDriver(
  port = 4444L,
  browser = "firefox",
  extraCapabilities = makeFirefoxProfile(
    list(
      browser.download.dir = raw_dir,
      browser.download.folderList = 2L,
      browser.helperApps.neverAsk.saveToDisk = "text/csv"
    )
  )
)

# navigate to the download site
remote_browser <- remote_driver$client
remote_browser$navigate("https://mainecampaignfinance.com/index.html#/dataDownload")

rows_xpath <- str_c(
  "/html/body/section/div[2]/div[2]/div/div/div[8]/div/div/dir-pagination-controls",
  "/div/div[1]/table/tbody/tr/td[2]/div/select/option[3]"
)

remote_browser$findElement("xpath", rows_xpath)$clickElement()

children <- seq(from = 2, to = 28, by = 2)
selectors <- glue("tr.ng-scope:nth-child({children}) > td:nth-child(3) > a:nth-child(1)")

for (css in selectors) {
  remote_browser$findElement("css", css)$clickElement()
}

# close the browser and driver
remote_browser$close()
remote_driver$server$stop()
```

## Read

```{r read_names}
me08 <- scan(file = glue("{raw_dir}/EXP_2008.csv.csv"), sep = ",", what = "", nmax = 21)
me19 <- scan(file = glue("{raw_dir}/EXP_2019.csv.csv"), sep = ",", what = "", nmax = 39)
```

The files come in two structures. For files from 2008 to 2017, there are `r length(me08)` variables.
For the newer files, 2018 and 2019, there are `r length(me19)` variables.

```{r print_names, echo=FALSE, collapse=TRUE}
print(me08)
print(me19)
```

```{r names_in}
to_snake_case(str_replace(me08, "MI", "Middle Name")) %in% to_snake_case(me19)
```

```{r remove_names, echo=FALSE}
rm(me08, me19)
```

We can read each type of file into a separate data frame and then combine the two. Any of the 
new variables. There are 4 rows with a field containing double quoation marks. This causes
`readr::read_delim()` to incorectly shift values right one column. We have no choice but to filter
out these 4 records or manually edit the text file.

```{r read_raw}
me_old_format <- 
  dir_ls(raw_dir) %>% 
  extract(1:10) %>% 
  map(
    read_delim,
    delim = ",",
    escape_backslash = FALSE,
    escape_double = FALSE,
    col_types = cols(
      .default = col_character(),
      ExpenditureAmount = col_number(),
      ExpenditureDate = col_date_usa(),
      FiledDate = col_date_usa()
    )
  ) %>% 
  bind_rows() %>% 
  clean_names() %>% 
  remove_empty("cols") %>% 
  rename(middle_name = mi)

me_new_format <- 
  dir_ls(raw_dir) %>% 
  extract(11:12) %>% 
  map(
    read_delim,
    delim = ",",
    escape_backslash = FALSE,
    escape_double = TRUE,
    col_types = cols(
      .default = col_character(),
      `Expenditure Amount` = col_number(),
      `Expenditure Date` = col_date_usa(),
      `Filed Date` = col_date_usa()
    ) 
  ) %>% 
  bind_rows() %>% 
  clean_names() %>% 
  remove_empty("cols") %>% 
  select(seq(-30, -39))

me <- 
  bind_rows(me_old_format, me_new_format) %>% 
  mutate(amended = to_logical(amended)) %>% 
  filter(!is.na(amended))
```

## Explore

```{r glimpse}
head(me)
tail(me)
glimpse(sample_frac(me))
```

### Missing

```{r glimpse_na}
glimpse_fun(me, count_na)
```

We can use `campfin::flag_na()` to create a new `na_flag` variable to flag any record missing a
variable needed to identify the parties to a transaction.

```{r flag_na, collapse=TRUE}
me <- me %>% 
  mutate(expender_name = coalesce(committee_name, candidate_name)) %>% 
  flag_na(
  expenditure_amount, 
  expenditure_date,
  last_name,
  expender_name
)

sum(me$na_flag)
percent(mean(me$na_flag))
```

The _vast_ majority of records flagged with `na_flag` are from 2018, where 
`r percent(mean(me$na_flag[year(me$expenditure_date) == 2018]))` of that year's records are missing
some variable needed to identify the transaction.

```{r count_flags}
me %>% 
  group_by(year = year(expenditure_date)) %>% 
  summarize(
    rows = n(), 
    na = sum(na_flag), 
    prop_na = na/rows
  )
```

### Duplicates

If we ignore the (supposedly) unique `expenditure_id` variable, there are a number of duplicated
records. We can flag every duplicate record (after the first) with a new `dupe_flag` variable.

```{r get_dupes, collapse=TRUE}
me <- flag_dupes(me, -expenditure_id)
sum(me$dupe_flag)
```

```{r view_dupes, echo=FALSE}
filter(me, dupe_flag)
```

Duplicate records are not clearly isolated to a single expenditure year.

```{r count_dupes}
me %>% 
  group_by(year = year(expenditure_date)) %>% 
  summarize(dupes = sum(dupe_flag))
```

### Categorical

```{r glimpse_distinct}
glimpse_fun(me, n_distinct)
```

```{r committe_type_bar, echo=FALSE}
explore_plot(
  data = me,
  var = committee_type,
  title = "Maine Expentitures by Committee Type",
  subtitle = "From 2008 to 2019",
  caption = "Source: Maine Ethics Commission"
)
```

```{r finance_type_bar, echo=FALSE}
explore_plot(
  data = drop_na(me, candidate_financing_type),
  var = candidate_financing_type,
  title = "Maine Expentitures by Finance Type",
  subtitle = "From 2018 to 2019",
  caption = "Source: Maine Ethics Commission"
)
```

```{r office_bar, echo=FALSE}
explore_plot(
  data = drop_na(me, office),
  var = office,
  nbar = 6,
  title = "Maine Expentitures by Office Sought",
  subtitle = "From 2018 to 2019",
  caption = "Source: Maine Ethics Commission"
)
```

```{r explanation_word_bar, echo=FALSE}
me %>% 
  unnest_tokens(word, explanation) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE) %>% 
  drop_na(word) %>% 
  head(30) %>% 
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_col(aes(fill = n)) +
  coord_flip() +
  scale_fill_continuous(guide = FALSE) +
  labs(
    title = "Maine Expentiture Explanation Word Usage",
    subtitle = "From 2008 to 2019",
    caption = "Source: Maine Ethics Commission",
    x = "Word",
    y = "Count"
  )
```

### Continuous

For continuous variables, we should check both the range and distribution of values for 
plausability.

#### Amounts

```{r summary_amount, collapse=TRUE}
summary(me$expenditure_amount)
sum(me$expenditure_amount < 0)
percent(mean(me$expenditure_amount < 0))
```

From this summary, we can see a suspicious similarity between the minimum expenditure value of `r
dollar(min(me$expenditure_amount))` and the maximum value of `r
dollar(max(me$expenditure_amount))`. We can see that the Mainers for Responsible Gun Ownership Fund
committee had to file an amended report with the `explanation` "Offset due to deletion of filed
item." The correction is properly flagged with the `amended` value of `TRUE`. This is a good
indication that the `expenditutre_amount` ranges are reasonable.

```{r min_max_amount, collapse=TRUE}
me %>%
  select(
    date = expenditure_date,
    amount = expenditure_amount,
    amended,
    explanation,
    payee = last_name,
    expender = committee_name
  ) %>% 
  filter(
    or(
      amount == min(amount), 
      amount == max(amount)
    )
  )
```

```{r amount_histogram, echo=FALSE}
me %>%
  ggplot(aes(expenditure_amount)) +
  geom_histogram() +
  scale_x_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "Maine Expentiture Amount Distribution",
    subtitle = "From 2008 to 2019",
    caption = "Source: Maine Ethics Commission",
    x = "Amount",
    y = "Count"
  )
```

```{r amount_violin_comm, echo=FALSE}
me %>% 
  filter(expenditure_amount > 1) %>% 
  mutate(
    committee_type = 
      str_replace(
        string = committee_type, 
        pattern = "^Political Party Committee$", 
        replacement = "Party Committee"
      )
  ) %>% 
  ggplot(
    mapping = aes(
      x = reorder(committee_type, X = expenditure_amount, FUN = median, na.rm = TRUE), 
      y = expenditure_amount,
      fill = committee_type
    )
  ) +
  geom_violin(
    trim = TRUE,
    draw_quantiles = c(0.25, 0.5, 0.75),
    scale = "count",
  ) +
  scale_y_continuous(
    breaks = c(1 %o% 10^(0:6)),
    trans = "log10",
    labels = dollar
  ) +
  scale_fill_brewer(
    type = "qual", 
    palette = "Dark2", 
    guide = FALSE
  ) +
  labs(
    title = "The Cost of Campaigning in Maine",
    subtitle = "from 2008 to 2019, by Committee Type",
    caption = "Source: Maine Ethics Commission",
    x = "",
    y = "Expenditure Amount"
  ) +
  theme(panel.grid.major.x = element_blank())
```

#### Dates

To better explore the distribution of `expenditure_date` values, we can create a new 
`expenditure_year` variable using `lubridate::year()`.

```{r add_year}
me <- mutate(me, expenditure_year = year(expenditure_date))
```

The `expenditure_date` value is very clean, with `r sum(me$expenditure_year < 2000)` records from
before `r min(me$expenditure_date)` and `r sum(me$expenditure_date > today())` records after
`r max(me$expenditure_date)`.

```{r date_range, collapse=TRUE}
prop_na(me$expenditure_date)
min(me$expenditure_date)
sum(me$expenditure_year < 2000)
max(me$expenditure_date)
sum(me$expenditure_date > today())
```

## Wrangle

To improve the searchability of the database, we will perform some programatic normalization of 
geographic data. This is done largely with the `campfin::normal_*()` functions, wich wrap around
a few different `stringr::str_*()` functions.

### Address

```{r normal_address}
me <- me %>% 
  unite(
    starts_with("address"),
    col = address_combine,
    sep = " ",
    remove = FALSE,
    na.rm = TRUE
  ) %>% 
  mutate(
    address_norm = normal_address(
      address = address_combine,
      add_abbs = usps,
      na_rep = TRUE
    )
  )
```

```{r view_address, echo=FALSE}
me %>% 
  select(starts_with("address")) %>% 
  sample_n(10)
```

### ZIP

```{r count_zip_pre, collapse=TRUE}
n_distinct(me$zip)
prop_na(me$zip)
prop_in(me$zip, geo$zip)
length(setdiff(me$zip, geo$zip))
```

```{r normal_zip}
me <- me %>% 
  mutate(
    zip_norm = normal_zip(
      zip = zip,
      na_rep = TRUE
    )
  )
```

```{r count_zip_post, collapse=TRUE}
n_distinct(me$zip_norm)
prop_na(me$zip_norm)
prop_in(me$zip_norm, geo$zip)
length(setdiff(me$zip_norm, geo$zip))
```

There are still some `zip_norm` values that are invalid. We will leave these unchanged for now.

```{r unfixed, collapse=TRUE}
sample(unique(me$zip[which(me$zip_norm %out% geo$zip)]), 20)
```

### State

```{r count_state_pre, collapse=TRUE}
n_distinct(me$state)
prop_na(me$state)
prop_in(me$state, geo$state)
length(setdiff(me$state, geo$state))
setdiff(me$state, geo$state)
```

```{r normal_state}
me <- me %>% 
  mutate(
    state_norm = normal_state(
      state = str_replace(state, "^M$", "ME"),
      abbreviate = FALSE,
      na_rep = TRUE,
      na = c("", "NA"),
      valid = geo$state
    )
  )
```

```{r count_state_post, collapse=TRUE}
n_distinct(me$state_norm)
prop_na(me$state)
prop_in(me$state_norm, geo$state, na.rm = TRUE)
length(setdiff(me$state_norm, geo$state))
```

### City

```{r count_city_pre, collapse=TRUE}
n_distinct(me$city)
prop_in(me$city, geo$city, na.rm = TRUE)
length(setdiff(me$city, geo$city))
```

#### Normalize

```{r normal_city}
me <- me %>% 
  mutate(
    city_norm = normal_city(
      city = city, 
      geo_abbs = usps_city,
      st_abbs = c("ME", "DC", "MAINE"),
      na = na_city,
      na_rep = TRUE
    )
  )
```

```{r count_city_post_norm, collapse=TRUE}
n_distinct(me$city_norm)
prop_in(me$city_norm, geo$city, na.rm = TRUE)
length(setdiff(me$city_norm, geo$city))
```

#### Swap

```{r swap_city}
me <- me %>% 
  rename(city_raw = city) %>% 
  left_join(
    y = geo,
    by = c(
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_dist = stringdist(city_norm, city_match),
    city_swap = if_else(
      condition = equals(match_dist, 1),
      true = city_match,
      false = city_norm
    )
  )
```

```{r city_progress, echo=FALSE}
tibble(
  step = c("raw", "norm", "swap"),
  n_distinct = c(
    n_distinct(me$city_raw), 
    n_distinct(me$city_norm), 
    n_distinct(me$city_swap)
  ),
  prop_in = c(
    prop_in(me$city_raw, geo$city, na.rm = TRUE),
    prop_in(me$city_norm, geo$city, na.rm = TRUE),
    prop_in(me$city_swap, geo$city, na.rm = TRUE)
  ),
  unique_bad = c(
    length(setdiff(me$city_raw, geo$city)),
    length(setdiff(me$city_norm, geo$city)),
    length(setdiff(me$city_swap, geo$city))
  )
)
```

## Conclude

1. There are `r nrow(me)` records in the database.
1. There are `r sum(me$dupe_flag)` (`r percent(mean(me$dupe_flag))`) duplicate records in the
database, flagged with `dupe_flag`.
1. The range and distribution of `expenditure_amount` and `expenditure_date` seem reasonable.
1. There are `r sum(me$na_flag)` (`r percent(mean(me$na_flag))`) records missing either the amount,
date, payee, or expender. Most from 2018.
1. Consistency in goegraphic data has been improved with `campfin::normal_*()`.
1. The 5-digit `zip_norm` variable has been created with `campfin::normal_zip(me$zip)`.
1. The 4-digit `expenditure_year` variable has been created with
`lubridate::year(me$expenditure_date)`.

## Export

```{r proc_dir}
proc_dir <- here("me", "expends", "data", "processed")
dir_create(proc_dir)
```

```{r write_clean}
me %>% 
  select(
    -city_match,
    -city_norm,
  ) %>% 
  write_csv(
    na = "",
    path = glue("{proc_dir}/me_expends_clean.csv")
  )
```

