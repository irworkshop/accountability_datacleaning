---
title: "Missouri Expenditures"
author: "Kiernan Nicholls"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  # it's nice to un-collapse df print
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
set.seed(5)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

The IRW's `campfin` package will also have to be installed from GitHub. This package contains
functions custom made to help facilitate the processing of campaign finance data.

```{r load_packages, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_current_gh("kiernann/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  RSelenium, # remote browser
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tidytext, # text analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  scales, # format strings
  rvest, # scrape html pages
  knitr, # knit documents
  vroom, # read files fast
  glue, # combine strings
  here, # relative storage
  fs # search storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

Data is obtained from the [Missouri Ethics Commission][03] (MEC). 

> The Commission makes transparency and public information a priority in their operations. The
Commission’s website provides detailed financial information about campaign expenditures and
contributions and includes many publications, brochures, and web tutorials explaining Missouri’s
ethics laws, requirements and regulations.

Their data can be downloaded as anual files on their [data download page][04].

> This search returns results from Campaign Finance Committee Reports filed for 2011 and later.
The first row of the file contains the header/column names; the results are downloaded into a
comma-separated value format only.

[03]: https://www.mec.mo.gov/MEC/Default.aspx
[04]: https://www.mec.mo.gov/MEC/Campaign_Finance/CF_ContrCSV.aspx

## Import

We can import each file into R as a single data frame to be explored, wrangled, and exported
as a single file to be indexed on the TAP database.

### Download

The files can only be downloaded after submitting a form request for a type of data for a specific
year. We can automate this process with the RSelenium package.

```{r raw_dir}
raw_dir <- here("mo", "expends", "data", "raw")
dir_create(raw_dir)
```

```{r download_raw, warning=FALSE, error=FALSE, message=FALSE, collapse=TRUE, eval=FALSE}
# open the driver with auto download options
remote_driver <- rsDriver(
  port = 4444L,
  browser = "firefox",
  extraCapabilities = makeFirefoxProfile(
    list(
      browser.download.dir = raw_dir,
      browser.download.folderList = 2L,
      browser.helperApps.neverAsk.saveToDisk = "text/csv"
    )
  )
)

# navigate to the FL DOE download site
remote_browser <- remote_driver$client
remote_browser$navigate("https://www.mec.mo.gov/MEC/Campaign_Finance/CF_ContrCSV.aspx")

# chose "All" from elections list
type_css <- "/html/body/form/div[4]/div/div/div[2]/div[2]/table/tbody/tr[6]/td[2]/select/option[3]"
remote_browser$findElement("xpath", type_css)$clickElement()

for (i in 2:10) {
  dropdown <- "/html/body/form/div[4]/div/div/div[2]/div[2]/table/tbody/tr[8]/td[2]/select"
  remote_browser$findElement("xpath", glue("{dropdown}/option[{i}]"))$clickElement()
  submit_button <- "#ContentPlaceHolder_ContentPlaceHolder1_btnExport"
  remote_browser$findElement("css", submit_button)$clickElement()
}

# close the browser and driver
remote_browser$close()
remote_driver$server$stop()
```

### Read

We can read each file as a data frame into a list of data frames by `vroom::vroom()`.

```{r read_raw}
mo <- vroom(
  file = dir_ls(raw_dir, glob = "*.csv$"),
  .name_repair = make_clean_names,
  delim = ",",
  escape_backslash = FALSE,
  escape_double = FALSE,
  col_types = cols(.default = "c")
)
```

Due to an small bug in `vroom::vroom()`, we will have to parse the `date` column using
`readr::parse_date()` _after_ initally reading the files.

```{r parse_raw}
mo <- mo %>% 
  mutate(
    date = parse_date(date, "%m/%d/%Y %H:%M:%S %p"),
    amount = parse_number(amount)
  )
```

## Explore

```{r glimpse}
head(mo)
tail(mo)
glimpse(sample_frac(mo))
```

### Missing

```{r glimpse_na}
glimpse_fun(mo, count_na)
```

There are very few records missing one of the key values needed to identify a transaction (who,
what, when). The `first_name` and `last_name` variables are used to identify individual payees,
with `company` used to identify non-individuals. We can flag any record with `campfin::flag_na()`
to create a new `na_flag` variable with value `TRUE` for any record missing _any_ of those key
variables.

```{r flag_na}
mo <- mo %>% 
  mutate(
    individual = coalesce(last_name, first_name),
    payee_name = coalesce(individual, company)
    ) %>% 
  flag_na(
    payee_name,
    committee_name,
    date,
    amount
  )

sum(mo$na_flag)
```

### Duplicates

We can use `campfin::flag_dupes()` to create a new `dupe_flag` variable with with value `TRUE` for any duplicate row, after the first occurance. We will ignore the supposedly unique `cd3_b_id` 
variable.

```{r flag_dupes}
mo <- flag_dupes(mo, -cd3_b_id)
sum(mo$dupe_flag)
percent(mean(mo$dupe_flag))
```

### Categorical

```{r glimpse_distinct}
glimpse_fun(mo, n_distinct)
```

```{r purpose_word_bar, echo=FALSE, fig.height=10}
mo %>% 
  drop_na(purpose) %>% 
  unnest_tokens(word, purpose) %>% 
  count(word, sort = TRUE) %>% 
  anti_join(stop_words, by = "word") %>% 
  filter(word != "contribution") %>% 
  head(35) %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_col(aes(fill = n)) +
  scale_fill_gradient(guide = FALSE) +
  coord_flip() +
  labs(
    title = "Missouri Expenditure Purpose (Words)",
    caption = "Source: Missouri Ethics Commission",
    x = "Word",
    y = "Count"
  )
```

### Continuous

For continuous variables, we should explore both the range and distribution. This can be done with
visually with `ggplot2::geom_histogram()` and `ggplot2::geom_violin()`.

#### Amounts

```{r summary_amount}
summary(mo$amount)
sum(mo$amount <= 0, na.rm = TRUE)
sum(mo$amount >= 100000, na.rm = TRUE)
```

```{r amount_histogram, echo=FALSE}
brewer_dark2 <- RColorBrewer::brewer.pal(n = 8, name = "Dark2")
mo %>%
  ggplot(aes(amount)) +
  geom_histogram(fill = brewer_dark2[1]) +
  geom_vline(xintercept = median(mo$amount, na.rm = TRUE)) +
  scale_x_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  scale_y_continuous(labels = comma) +
  labs(
    title = "Indiana Expenditures Amount Distribution",
    caption = "Source: Missouri Ethics Commission",
    x = "Amount",
    y = "Count"
  )
```

#### Dates

```{r add_year}
mo <- mutate(mo, year = year(date))
```

```{r date_range, collapse=TRUE}
count_na(mo$date)
min(mo$date, na.rm = TRUE)
sum(mo$year < 2010, na.rm = TRUE)
max(mo$date, na.rm = TRUE)
sum(mo$date > today(), na.rm = TRUE)
```

```{r count_year}
count(mo, year)
```

```{r flag_fix_dates}
mo <- mo %>% 
  mutate(
    date_flag = year < 2010 | date > today(),
    date_clean = case_when(date_flag ~ as.Date(NA), not(date_flag) ~ date),
    year_clean = year(date_clean)
  )

sum(mo$date_flag, na.rm = TRUE)
```

```{r year_bar_count, echo=FALSE}
mo %>% 
  count(year_clean) %>% 
  mutate(even = is_even(year_clean)) %>% 
  filter(n > 100) %>% 
  ggplot(aes(x = year_clean, y = n)) +
  geom_col(aes(fill = even)) +
  scale_fill_brewer(type = "qual", palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(breaks = 2010:2019) +
  labs(
    title = "Missouri Expenditures Count by Year",
    caption = "Source: Missouri Ethics Commission",
    fill = "Election Year",
    x = "Year Made",
    y = "Number of Expenditures"
  ) +
  theme(legend.position = "bottom")
```

```{r year_bar_sum, echo=FALSE}
mo %>% 
  group_by(year_clean) %>% 
  summarise(sum = sum(amount, na.rm = TRUE)) %>% 
  mutate(even = is_even(year_clean)) %>% 
  ggplot(aes(x = year_clean, y = sum)) +
  geom_col(aes(fill = even)) +
  scale_fill_brewer(type = "qual", palette = "Dark2") +
  scale_y_continuous(labels = dollar) +
  scale_x_continuous(breaks = 2010:2019) +
  labs(
    title = "Missouri Expenditures Total by Year",
    caption = "Source: Missouri Ethics Commission",
    fill = "Election Year",
    x = "Year Made",
    y = "Total Amount"
  ) +
  theme(legend.position = "bottom")
```

```{r year_bar_mean, echo=FALSE}
mo %>% 
  group_by(year_clean) %>% 
  summarise(mean = mean(amount, na.rm = TRUE)) %>% 
  mutate(even = is_even(year_clean)) %>% 
  ggplot(aes(x = year_clean, y = mean)) +
  geom_col(aes(fill = even)) +
  scale_fill_brewer(type = "qual", palette = "Dark2") +
  scale_y_continuous(labels = dollar) +
  scale_x_continuous(breaks = 2010:2019) +
  labs(
    title = "Missouri Expenditures Mean by Year",
    caption = "Source: Missouri Ethics Commission",
    fill = "Election Year",
    x = "Year Made",
    y = "Mean Amount"
  ) +
  theme(legend.position = "bottom")
```

```{r month_line_count, echo=FALSE}
mo %>% 
  mutate(month = month(date_clean), even = is_even(year_clean)) %>% 
  group_by(month, even) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(x = month, y = n)) +
  geom_line(aes(color = even), size = 2) +
  scale_color_brewer(type = "qual", palette = "Dark2") +
  scale_y_continuous(labels = dollar) +
  scale_x_continuous(breaks = 1:12, labels = month.abb) +
  labs(
    title = "Missouri Expenditures Count by Month",
    caption = "Source: Missouri Ethics Commission",
    color = "Election Year",
    x = "Month Made",
    y = "Number of Expenditures"
  ) +
  theme(legend.position = "bottom")
```

## Wrangle

We should use the `campfin::normal_*()` functions to perform some basic, high-confidence text
normalization to improve the searchability of the database.

### Address

First, we will normalize the street address by removing punctuation and expanding abbreviations.

```{r normal_address}
if (packageVersion("tidyr") > "0.8.3.9") {
  mo <- mo %>% 
    unite(
      col = adress_full,
      starts_with("address"),
      remove = FALSE,
      na.rm = TRUE
    ) %>% 
    mutate(
      address_norm = normal_address(
        address = adress_full,
        add_abbs = usps,
        na_rep = TRUE
      )
    )
} else {
  stop(
    glue("tidyr needs a newer version, found {packageVersion('tidyr')}, need at least 0.8.3.9")
  )
}
```

We can see how this improves consistency across the `address_1` and `address_2` fields.

```{r view_address_change, echo=FALSE}
mo %>% 
  select(starts_with("address")) %>% 
  drop_na() %>% 
  sample_n(10)
```

### ZIP

The `zip` address is already pretty good, with 
`r percent(prop_in(mo$zip, valid_zip, na.rm = TRUE))` of the values already in our 95% 
comprehensive `valid_zip` list.

```{r count_zip_pre, collapse=TRUE}
n_distinct(mo$zip)
prop_in(mo$zip, valid_zip)
length(setdiff(mo$zip, valid_zip))
```

We can improve this further by lopping off the uncommon four-digit extensions and removing common
invalid codes like 00000 and 99999.

```{r normal_zip}
mo <- mo %>% 
  mutate(
    zip_norm = normal_zip(
      zip = zip,
      na_rep = TRUE
    )
  )
```

This brings our valid percentage to `r percent(prop_in(mo$zip_norm, valid_zip, na.rm = TRUE))`.

```{r count_zip_post, collapse=TRUE}
n_distinct(mo$zip_norm)
prop_in(mo$zip_norm, valid_zip)
length(setdiff(mo$zip_norm, valid_zip))
count_na(mo$zip_norm) - count_na(mo$zip)
```

### State

The `state` variable is also very clean, already at 
`r percent(prop_in(mo$state, valid_state, na.rm = TRUE))`.

```{r count_state_pre, collapse=TRUE}
n_distinct(mo$state)
prop_in(mo$state, valid_state, na.rm = TRUE)
length(setdiff(mo$state, valid_state))
setdiff(mo$state, valid_state)
```

There are still `r length(setdiff(mo$state, valid_state))` invalid values which we can remove.

```{r normal_state}
mo <- mo %>% 
  mutate(
    state_norm = normal_state(
      state = str_replace(str_trim(state), "^I$", "IN"),
      abbreviate = TRUE,
      na_rep = TRUE,
      valid = valid_state
    )
  )
```

```{r count_state_post, collapse=TRUE}
n_distinct(mo$state_norm)
prop_in(mo$state_norm, valid_state)
```

### City

The `city` value is the hardest to normalize. We can use a four-step system to functionally improve
the searchablity of the database.

1. **Normalize** the raw values with `campfin::normal_city()`
1. **Match** the normal values with the _expected_ value for that ZIP code
1. **Swap** the normal values with the expected value if they are _very_ similar
1. **Refine** the swapped values the [OpenRefine algorithms][08] and keep good changes

[08]: https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth

The raw `city` values are not very normal, with only
`r percent(prop_in(mo$city, valid_city, na.rm = TRUE))` already in `valid_city`, mostly due to case difference. If we simply convert to uppcase that numbers increases to 
`r percent(prop_in(str_to_upper(mo$city), valid_city, na.rm = TRUE))`. We will aim to get this number over 99% using the other steps in the process.

```{r count_city_pre, collapse=TRUE}
n_distinct(mo$city)
prop_in(str_to_upper(mo$city), valid_city, na.rm = TRUE)
length(setdiff(mo$city, valid_city))
count_na(mo$city)
```

#### Normalize

```{r normal_city}
mo <- mo %>% 
  mutate(
    city_norm = normal_city(
      city = city, 
      geo_abbs = usps_city,
      st_abbs = c("MO", "DC", "MISSOURI"),
      na = na_city,
      na_rep = TRUE
    )
  )
```

This process brought us to `r percent(prop_in(mo$city_norm, valid_city, na.rm = TRUE))` valid.

```{r count_city_post_norm, collapse=TRUE}
n_distinct(mo$city_norm)
prop_in(mo$city_norm, valid_city, na.rm = TRUE)
length(setdiff(mo$city_norm, valid_city))
count_na(mo$city_norm)
```

It also increased the proportion of `NA` values by 
`r percent(prop_na(mo$city_norm) - prop_na(mo$city))`. These new `NA` values were either a single
(possibly repeating) character, or contained in the `na_city` vector.

```{r new_city_na, echo=FALSE}
mo %>% 
  filter(is.na(city_norm) & !is.na(city)) %>% 
  select(zip_norm, state_norm, city, city_norm) %>% 
  distinct() %>% 
  sample_frac()
```

#### Swap

Then, we will compare these normalized `city_norm` values to the _expected_ city value for that
vendor's ZIP code. If the [levenshtein distance][09] is less than 3, we can confidently swap these
two values.

[09]: https://en.wikipedia.org/wiki/Levenshtein_distance

```{r swap_city}
mo <- mo %>% 
  rename(city_raw = city) %>% 
  left_join(
    y = geo,
    by = c(
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_dist = stringdist(city_norm, city_match),
    city_swap = if_else(
      condition = is_less_than(match_dist, 3),
      true = city_match,
      false = city_norm
    )
  )
```

This is a very fast way to increase the valid proportion to
`r percent(prop_in(mo$city_swap, valid_city, na.rm = TRUE))` and reduce the number of distinct
_invalid_ values from `r length(setdiff(mo$city_norm, valid_city))` to only
`r length(setdiff(mo$city_swap, valid_city))`

```{r count_city_post_swap, collapse=TRUE}
n_distinct(mo$city_swap)
prop_in(mo$city_swap, valid_city, na.rm = TRUE)
length(setdiff(mo$city_swap, valid_city))
```

#### Refine

Finally, we can pass these swapped `city_swap` values to the OpenRefine cluster and merge 
algorithms. These two algorithms cluster similar values and replace infrequent values with their
more common counterparts. This process can be harmful by making _incorrect_ changes. We will only
keep changes where the state, ZIP code, _and_ new city value all match a valid combination.

```{r refine_city}
good_refine <- mo %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge() %>% 
      n_gram_merge(numgram = 1)
  ) %>% 
  filter(city_refine != city_swap) %>% 
  inner_join(
    y = geo,
    by = c(
      "city_refine" = "city",
      "state_norm" = "state",
      "zip_norm" = "zip"
    )
  )

nrow(good_refine)
```

```{r view_city_refines, echo=FALSE}
good_refine %>%
  count(
    state_norm, 
    zip_norm, 
    city_raw, 
    city_refine,
    sort = TRUE
  )
```

We can join these good refined values back to the original data and use them over their incorrect
`city_swap` counterparts in a new `city_refine` variable.

```{r join_refine}
mo <- mo %>% 
  left_join(good_refine) %>% 
  mutate(city_refine = coalesce(city_refine, city_swap))
```

This brings us to `r percent(prop_in(mo$city_refine, valid_city, na.rm = TRUE))` valid values.

```{r count_city_post_refine, collapse=TRUE}
n_distinct(mo$city_refine)
prop_in(mo$city_refine, valid_city, na.rm = TRUE)
length(setdiff(mo$city_refine, valid_city))
```

#### Progress

We can make very few manual changes to capture the last few big invalid values. Local city
abbreviations (e.g., SPFD) often need to be changed by hand.

```{r view_final_bad}
mo %>%
  filter(city_refine %out% valid_city) %>% 
  count(state_norm, city_refine, sort = TRUE) %>% 
  drop_na(city_refine)
```

```{r city_final}
mo <- mo %>% 
  mutate(
    city_final = city_refine %>% 
      str_replace("^STLOUIS$", "SAINT LOUIS") %>% 
      str_replace("^SPFD$", "SPRINGFIELD") %>% 
      str_replace("^INDEP$", "INDEPENDENCE") %>% 
      str_replace("^NKC$", "NORTH KANSAS CITY")
  )
```

By adding a dozen popular Missouri cities to our `valid_city` list, we can reach our 99% goal.

```{r increase_valid_city}
valid_city <- c(
  valid_city,
  "OVERLAND",
  "OVERLAND PARK",
  "RAYTOWN",
  "NORTH KANSAS CITY",
  "PRAIRIE VILLAGE",
  "UNIVERSITY CITY",
  "WEBSTER GROVES",
  "RICHMOND HEIGHTS",
  "LENEXA",
  "STE GENEVIEVE",
  "LEAWOOD",
  "DES PERES",
  "OLIVETTE",
  "TOWN AND COUNTRY",
  "AFFTON"
)
```

```{r progress_table, echo=FALSE}
progress_table <- tibble(
  stage = c("raw", "norm", "swap", "refine", "final"),
  prop_good = c(
    prop_in(str_to_upper(mo$city_raw), valid_city, na.rm = TRUE),
    prop_in(mo$city_norm, valid_city, na.rm = TRUE),
    prop_in(mo$city_swap, valid_city, na.rm = TRUE),
    prop_in(mo$city_refine, valid_city, na.rm = TRUE),
    prop_in(mo$city_final, valid_city, na.rm = TRUE)
  ),
  total_distinct = c(
    n_distinct(str_to_upper(mo$city_raw)),
    n_distinct(mo$city_norm),
    n_distinct(mo$city_swap),
    n_distinct(mo$city_refine),
    n_distinct(mo$city_final)
  ),
  unique_bad = c(
    length(setdiff(str_to_upper(mo$city_raw), valid_city)),
    length(setdiff(mo$city_norm, valid_city)),
    length(setdiff(mo$city_swap, valid_city)),
    length(setdiff(mo$city_refine, valid_city)),
    length(setdiff(mo$city_final, valid_city))
  )
)

diff_change <- progress_table$unique_bad[5]-progress_table$unique_bad[1]
prop_change <- diff_change/progress_table$unique_bad[1]
```

Still, our progress is significant without having to make a single manual or unconfident change.
The percent of valid cities increased from `r percent(progress_table$prop_good[1])` to 
`r percent(progress_table$prop_good[5])`. The number of total distinct city values decreased from
`r comma(progress_table$total_distinct[1])` to `r comma(progress_table$total_distinct[5])`. The
number of distinct invalid city names decreased from `r comma(progress_table$unique_bad[1])` to
only `r comma(progress_table$unique_bad[5])`, a change of `r percent(prop_change)`.

```{r print_progress, echo=FALSE}
kable(
  x = progress_table,
  format = "markdown", 
  digits = 4,
  col.names = c("Normalization Stage", "Total Distinct", "Percent Valid", "Unique Invalid")
)
```

```{r wrangle_bar_prop, echo=FALSE}
progress_table %>% 
  mutate(stage = as_factor(stage)) %>% 
  ggplot(aes(x = stage, y = prop_good)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = brewer_dark2[2]) +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Missouri Expenditures Payee City Progress",
    subtitle = "Percent of total values contained in pre-defined list of cities",
    caption = "Source: Missouri Ethics Commission",
    x = "Wrangling Stage",
    y = "Proportion Valid Cities"
  )
```

```{r wrangle_bar_distinct, echo=FALSE}
progress_table %>% 
  mutate(stage = as_factor(stage)) %>% 
  select(-prop_good) %>% 
  rename(
    All = total_distinct,
    Invalid = unique_bad
  ) %>% 
  gather(
    -stage,
    key = "key",
    value = "value"
  ) %>% 
  ggplot(aes(x = stage, y = value)) +
  geom_col(aes(fill = key)) +
  scale_fill_brewer(type = "qual", palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  labs(
    title = "Missouri Expenditures Payee City Progress",
    subtitle = "Total distinct number of city values",
    caption = "Source: Missouri Ethics Commission",
    fill = "Distinct Values",
    x = "Wrangling Stage",
    y = "Number of Expenditures"
  )
```

## Conclude

1. There are `r nrow(mo)` records in the database.
1. There are `r sum(mo$dupe_flag)` duplicate records in the database.
1. The range and distribution of `amount` seems reasomable, and `date` has been cleaned by removing
`r sum(mo$date_flag, na.rm = T)` values from the distance past or future.
1. There are `r sum(mo$na_flag)` records missing either recipient or date.
1. Consistency in geographic data has been improved with `campfin::normal_*()`.
1. The 5-digit `zip_norm` variable has been created with `campfin::normal_zip()`.
1. The 4-digit `year_clean` variable has been created with `lubridate::year()`.

## Export

```{r create_proc_dir}
proc_dir <- here("in", "expends", "data", "processed")
dir_create(proc_dir)
```

```{r write_clean}
mo %>% 
  select(
    -city_norm,
    -city_swap,
    -city_match,
    -city_swap,
    -match_dist,
    -city_refine,
    -year
  ) %>% 
  write_csv(
    path = glue("{proc_dir}/mo_expends_clean.csv"),
    na = ""
  )
```

