---
title: "Pennsylvania Campaign Filers Data Diary"
author: "Yanqi Xu"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---
## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

```{r p_load, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  tidyverse, # data manipulation
  readxl, # read excel files
  lubridate, # datetime strings
  tidytext, # string analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  knitr, # knit documents
  glue, # combine strings
  scales, #format strings
  here, # relative storage
  fs, # search storage 
  vroom #read deliminated files
)
```
This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

```{r}
raw_dir <- here("pa", "expends", "data", "raw")
dir_create(raw_dir)
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data

[03]: https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Pages/FullCampaignFinanceExport.aspx "source"

### About

More information about the record layout can be found here https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Documents/readme.txt.

## Import

### Download

Download raw, **immutable** data file. Go to https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Pages/FullCampaignFinanceExport.aspx. We'll download the files from 2015 to 2019 (file format: zip file) with the script. We are processing the filer records specifically in each zip file.

```{r download to raw_dir, eval = FALSE}
#download the files into the directory
pa_exp_urls <- glue("https://www.dos.pa.gov//VotingElections/CandidatesCommittees/CampaignFinance/Resources/Documents/{2015:2019}.zip")

if (!all_files_new(raw_dir)) {
  for (url in pa_exp_urls) {
    download.file(
      url = url,
      destfile = glue("{raw_dir}/{basename(url)}")
    )
  }
}
```

```{r}
filer_files <- list.files(raw_dir, pattern = "filer.+", recursive = TRUE, full.names = TRUE)

filer_fields <- c("FILERID", "EYEAR", "CYCLE", "AMMEND", "TERMINATE", "FILERTYPE", "FILERNAME", "OFFICE", "DISTRICT", "PARTY", "ADDRESS1", "ADDRESS2", "CITY", "STATE", "ZIPCODE", "COUNTY", "PHONE", "BEGINNING", "MONETARY", "INKIND")

pa_filer <- filer_files %>% 
  map_dfr(read_delim, delim = ",", escape_double = FALSE,
      escape_backslash = TRUE, col_names = filer_fields, 
      col_types = cols(.default = col_character(),
                       CYCLE = col_integer(),
                       EYEAR = col_integer(),
                       BEGINNING = col_double(),
                       MONETARY = col_double(),
                       INKIND = col_double()
                       )) %>% 
  mutate_if(is_character, str_to_upper)

glimpse(pa_filer)

pa_filer <- pa_filer %>% 
  unite(
    ADDRESS1, ADDRESS2,
    col = address_clean,
    sep = " ",
    remove = FALSE,
    na.rm = TRUE
  ) %>% 
  mutate(
    address_clean = normal_address(
      address = address_clean,
      add_abbs = usps_city,
      na_rep = TRUE
    )
  ) %>% 
  select(
    everything(),
    address_clean
  )
```
## Wrangle
We will normalize the `CITY`, `STATE` and `ZIP` fields.
### State
The `STATE` field is clean, as all the values are valid state names in the U.S. 
```{r state}
prop_in(pa_filer$STATE, valid_state, na.rm = TRUE) %>% percent()
```

### ZIP
```{r normal zip}
pa_filer <- pa_filer %>% mutate(zip_clean = normal_zip(pa_filer$ZIPCODE))
```



```{r filer clean}
pa_filer <- pa_filer %>% mutate(city_prep = normal_city(city = CITY, 
                                                          geo_abbs = usps_city,
                                            st_abbs = c(valid_state),
                                            na = invalid_city,
                                            na_rep = TRUE))
pa_filer <- tibble::rowid_to_column(pa_filer, "index")

pa_filer$FILERNAME <- pa_filer$FILERNAME %>%  str_replace("&AMP;", "&") 

# Match
pa_filer <- pa_filer %>%
  left_join(
    y = zipcodes,
    by = c(
      "zip_clean" = "zip",
      "STATE" = "state"
    )
  ) %>% 
  rename(city_match = city)
# Swap
pa_filer <- pa_filer %>% 
mutate(
    match_dist = stringdist(city_match, city_prep),
    city_swap = if_else(
      condition = !is.na(match_dist) & match_dist <= 2,
      true = city_match,
      false = city_prep
    )
  )

# Refine
pa_filer_refined <- pa_filer %>%
  filter(match_dist != 1) %>% 
  filter(STATE =="PA") %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge(dict = valid_city) %>% 
      n_gram_merge(numgram = 2),
    refined = (city_swap != city_refine)
  ) %>% 
  filter(refined) %>% 
  select(
    index,
    FILERID, 
    CITY,
    city_prep,
    city_match,
    city_swap,
    match_dist,
    city_refine,
    STATE, 
    ZIPCODE,
    zip_clean
  )

pa_filer_refined %>% 
  count(city_swap, city_refine) %>% 
  arrange(desc(n))

pa_filer_refined$city_refine <- pa_filer_refined$city_refine %>% 
  str_replace("^MCCBG$", "MCCONNELLSBURG")


filer_refined_table <-pa_filer_refined %>% 
  select(index, city_refine)
pa_filer <- pa_filer %>% 
  left_join(filer_refined_table, by ="index") %>% 
  mutate(city = coalesce(city_refine, city_swap)) 

prop_in(pa_filer$CITY, valid_city, na.rm = TRUE)
prop_in(pa_filer$city_prep, valid_city, na.rm = TRUE)
prop_in(pa_filer$city, valid_city, na.rm = TRUE)

valid_places <- unique(c(valid_city, extra_city))
```
Use `campfin` pacakage's check_city function to check if a locality is a valid place against returned results from Google. To use this function, you will need to substitute `api_key` with your own api key. 
```{r filer check_city, eval=FALSE}
pa_filer_out <- pa_filer %>% 
  filter(city %out% valid_city) %>% 
  drop_na(city,STATE) %>% 
  count(city, STATE) %>% 
  mutate(check_city_flag = pmap_lgl(.l = list(city, STATE), .f = check_city, key = api_key))

pa_add_valid <- pa_filer_out %>% filter(check_city_flag) %>% print_all() 
# Use the following command to paste to the extra_city Google sheet
# pa_add_valid$city %>% cat( sep = "\n")
valid_places <- unique(c(valid_places, pa_filer_out$city[pa_filer_out$check_city_flag]))
```

```{r fetch_city, eval=FALSE}
pa_filer <- pa_filer %>% mutate(city = city %>% 
  str_replace("^PHILA$", "PHILADELPHIA") %>% 
  str_replace("^PGH$", "PITSSBURGH"))

pa_filer_out <- pa_filer %>% filter(city %out% valid_places) %>% 
  drop_na(STATE,city) %>% 
  count(STATE, city) 

api_key <- Sys.getenv("GEOCODING_API")


pa_filer_out <- pa_filer_out %>% cbind(
  pmap_dfr(.l = list(pa_filer_out$city, pa_filer_out$STATE), .f = check_city, key = api_key, guess = T))
```

```{r join guess results}
pa_filer_out <- pa_filer_out %>% mutate(city_fetch = coalesce(guess_city, guess_place)) 

pa_filer_out$city_fetch[which(pa_filer_out$city == "HBRG")] <- "HARRISBURG"

pa_filer <- pa_filer_out %>% 
  filter(!check_city_flag) %>% 
  select(city, STATE, city_fetch) %>% 
  right_join(pa_filer, by = c("city", "STATE")) 

valid_places <- unique(c(valid_places, pa_filer_out$city[pa_filer_out$check_city_flag]))

pa_filer <- pa_filer %>% mutate(city_clean = coalesce(city_fetch,city))
```

```{r add to extra_city, eval=FALSE}
extra_city <- gs_title("extra_city")

extra_city <- extra_city %>% 
  gs_add_row(ws = 1, input = pa_filer_out %>% filter(check_city_flag) %>% select(city))
```

```{r}
prop_in(pa_filer$CITY, valid_city, na.rm = T)
prop_in(pa_filer$city_swap, valid_city, na.rm = T)
prop_in(pa_filer$city, valid_city, na.rm = T)
prop_in(pa_filer$city_clean, valid_places, na.rm = T)
```


## Conclude

1. There are `r nrow(pa)` records in the database
1. There are `r sum(pa$dupe_flag)` records with suspected duplicate filerID, recipient, date, _and_ amount
(flagged with `dupe_flag`)
1. The ranges for dates and amounts are reasonable
1. Consistency has been improved with `stringr` package and custom `normal_*()` functions.
1. The five-digit `zip_clean` variable has been created with `zipcode::clean.zipcode()`
1. The `year` variable has been created with `lubridate::year()`
1. There are `r count_na(pa$CITY)` records with missing `city` values and `r count_na(pa$EXPNAME)` records with missing `payee` values (both flagged with the `na_flag`).

## Export

```{r write_clean}
clean_dir <- here("pa", "expends", "data", "processed")
dir_create(clean_dir)
pa_filer %>% 
  select(
    -city_prep,
    -city,
    -match_dist,
    -city_swap,
    -city_refine,
    -city_fetch
  ) %>% 
  write_csv(
    path = glue("{clean_dir}/pa_filers_clean.csv"),
    na = ""
  )
```
