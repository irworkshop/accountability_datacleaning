---
title: "Pennsylvania Campaign Expenditures Data Diary"
author: "Yanqi Xu"
date: "`r Sys.time()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
options(width = 99)
```

## Project

The Accountability Project is an effort to cut across data silos and give journalists, policy
professionals, activists, and the public at large a simple way to search across huge volumes of
public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each dataset row as a
transaction. For each transaction there should be (at least) 3 variables:

1. All **parties** to a transaction
2. The **date** of the transaction
3. The **amount** of money involved

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for duplicates
1. Check ranges
1. Is there anything blank or missing?
1. Check for consistency issues
1. Create a five-digit ZIP Code called `ZIP5`
1. Create a `YEAR` field from the transaction date
1. Make sure there is data on both parties to a transaction

## Packages

The following packages are needed to collect, manipulate, visualize, analyze, and communicate
these results. The `pacman` package will facilitate their installation and attachment.

```{r p_load, message=FALSE, dfrning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh("irworkshop/campfin")
pacman::p_load(
  stringdist, # levenshtein value
  tidyverse, # data manipulation
  readxl, # read excel files
  lubridate, # datetime strings
  tidytext, # string analysis
  magrittr, # pipe opperators
  janitor, # dataframe clean
  refinr, # cluster and merge
  knitr, # knit documents
  glue, # combine strings
  scales, #format strings
  here, # relative storage
  fs, # search storage 
  vroom #read deliminated files
)
```

```{r fix_fun, echo=FALSE, collapse = TRUE}
# fix conflict
here <- here::here
# custom utility functions
print_all <- function(df) df %>% print(n = nrow(.)) 
```

This document should be run as part of the `R_campfin` project, which lives as a sub-directory of
the more general, language-agnostic [`irworkshop/accountability_datacleaning`][01] GitHub
repository.

The `R_campfin` project uses the [RStudio projects][02] feature and should be run as such. The
project also uses the dynamic `here::here()` tool for file paths relative to _your_ machine.

```{r where_here, collapse=TRUE}
# where dfs this document knit?
here::here()
```

[01]: https://github.com/irworkshop/accountability_datacleaning "TAP repo"
[02]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects "Rproj"

## Data


[03]: https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Pages/FullCampaignFinanceExport.aspx "source"

### About

More information about the record layout can be found here https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Documents/readme.txt.

## Import

### Download

Download raw, **immutable** data file. Go to https://www.dos.pa.gov/VotingElections/CandidatesCommittees/CampaignFinance/Resources/Pages/FullCampaignFinanceExport.aspx. We'll download the files from 2015 to 2019 (file format: zip file) with the script.

```{r raw_dir}
# create a directory for the raw data
raw_dir <- here("pa", "expends", "data", "raw")
dir_create(raw_dir)
```

Download all the file packages containing all campaign-finance-related files. 
```{r download to raw_dir, eval = FALSE}
#download the files into the directory
pa_exp_urls <- glue("https://www.dos.pa.gov//VotingElections/CandidatesCommittees/CampaignFinance/Resources/Documents/{2015:2019}.zip")

if (!all_files_new(raw_dir)) {
  for (url in pa_exp_urls) {
    download.file(
      url = url,
      destfile = glue("{raw_dir}/{basename(url)}")
    )
  }
}
```
### Read
Read individual csv files from the downloaded zip files
```{r read_many, eval = FALSE}

zip_files <- dir_ls(raw_dir, glob = "*.zip")

if (all_files_new(path = raw_dir, glob = "*.txt")) {
  for (i in seq_along(zip_files)) {
    unzip(
      zipfile = zip_files[i],
      #Matches the csv files that starts with expense, and trim the "./ " from directory names
      files = grep("expense.+", unzip(zip_files[i]), value = TRUE) %>% substring(3,),
      exdir = raw_dir
    )
  }
}
```
Read multiple csvs into R
```{r read multiple files}
#recursive set to true because 2016 and 2015 have subdirectories under "raw"
expense_files <- list.files(raw_dir, pattern = "expense.+", recursive = TRUE, full.names = TRUE)
#pa_lines <- list.files(raw_dir, pattern = ".txt", recursive = TRUE) %>% map(read_lines) %>% unlist()
pa_col_names <- c("FILERID","EYEAR","CYCLE","EXPNAME","ADDRESS1","ADDRESS2","CITY","STATE","ZIPCODE","EXPDATE","EXPAMT","EXPDESC")


pa <- expense_files %>% 
  map(read_delim, delim = ",", escape_double = FALSE,
      escape_backslash = FALSE, col_names = pa_col_names, 
      col_types = cols(.default = col_character())) %>% 
bind_rows()
```
There are `r nrow(pa %>% filter(nchar(STATE)!=2))` parsing failures.
We'll move along the information in rows that were read incorrectly due to double quotes in the address column. 
```{r}
nudge <- which(nchar(pa$STATE) > 2)
# However, the information in the last column EXPDESC for these columns has been lost at the time of data import.
for (index in nudge) {
  for (column in c(7:11)){
    pa[[index, column]] <- pa[[index, column+1]]
  }
}
#All the fields are converted to strings. Convert to date and double.
pa$EXPDATE <- as.Date(pa$EXPDATE, "%Y%m%d")
pa$EXPAMT <- as.double(pa$EXPAMT)
pa$ADDRESS1 <- normal_address(pa$ADDRESS1)
pa$ADDRESS2 <- normal_address(pa$ADDRESS2)
```

The text fields contain both lower-case and upper-case letters. The for loop converts them to all upper-case letters unifies the encoding to "UTF-8", replaces the "&amp;", the HTML expression of "An ampersand".

```{r}
for (i in c(4:7)) {
  pa[[i]] <- iconv(pa[[i]], 'UTF-8', 'ASCII') %>% 
    toupper() %>% 
   str_replace("&AMP;", "&") 
}

pa$EXPDESC <- toupper(pa$EXPDESC)

```

## Explore

There are `nrow(pa)` records of `length(pa)` variables in the full database.

```{r glimpse}
head(pa)
tail(pa)
glimpse(pa)
```

### Distinct

The variables range in their degree of distinctness.

```{r n_distinct}
pa %>% glimpse_fun(n_distinct)
```


### Missing

The variables also vary in their degree of values that are `NA` (missing).

```{r count_na}
pa %>% glimpse_fun(count_na)
```

We will flag any records with missing values in the key variables used to identify an expenditure.
There are `r sum(pa$na_flag)` elements that are flagged as missing at least one value.
```{r na_flag}
pa <- pa %>% flag_na(EXPNAME, EXPDATE, EXPDESC, EXPAMT, CITY)
```

### Duplicates

```{r get_dupes, collapse=TRUE}
pa <- flag_dupes(pa, dplyr::everything())
sum(pa$dupe_flag)
```

### Ranges

#### Amounts

```{r, collapse = TRUE}
summary(pa$EXPAMT)
sum(pa$EXPAMT < 0 , na.rm = TRUE)
```

See how the campaign expenditures were distributed

```{r amount distribution, eval = TRUE}
pa %>% 
  ggplot(aes(x = EXPAMT)) + 
  geom_histogram() +
  scale_x_continuous(
    trans = "log10", labels = dollar)
```

Expenditures out of state
```{r}
sum(pa$STATE != "PA", na.rm = TRUE)
```

Top spending purposes
```{r eval = TRUE, echo = FALSE}
pa %>%   drop_na(EXPDESC) %>% 
  group_by(EXPDESC) %>% 
  summarize(total_spent = sum(EXPAMT)) %>% 
  arrange(desc(total_spent)) %>% 
  head(10) %>% 
  ggplot(aes(x = EXPDESC, y = total_spent)) +
  geom_col() +
  labs(title = "Pennsylvania Campaign Expenditures by Total Spending",
       caption = "Source: Pennsylvania Dept. of State") +
  scale_y_continuous(labels = scales::dollar) +
  coord_flip() +
  theme_minimal()

```

### Dates
Some of the dates are too far back and some are past the current dates. 
```{r}
summary(pa$EXPDATE)
```

### Year

Add a `year` variable from `date` after `col_date()` using `lubridate::year()`.

```{r add_year}
pa <- pa %>% mutate(year = year(EXPDATE), on_year = is_even(year))
```
Turn some year and date values to NAs. 
```{r year_flag}
pa <- pa %>% mutate(date_flag = year < 2000 | year > format(Sys.Date(), "%Y"), 
                    date_clean = ifelse(
                    date_flag, NA, EXPDATE),
                    year_clean = ifelse(
                    date_flag, NA, year))
```


```{r year_count_bar, eval = TRUE, echo=FALSE}

pa %>% 
  filter( 2014 < year & year < 2020) %>% 
  count(on_year, year) %>% 
  ggplot(aes(x = year, y = n)) +
  geom_col(aes(fill=on_year)) +
  scale_fill_brewer(
    type = "qual",
    palette = "Dark2",
    guide = FALSE
  ) +
  labs(
    title = "Pennsylvania Expenditure Counts per Year",
    caption = "Source: Pennsylvania Dept. of State",
    x = "Year",
    y = "Count"
  )
  
```

```{r amount_year_bar, eval = TRUE, echo=FALSE}
pa %>% 
  filter( 2014 < year & year < 2020) %>% 
  group_by(year, on_year) %>% 
  summarize(mean = mean(EXPAMT)) %>% 
  ggplot(aes(x = year, y = mean)) +
  geom_col(aes(fill = on_year)) +
  scale_fill_brewer(
    type = "qual",
    palette = "Dark2",
    guide = FALSE
  ) +
  scale_y_continuous(labels = scales::dollar) +
  labs(
    title = "Pennsylvania Expenditure Mean Amount per Year",
    caption = "Source: Pennsylvania Dept. of State",
    x = "Year",
    y = "Amount"
  ) 
```

```{r amount_month_line, echo = FALSE}
pa %>% 
  mutate(month = month(EXPDATE)) %>% 
  drop_na(month) %>% 
  group_by(on_year, month) %>% 
  summarize(mean = mean(EXPAMT)) %>% 
  ggplot(aes(month, mean)) +
  geom_line(aes(color = on_year), size = 2) +
  scale_y_continuous(labels = dollar) +
  scale_x_continuous(labels = month.abb, breaks = 1:12) +
  scale_color_brewer(
    type = "qual",
    palette = "Dark2"
  ) +
  labs(
    title = "Pennsylvania Expenditure Amount by Month",
    caption = "Source: Pennsylvania Dept. of State",
    color = "Election Year",
    x = "Month",
    y = "Amount"
  )
```

```{r}
pa %>% group_by(EXPDESC) %>% summarize(total = sum(EXPAMT)) %>% arrange(desc(total))
```

## Wrangle
The state column is now pretty clean, as all non-NA columns have two characters.

### Indexing
```{r}
pa <- tibble::rowid_to_column(pa, "index")
```


### Zipcode
The Zipcode column can range from 1 to 13 columns.

```{r collapse = TRUE}
table(nchar(pa$ZIPCODE))
```


```{r normalize zip, collapse = TRUE}
pa <- pa %>% 
  mutate(
    zip_clean = ZIPCODE %>% 
      normal_zip(na_rep = TRUE))
sample(pa$zip_clean, 10)
```


### State
View values in the STATE field is not a valid state abbreviation
```{r fix state abbrev, collapse = TRUE, echo=FALSE}
{pa$STATE[pa$STATE %out% valid_state]}[!is.na(pa$STATE[pa$STATE %out% valid_state])]

pa %>% filter(STATE == "CN")
```
These are expenditures in Canada, which we can leave in. 
### City

Cleaning city values is the most complicated. This process involves four steps:

1. Prepare raw city values by removing invalid data and reducing inconsistencies
1. Match prepared city values with the _actual_ city name of that record's ZIP code
1. swap prepared city values with the ZIP code match _if_ only 1 edit is needed
1. Refine swapped city values with key collision and n-gram fingerprints

#### Prep

`r sum(!is.na(pa$CITY))` distinct cities were in the original dataset in column 
```{r prep_city, collapse = TRUE}
pa <- pa %>% mutate(city_prep = normal_city(city = CITY,
                                            geo_abbs = usps_city,
                                            st_abbs = c(valid_state),
                                            na = invalid_city,
                                            na_rep = TRUE))
n_distinct(pa$city_prep)
```

#### Match

```{r match_dist}
pa <- pa %>%
  left_join(
    y = zipcodes,
    by = c(
      "zip_clean" = "zip",
      "STATE" = "state"
    )
  ) %>% 
  rename(city_match = city)
```

#### Swap

To replace city names with expected city names from zipcode when the two variables are no more than two characters different
```{r }
pa <- pa %>% 
  mutate(
    match_dist = stringdist(city_prep, city_match),
    city_swap = if_else(condition = is.na(city_match) == FALSE,
                        if_else(
      condition = match_dist <= 2,
      true = city_match,
      false = city_prep
    ),
      false = city_prep
  ))


summary(pa$match_dist)
sum(pa$match_dist == 1, na.rm = TRUE)
n_distinct(pa$city_swap)
```

#### Refine

```{r valid_city}
valid_city <- campfin::valid_city
```
Use the OpenRefine algorithms to cluster similar values and merge them together. This can be done using the refinr::key_collision_merge() and refinr::n_gram_merge() functions on our prepared and swapped city data.
```{r view_refine}
pa_refined <- pa %>%
  filter(match_dist != 1) %>% 
  filter(STATE =="PA") %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge(dict = valid_city) %>% 
      n_gram_merge(numgram = 2),
    refined = (city_swap != city_refine)
  ) %>% 
  filter(refined) %>% 
  select(
    index,
    FILERID, 
    CITY,
    city_prep,
    city_match,
    city_swap,
    match_dist,
    city_refine,
    STATE, 
    ZIPCODE,
    zip_clean
  )

pa_refined %>% 
  count(city_swap, city_refine) %>% 
  arrange(desc(n))
```

```{r}
refined_values <- unique(pa_refined$city_refine)
count_refined <- tibble(
  city_refine = refined_values, 
  refine_count = NA
)

for (i in seq_along(refined_values)) {
  count_refined$refine_count[i] <- sum(str_detect(pa$city_swap, refined_values[i]), na.rm = TRUE)
}

swap_values <- unique(pa_refined$city_swap)
count_swap <- tibble(
  city_swap = swap_values, 
  swap_count = NA
)

for (i in seq_along(swap_values)) {
  count_swap$swap_count[i] <- sum(str_detect(pa$city_swap, swap_values[i]), na.rm = TRUE)
}

pa_refined %>% 
  left_join(count_swap) %>% 
  left_join(count_refined) %>%
  select(
    FILERID,
    city_match,
    city_swap,
    city_refine,
    swap_count,
    refine_count
  ) %>% 
  mutate(diff_count = refine_count - swap_count) %>%
  mutate(refine_dist = stringdist(city_swap, city_refine)) %>%
  distinct() %>%
  arrange(city_refine) %>% 
  print_all()
```

Manually change the city_refine fields due to overcorrection.


```{r revert overcorrected refine}
st_pattern <- str_c("\\s",unique(zipcodes$state), "$", collapse = "|")


pa_refined$city_refine <- pa_refined$city_refine %>% 
  str_replace("^DU BOIS$", "DUBOIS") %>% 
  str_replace("^PIT$", "PITTSBURGH") %>% 
  str_replace("^MCCBG$", "MCCONNELLSBURG") %>% 
  str_replace("^PLUM BORO$", "PLUM") %>% 
  str_replace("^GREENVILLE$", "EAST GREENVILLE") %>% 
  str_replace("^NON$", "ONO") %>% 
  str_replace("^FORD CLIFF$", "CLIFFORD") %>% 
  str_replace("^W\\sB$", "WILKES BARRE") 

  

refined_table <-pa_refined %>% 
  select(index, city_refine)
```
  

#### Merge 

```{r join_refine}
pa <- pa %>% 
  left_join(refined_table, by ="index") %>% 
  mutate(city = coalesce(city_refine, city_swap)) 

pa$city <- pa$city %>% 
  str_replace("^MT PLEASANT$", "MOUNT PLEASANT") %>% 
  str_replace("^ST\\s", "SAINT ") %>% 
  str_replace("^MT\\s", "MOUNT ") %>%  
  str_replace("^FT\\s", "FORT ") %>% 
  str_replace("^W\\sB$|WB", "WILKES BARRE") %>% 
  str_replace("\\sHTS$|\\sHGTS$", " HEIGHTS") %>% 
  str_replace("\\sSQ$", " SQUARE") %>% 
  str_replace("\\sSPGS$|\\sSPR$|\\sSPRG$", "  SPRINGS") %>% 
  str_replace("\\sJCT$", " JUNCTION") %>% 
  str_replace("^E\\s", "EAST ") %>% 
  str_replace("^N\\s", "NORTH ") %>% 
  str_replace("^W\\s", "WEST ") %>% 
  str_remove(valid_state) %>% 
  str_remove("^X+$")
```

```{r after lookup}
pa_match_table <- pa %>% 
  filter(str_sub(pa$city, 1,1) == str_sub(pa$city_match, 1,1)) %>% 
  filter(city %out% valid_city)  %>% 
  mutate(string_dis = stringdist(city, city_match)) %>% 
  select (index, zip_clean, STATE, city, city_match, string_dis) %>% 
  distinct() %>% 
  add_count(city_match) %>% 
  rename("sec_city_match" = "city_match")

# Manually change overcorrected city names to original 
pa_match_table$sec_city_match <- pa_match_table$sec_city_match %>% 
  str_replace("^ARLINGTON$", "ALEXANDRIA") %>% 
  str_replace("^BROWNSVILLE$", "BENTLEYVILLE") %>% 
  str_replace("^FEASTERVILLE\\sTREVOSE", "FEASTERVILLE") %>% 
  str_replace("LEES SUMMIT", "LAKE LOTAWANA") %>% 
  str_replace("HAZLETON", "HAZLE TOWNSHIP") %>% 
  str_replace("DANIA", "DANIA BEACH") %>% 
  str_replace("CRANBERRY TWP", "CRANBERRY TOWNSHIP")

pa_match_table[pa_match_table$city == "HOLLIDASBURG", "city_match"] <- "HOLLIDAYSBURG"
pa_match_table[pa_match_table$city == "PENN HELLE", "city_match"] <- "PENN HILLS"
pa_match_table[pa_match_table$city == "PHUM", "city_match"] <- "PLUM"
pa_match_table[pa_match_table$city == "CLARKSGREEN", "city_match"] <- "CLARKS GREEN"
pa_match_table[pa_match_table$city == "SANFRANCISCO", "city_match"] <- "SAN FRANCISCO"
pa_match_table[pa_match_table$city == "RIEFFTON", "city_match"] <- "REIFFTON"
pa_match_table[pa_match_table$city == "SHOREVILLE", "city_match"] <- "SHOREVIEW"
pa_match_table[pa_match_table$city == "PITTSBURGH PLUM", "city_match"] <- "PLUM"
pa_match_table[pa_match_table$city == "MOUNTVIEW", "city_match"] <- "MOUNT VIEW"
pa_match_table[pa_match_table$city == "PLUM BORO", "city_match"] <- "PLUM"
pa_match_table[pa_match_table$city == "HAZELTON CITY", "city_match"] <- "HAZLE TOWNSHIP"
pa_match_table[pa_match_table$city == "BARNSVILLE", "city_match"] <- "BARNESVILLE"

keep_original <- c( "SHOREVIEW" ,
   "CUYAHOGA" ,
   "MEDFORD LAKES" ,
   "WEST GOSHEN" ,
   "CLEVELAND HEIGHTS" ,
   "LAHORNE" ,
   "ROCHESTER HILLS" ,
   "PENLLYN" ,
   "SOUTHERN" ,
   "WEST DEPTFORD" ,
   "SEVEN FIELDS" ,
   "LORDS VALLEY" ,
   "WILDWOOD CREST" ,
   "BETHLEHEM TOWNSHIP" ,
   "MOON TOWNSHIP" ,
   "BELFONTE" ,
   "NEWPORT TOWNSHIP" , 
   "LINTHICUM" , 
   "WARRIOR RUN" ,
   "PRIMOS SECANE" ,
   "COOKPORT" , 
   "MANASSAS PARK" ,
   "MCMURRAY" ,
   "MOYLAN" ,
   "BELMONT HILLS" ,
   "THORNBURY" ,
   "HANOVER TOWNSHIP" ,
   "MIAMI SPRINGS" ,
   "BROOKLYN PARK" )

pa_match_table[pa_match_table$city %in% keep_original,"sec_city_match"] <-
  pa_match_table[pa_match_table$city %in% keep_original,"city"]


pa <-pa %>% 
  left_join(select(pa_match_table, index, sec_city_match), by = "index") %>% mutate(city_clean = coalesce(sec_city_match, city))

pa$city_clean <- pa$city_clean %>% str_replace("\\sTWP$", " TOWNSHIP")

n_distinct(pa$city_clean[pa$city_clean %out% valid_city])
```

```{r}
valid_city <- unique(c(valid_city, extra_city))

pa_city_lookup <- read_csv(file = here("pa", "expends", "data", "raw", "pa_city_lookup.csv"), col_names = c("city", "city_lookup", "changed", "count"), skip = 1)

pa_out <- pa %>% 
  count( city_clean, sort = TRUE) %>% 
  filter(city_clean %out% valid_city) 

pa_out <- pa_out %>% left_join(pa_city_lookup, by = c("city_clean" = "city")) %>% filter(city_clean != city_lookup | is.na(city_lookup)) %>% drop_na(city_clean)

pa <- pa %>% left_join(pa_out, by = "city_clean") %>% mutate(city_after_lookup = ifelse(
  is.na(city_lookup) & city_clean %out% valid_city,
  NA,
  coalesce(city_lookup, city_clean))) 

pa$city_after_lookup <- pa$city_after_lookup %>% str_replace("^\\sTWP$", " TOWNSHIP")

pa[pa$index == which(pa$city_after_lookup == "MA"),8:9] <- c("LEXINGTON", "MA")
pa[pa$index == which(pa$city_after_lookup == "L"),8] <- c("LOS ANGELES")
pa[pa$index %in% which(pa$city_after_lookup %in% c("PA", "NJ")), 8] <- ""
pa[pa$index == "319505", 8] <- "HARRISBURG"

#pa_final_lookup <- read_excel(glue("{raw_dir}/pa_final_fixes.xlsx"))
pa_final_lookup <- read_excel(glue("{raw_dir}/pa_final_fixes.xlsx"), col_types = "text")


pa <- pa %>% left_join(pa_final_lookup, by = c("city_after_lookup" = "city_clean")) %>% 
mutate(city_output = if_else(condition = is.na(fix) & city_after_lookup %out% valid_city, NA_character_,
                               coalesce(fix,city_after_lookup)))
```



Each process also increases the percent of valid city names.

```{r city_progress, collapse=TRUE}
prop_in(pa$CITY, valid_city, na.rm = TRUE)
prop_in(pa$city_prep, valid_city, na.rm = TRUE)
prop_in(pa$city_swap, valid_city, na.rm = TRUE)
prop_in(pa$city, valid_city, na.rm = TRUE)
prop_in(pa$city_clean, valid_city, na.rm = TRUE)
prop_in(pa$city_after_lookup, valid_city, na.rm = TRUE)
prop_in(pa$city_output, valid_city, na.rm = TRUE)

progress_table <- tibble(
  stage = c("raw", "norm", "swap", "refine","second match", "lookup", "final lookup"),
  prop_good = c(
    prop_in(str_to_upper(pa$CITY), valid_city, na.rm = TRUE),
    prop_in(pa$city_prep, valid_city, na.rm = TRUE),
    prop_in(pa$city_swap, valid_city, na.rm = TRUE),
    prop_in(pa$city, valid_city, na.rm = TRUE),
    prop_in(pa$city_clean, valid_city, na.rm = TRUE),
    prop_in(pa$city_after_lookup, valid_city, na.rm = TRUE),
    prop_in(pa$city_output, valid_city, na.rm = TRUE)
  ),
  total_distinct = c(
    n_distinct(str_to_upper(pa$CITY)),
    n_distinct(pa$city_prep),
    n_distinct(pa$city_swap),
    n_distinct(pa$city),
    n_distinct(pa$city_clean),
    n_distinct(pa$city_after_lookup),
    n_distinct(pa$city_output)
  ),
  unique_bad = c(
    length(setdiff(str_to_upper(pa$CITY), valid_city)),
    length(setdiff(pa$city_prep, valid_city)),
    length(setdiff(pa$city_swap, valid_city)),
    length(setdiff(pa$city, valid_city)),
    length(setdiff(pa$city_clean, valid_city)),
    length(setdiff(pa$city_after_lookup, valid_city)),
    length(setdiff(pa$city_output, valid_city))
  )
)

diff_change <- progress_table$unique_bad[1]-progress_table$unique_bad[4]
prop_change <- diff_change/progress_table$unique_bad[1]
```

Each step of the cleaning process reduces the number of distinct city values.
There are `r sum(!is.na(pa$CITY))` with `r n_distinct(pa$CITY)` distinct values, after the swap and refine processes, there are `r sum(!is.na(pa$city_output))` entries with `r n_distinct(pa$city_output)` distinct values. 

```{r print_progress, echo=FALSE}
kable(
  x = progress_table,
  format = "markdown", 
  digits = 4,
  col.names = c("Normalization Stage", "Percent Valid", "Total Distinct", "Unique Invalid")
)
```

```{r wrangle_bar_prop, echo=FALSE}
progress_table %>% 
  mutate(stage = as_factor(stage)) %>% 
  ggplot(aes(x = stage, y = prop_good)) +
  geom_hline(yintercept = 0.99) +
  geom_col(fill = "#D95F02") +
  coord_cartesian(ylim = c(0.75, 1)) +
  scale_y_continuous(labels = percent) +
  labs(
    title = "Pennsylvania Expenditures Payee City Progress",
    subtitle = "Percent of total values contained in pre-defined list of cities",
    caption = "Source: Pennsylvania Dept. of State",
    x = "Wrangling Stage",
    y = "Proportion Valid Cities"
  )
```

```{r clean address, echo = TRUE, execute = FALSE}
pa <- pa %>%   
  unite(
    ADDRESS1, ADDRESS2,
    col = address_clean,
    sep = " ",
    remove = FALSE,
    na.rm = TRUE
  ) %>% 
  mutate(address_clean = normal_address(
      address = address_clean,
      add_abbs = usps_city,
      na_rep = TRUE
    ))
```

We also need to pull up the processed filer table to join back to the `FILERID`, `EYEAR` and `CYCLE` field. 

```{r read filer table}
clean_dir <- here("pa", "expends", "data", "processed")
pa_filer <- read_csv(glue("{clean_dir}/pa_filers_clean.csv"), 
                     col_types = cols(.default = col_character())) %>% 
  rename_at(vars(contains("clean")), list(~str_c("filer_",.))) %>% 
  select(
    FILERID,
    EYEAR,
    CYCLE,
    FILERNAME,
    FILERTYPE,
    filer_state,
    ends_with("clean")
  )
```

```{r join with filer}
pa <- pa %>% left_join(pa_filer, by = c("FILERID", "EYEAR", "CYCLE"))
```


## Conclude

1. There are `r nrow(pa)` records in the database
1. There are `r sum(pa$dupe_flag)` records with suspected duplicate filerID, recipient, date, _and_ amount
(flagged with `dupe_flag`)
1. The ranges for dates and amounts are reasonable
1. Consistency has been improved with `stringr` package and custom `normal_*()` functions.
1. The five-digit `zip_clean` variable has been created with `zipcode::clean.zipcode()`
1. The `year` variable has been created with `lubridate::year()`
1. There are `r count_na(pa$CITY)` records with missing `city` values and `r count_na(pa$EXPNAME)` records with missing `payee` values (both flagged with the `na_flag`).

## Export

```{r write_clean}
clean_dir <- here("pa", "expends", "data", "processed")
dir_create(clean_dir)
pa %>% 
  select(
    -index,
    -city_prep,
    -on_year,
    -city_match,
    -match_dist,
    -city_swap,
    -city_refine,
    -city_clean,
    -city_lookup,
    -city_after_lookup,
    -sec_city_match,
    -n,
    -fix,
    -changed,
    -city,
    -count
  ) %>% 
  write_csv(
    path = glue("{clean_dir}/pa_expends_clean.csv"),
    na = ""
  )
```

