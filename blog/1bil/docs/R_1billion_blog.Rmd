---
title: "Using R to Reach One Billion Records"
author: "Kiernan Nicholls"
date: "`r date()`"
output:
  html_document:
    highlight: tango
    fig_height: 5
    fig_width: 10
    code_folding: show
    keep_md: true
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  dpi = 300
)
if (!interactive()) {
  options(width = 120)
  set.seed(5)
}
```

```{r load_packages, echo=FALSE}
if (!require("pacman")) {
  install.packages("pacman")
}
pacman::p_load(
  tidyverse, # data manipulation
  lubridate, # datetime strings
  tabulizer, # read pdf tables
  gluedown, # printing markdown
  janitor, # clean data frames
  campfin, # custom irw tools
  aws.s3, # aws cloud storage
  refinr, # cluster & merge
  readxl, # read excel file
  scales, # format strings
  knitr, # knit documents
  rvest, # scrape html
  glue, # code strings
  here, # project paths
  httr, # http requests
  fs # local storage 
)
```

[The Accountability Project][tap] database recently surpassed one _billion_ 
records! That's one billion rows of campaign contributions, government
contracts, registered voters, property records, lobbyist registrations, stimulus
spending, and more. All searchable alongside one another with blazing speed. To
put that in perspective: one million seconds (the Excel spreadsheet maximum) is
less than twelve days, while one billion seconds is over 31 _years_.

```{r duration_billion}
duration(1e6)
duration(1e9)
```

To reach this impressive milestone, the Investigative Reporting Workshop has
employed a number of tools to scrape, read, parse, request, and manipulate such
a vast amount of data. Here, I am going to cover just some of the work we've 
done using coding workflows to help automate some consistent data processing.

[R] is a free, open source programming language first developed for statistical
computing and taught in many universities. With the rise of the data science
field, R has grown into a fantastic choice for the kind of data processing we do
here at IRW. In fact, this very blog was written in R, like all our data guides.

The ability to write self-contained data diaries with our journalist's thoughts
intermixed alongside actual code means we can publish comprehensive and
comprehensible catalogs of every change we make to public data. This means
anybody, most importantly our future selves, can easy replicate our work from 
start to finish.

All such code is found on the [TAP GitHub Repository][gh], where our markdown
files render as [easy-to-read web pages][al].

[tap]: https://publicaccountability.org/
[R]: https://www.r-project.org/
[gh]: https://github.com/irworkshop/accountability_datacleaning
[al]: https://github.com/irworkshop/accountability_datacleaning/blob/master/R_tap/al/contribs/docs/al_contribs_diary.md
[sc]: https://github.com/irworkshop/campfin
[CRAN]: https://cran.r-project.org/package=campfin

## Software

To facilitate a lot of this work, IRW developed our own R software package. This
`campfin` package helps with everything from reading data to exploring its 
structure and normalizing messy values. The [source code][sc] for this package
also lives on GitHub and can be installed from there or the [CRAN] repository.

```{r install_campfin, eval=FALSE}
install.packages("campfin") # release version
remotes::install_github("irworkshop/campfin") # development
```

Later in this blog, we'll show how `campfin` helps with some specific problems
that come up often in public data like campaign finance.

Other software used is all free and open source and can be easily installed from
withing R, making our entire workflow easy for new staff and third parties.
Every step of the data wrangling process is handled from with code: getting the
data, reading it, exploring it, improving it, and uploading it the
Accountability Project server where it can be searched by our users.

## Getting data

IRW has to use many different methods to obtain all the different kinds of data
found on the Accountability Project. A lot of the time, data can be
downloaded directly from an agency's website. Other times, we have to 
submit formal records requests either ourselves or with a partner.

Downloading files directly is an important step in our reproducible, automated
workflow. By doing everything through code, data can be easily updated at any
time without having to track down the website and download everything by hand.
We can download files from a regular URL, fill out and submit web forms, or even
open a remote web browser window and navigate it with code.

It's easiest when files can be retrieved from a direct URL.

```{r al_url}
al_url <- glue(
  "https://fcpa.alabamavotes.gov/PublicSite/Docs/BulkDataDownloads/",
  "{2019:2021}_CashContributionsExtract.csv.zip"
)
```

```{r echo=FALSE, results='asis'}
md_bullet(al_url)
```

Sometimes, in an attempt to make things "easier" for their users, agencies only
make their data available using a search portal. Here, we are submitting the
same kind of `GET` requests sent by your browser when you fill out a form and
click the "export" button.

```{r echo=FALSE}
data_dir <- here("xx", "1bil", "data")
```

```{r get}
nm_csv <- path(data_dir, "CON_2020.csv")
if (!file_exists(nm_csv)) {
  nm_get <- GET(
    url = "https://login.cfis.sos.state.nm.us",
    path = c("api", "DataDownload", "GetCSVDownloadReport"),
    write_disk(path = nm_csv),
    progress(type = "down"),
    query = list(
      year = "2020",
      transactionType = "CON",
      reportFormat = "csv",
      fileName = "CON_2020.csv"
    )
  )
}
```

In the worst case, IRW uses state and federal records laws like the Freedom of
Information Act to formally request public data. In some case, we've partnered
with the non-profit [MuckRock][mr], specialists in filing records requests. This
was was the case when obtaining a copy of [New Jersey state contracts][nj].
Unfortunately, the request was fulfilled as a 65 page PDF... not the easiest
data to read, but even that can be downloaded directly with R code.

```{r raw_download}
pdf_url <- "https://cdn.muckrock.com/foia_files/2020/05/08/Morisy_Response.pdf"
pdf_path <- path(data_dir, basename(pdf_url))
if (!file_exists(pdf_path)) download.file(pdf_url, pdf_path)
```

[mr]: https://www.muckrock.com/
[nj]: https://www.muckrock.com/foi/new-jersey-229/state-contracts-office-of-the-state-comptroller-91636/

## Reading data

The Accountability Project currently sources data from 192 different publishers.
Everybody from the Alabama Ethics Commission to the Department of Veterans
Affairs. When working with data from literally hundreds of different sources,
you can never expect two files to look the same.

The Accountability Project standardizes data by thinking of every row as a
transaction between two parties. The first step in that standardization data is
to read everything as a tabular data frame in R. NEarly everything can be read:
a delimited text file, Excel spreadsheet, Microsoft Access database, or
thousands of PDF pages.

Sometimes, even simple text files need some work. Regular expressions can help.

```{r read_nm2}
nm_contribs <- nm_csv %>% 
  read_lines(progress = TRUE) %>% 
  # remove quotes within quotes
  str_replace_all("(?<=\\s)\"(?!,)|\"(?=\\s)", "'") %>% 
  read_delim(
    delim = ",",
    escape_double = FALSE,
    escape_backslash = FALSE,
    col_types = cols(
      .default = col_character(),
      # read these text columns as real data
      `Transaction Amount` = col_double(),
      `Transaction Date` = col_date("%m/%d/%Y %H:%M:%S %p"),
      `Filed Date` = col_date("%m/%d/%Y %H:%M:%S %p"),
      `Start of Period` = col_datetime("%b  %d %Y %H:%M%p"),
      `End of Period` = col_datetime("%b  %d %Y %H:%M%p")
    )
  )
```

```{r echo=FALSE}
nm_contribs <- nm_contribs %>% 
  clean_names(case = "snake") %>% 
  rename_with(~str_remove(., "contributor_")) %>% 
  rename_with(~str_remove(., "transaction_"))

rmarkdown::paged_table(
  x = nm_contribs,
  options = list(max.print = 100)
)
```

Sometimes we need to look for more specialized tools. Tabula is used by
newsrooms around the country to read tables from a PDF. In R, we can use the
`tabulizer` package to access those same popular tools.

```{r pdf_extract}
tsv_dir <- dir_create(path(data_dir, "tsv"))
nj_tsv <- dir_ls(tsv_dir, glob = "*.tsv")
if (length(nj_tsv) != 9) {
  extract_tables(
    file = pdf_path,
    pages = 1:9,
    guess = TRUE,
    output = "tsv",
    method = "stream",
    outdir = tsv_dir
  )
}
```

```{r tsv_read, class.source = "fold-hide"}
nj_names <- read_names(nj_tsv[1], delim = "\t")
nj_contracts <- map_df(
    .x = nj_tsv,
    .f = read_tsv,
    skip = 1,
    col_names = nj_names,
    col_types = cols(
      .default = col_character(),
      `Creation Date` = col_date("%m/%d/%y"),
      `Budget Fiscal Year` = col_integer(),
      `Quantity` = col_number(),
      `Unit Price Amount` = col_number()
    )
  )
```

```{r echo=FALSE}
nj_contracts <- nj_contracts %>% 
  clean_names(case = "snake") %>% 
  rename(
    fiscal_year = budget_fiscal_year,
    po_number = purchase_order_number,
    order_desc1 = order_line_description1,
    order_desc2 = order_line_description2
  )

rmarkdown::paged_table(
  x = nj_contracts,
  options = list(max.print = 100)
)
```

## Exploring data

After we've read all these different kinds of data into R, we typically like to
do a little exploratory analysis. Even simple stuff like counting the unique
values of a variable might lead to a story or expose some underlying flaw in the
data.

Why were almost none of the `r comma(nrow(nm_contribs))` contributions made in
New Mexico during the 2020 elections reported on one of the four Primary 
reports? We know primary elections were held for [multiple state races][ss] on
June 2 of 2020. If we do a little follow up, most of the dozen contributions
apparently filed on Primary reports have issues with the contributor name. 
Not too many such contributions, but an issue nonetheless.

[ss]: https://www.kob.com/new-mexico-news/several-conservative-democratic-state-lawmakers-lose-primary-races/5750206/

```{r report_name_col, class.source = "fold-hide"}
nm_contribs %>% 
  count(report_name) %>% 
  add_prop() %>% 
  head(8) %>% 
  ggplot(aes(x = reorder(report_name, -p), y = p)) +
  geom_col(aes(fill = report_name)) +
  scale_fill_brewer(palette = "Dark2", guide = FALSE) +
  scale_y_continuous(labels = percent) +
  scale_x_wrap(15) +
  theme(
    axis.title.x = element_text(margin = margin(t = 10)),
    axis.title.y = element_text(margin = margin(r = 10))
  ) +
  labs(
    title = "New Mexico Contributions, Report Types",
    x = "Report Name",
    y = "Percent"
  )
```

```{r echo=FALSE}
prim_contribs <- nm_contribs %>% 
  filter(str_detect(report_name, "Primary")) %>% 
  select(first_name, last_name, amount, date, committee_name)
rmarkdown::paged_table(
  x = prim_contribs,
  options = list(max.print = 100)
)
```

We also visually and statistically explore amount values, looking at the
smallest and largest transactions for anything out of the ordinary. We try not
to filter our or remove any data provided by the source, but understanding any
potential flaws is an important step.

```{r amt_summary}
noquote(map_chr(summary(nm_contribs$amount), dollar))
```

```{r amt_histogram, class.source = "fold-hide"}
nm_contribs %>%
  filter(amount >= 1) %>% 
  ggplot(aes(amount)) +
  geom_histogram(fill = dark2["purple"], bins = 50) +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(
    breaks = sort(c(1 %o% 10^(0:3), 2.5 %o% 10^(0:3), 5 %o% 10^(0:3))),
    labels = function(x) {
      str_remove(dollar(x), "\\.\\d{2}$")
    },
    trans = "log10"
  ) +
  theme(
    axis.title.x = element_text(margin = margin(t = 10)),
    axis.title.y = element_text(margin = margin(r = 10))
  ) +
  labs(
    title = "New Mexico Contributions, Transaction Amount",
    x = "Amount",
    y = "Count"
  )
```

We also explore the range of date variables. In this Ohio voter registration
data from 2020, we can plot the amount of voters registered each year and
organize them by election cycle and federal election type.

```{r yr_oh_voters, class.source = "fold-hide"}
oh_voter_years <- read_csv(path(data_dir, "oh_voter_years.csv"))
yrs <- seq(1900, 2020, by = 4)
oh_voter_years %>% 
  expand(reg_year = 1900:2021) %>% 
  left_join(oh_voter_years, by = "reg_year") %>% 
  replace(is.na(.), 0) %>% 
  mutate(
    prez = reg_year %% 4 == 0,
    cycle = yrs[cumsum(reg_year %% 4 == 0)],
    type = if_else(
      condition = reg_year %% 4 == 0,
      true = "Presidential",
      false = if_else(
        condition = reg_year %% 2 == 0,
        true = "Midterm",
        false = "NA"
      )
    )
  ) %>% 
  ggplot(aes(x = cycle, y = n)) +
  geom_col(aes(fill = type)) +
  scale_fill_manual(values = c("#1B9E77", "#666666", "#D95F02")) +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(breaks = seq(1900, 2020, by = 8)) +
  theme(
    legend.position = "bottom", 
    panel.grid.minor = element_blank(),
    axis.title.x = element_text(margin = margin(t = 10)),
    axis.title.y = element_text(margin = margin(r = 10))
  ) +
  labs(
    title = "Current Ohio Voters Registered by Federal Election Cycle",
    subtitle = "Two non-election years, one miderm, one presidential per cycle",
    fill = "Federal Election Type",
    x = "Election Cycle",
    y = "Voters Registered"
  )
```

You may have noticed the unusual number of 138 year old voters currently
registered in the state of Ohio. In fact, all the voters registered in that year
were supposedly registered on January 1. It's pretty obvious this date is being
used as the default when no actual date is used. In a case like this, we would
create a copy column with those dates removed. This fixed date is the one users
would see on our site when searching the data. Dates aren't the only types of
weird data we sometimes have to improve.

## Improving data

Users on the Accountability Project search for transactions using a name and/or
address. To make this easier for the user, we typically standardize addresses
using USPS standards. That way, when they search an address, ZIP code, or city
name, they get the results they are looking for.

It's not uncommon to get a handful of records spelling out the full state or
using a nine digit ZIP code. We make everything consistent using the tools
in IRW's `campfin` package.

```{r state_norm}
nm_contribs %>% 
  filter(zip_code %out% valid_zip) %>% 
  count(zip_code, sort = TRUE) %>% 
  mutate(new_zip = normal_zip(zip_code))
```

```{r zip_norm}
nm_contribs %>% 
  filter(state %out% valid_state) %>% 
  count(state, sort = TRUE) %>% 
  mutate(new_state = normal_state(state))
```

For addresses, we use official USPS abbreviations and fix spacing,
capitalizationm and punctuation.

```{r addr_norm}
normal_address("12east 2nd street, suite-209", abbs = usps_street)
```

Cities are the hardest to normalize. I chose to highlight New Mexico
contributions here for one reason. Albuquerque... or is it Albuqueruqe? There
are **88** different city names that appear to be referring to Albuquerque.

```{r abq_city}
nm_cities <- nm_contribs$city[nm_contribs$state == "NM"]
sort(unique(str_subset(nm_cities, regex("ALB", ignore_case = TRUE))))
```

We can use our city normalization workflow. Forcing capitalization, punctuation,
and abbreviation and comparing against the city name we would expect for a
contribution made from that ZIP code. Comparing against these expected names
gives us a very confident and consistent way to correct misspellings. We only
make changes that give us a valid combination, leaving everything as it was.

```{r city_norm, class.source = "fold-hide"}
nm_city <- nm_contribs %>%
  distinct(city, state, zip_code) %>% 
  filter(
    state == "NM", 
    str_detect(city, regex("ALB", ignore_case = TRUE))
  ) %>% 
  mutate(
    city_norm = normal_city(
      city = city, 
      abbs = usps_city, 
      states = c("NM", "US", "UNITED STATES"), 
      na = invalid_city
    ),
    state = normal_state(state), zip_code = normal_zip(zip_code),
  ) %>% 
  rename(city_raw = city) %>% 
  left_join(
    y = zipcodes,
    by = c("state", "zip_code" = "zip")
  ) %>% 
  rename(city_match = city) %>% 
  mutate(
    match_abb = is_abbrev(city_norm, city_match),
    match_dist = str_dist(city_norm, city_match),
    city_swap = if_else(
      condition = !is.na(match_dist) & (match_abb | match_dist <= 2),
      true = city_match,
      false = city_norm
    )
  ) %>% 
  select(
    -city_match,
    -match_dist,
    -match_abb
  ) %>% 
  rename(city = city_raw) %>% 
  distinct()

nm_contribs <- left_join(nm_contribs, nm_city, c("city", "state", "zip_code"))

good_refine <- nm_contribs %>% 
  mutate(
    city_refine = city_swap %>% 
      key_collision_merge() %>% 
      n_gram_merge(numgram = 1)
  ) %>% 
  filter(city_refine != city_swap) %>% 
  inner_join(
    y = zipcodes,
    by = c(
      "city_refine" = "city",
      "state",
      "zip_code" = "zip"
    )
  )

nm_contribs <- nm_contribs %>% 
  left_join(good_refine, by = names(.)) %>% 
  mutate(city_refine = coalesce(city_refine, city_swap))

abq_city <- nm_contribs %>% 
  select(starts_with("city"), state, zip_code) %>% 
  filter(state == "NM", str_detect(city, regex("ALB", ignore_case = TRUE)))
```

This process brought our selection of Albuquerque-esque cities down from 88 to
just _15_, some of which are actually other cities in New Mexico. The remaining
probably have a ZIP code outside what we would expect, making it difficult to
compare the misspellings. They can be corrected by hand if need be.

```{r city_post}
count(abq_city, city_refine, sort = TRUE)
```

This is the improvements we made on a single city, for a single year. When 
dealing with millions city names across dozens of years, this process can
drastically reduce the number of weird city misspellings and make the data
easier to search for our users.

```{r echo=FALSE}
abq_city %>% 
  distinct() %>% 
  select(city, state, zip_code, city_refine) %>% 
  filter(str_to_upper(city) != city_refine) %>% 
  rmarkdown::paged_table(options = list(max.print = 100))
```
