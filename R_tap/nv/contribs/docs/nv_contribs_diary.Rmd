---
title: "Nevada Contributions"
author: "Kiernan Nicholls"
date: "`r date()`"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 3
editor_options: 
  chunk_output_type: console
---

<!-- Place comments regarding knitting here -->

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.path = "../plots/",
  fig.width = 10,
  dpi = 300
)
if (!interactive()) {
  options(width = 120)
  set.seed(5)
}
```

```{r create_docs_dir, eval=FALSE, echo=FALSE, include=FALSE}
doc_dir <- fs::dir_create(here::here("nv", "contribs", "docs"))
```

## Project

The Accountability Project is an effort to cut across data silos and give
journalists, policy professionals, activists, and the public at large a simple
way to search across huge volumes of public data about people and organizations.

Our goal is to standardizing public data on a few key fields by thinking of each
dataset row as a transaction. For each transaction there should be (at least) 3
variables:

1. All **parties** to a transaction.
2. The **date** of the transaction.
3. The **amount** of money involved.

## Objectives

This document describes the process used to complete the following objectives:

1. How many records are in the database?
1. Check for entirely duplicated records.
1. Check ranges of continuous variables.
1. Is there anything blank or missing?
1. Check for consistency issues.
1. Create a five-digit ZIP Code called `zip`.
1. Create a `year` field from the transaction date.
1. Make sure there is data on both parties to a transaction.

## Packages

The following packages are needed to collect, manipulate, visualize, analyze,
and communicate these results. The `pacman` package will facilitate their
installation and attachment.

```{r load_packages, message=FALSE, warning=FALSE, error=FALSE}
if (!require("pacman")) {
  install.packages("pacman")
}
pacman::p_load(
  tidyverse, # data manipulation
  lubridate, # datetime strings
  gluedown, # printing markdown
  janitor, # clean data frames
  campfin, # custom irw tools
  aws.s3, # aws cloud storage
  refinr, # cluster & merge
  scales, # format strings
  knitr, # knit documents
  vroom, # fast reading
  rvest, # scrape html
  glue, # code strings
  here, # project paths
  httr, # http requests
  fs # local storage 
)
```

This document should be run as part of the `R_campfin` project, which lives as a
sub-directory of the more general, language-agnostic
[`irworkshop/accountability_datacleaning`][tap] GitHub repository.

The `R_campfin` project uses the [RStudio projects][rproj] feature and should be
run as such. The project also uses the dynamic `here::here()` tool for file
paths relative to _your_ machine.

```{r where_here}
# where does this document knit?
here::i_am("nv/contribs/docs/nv_contribs_diary.Rmd")
```

[tap]: https://github.com/irworkshop/accountability_datacleaning
[rproj]: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

## Data

The Nevada Secretary of State (NVSOS) office requires that one register for an
account to access "[bulk data download][bulk]" service page.

> Welcome to the Nevada Secretary of State online unified login system.  Here
you may access the following systems all with one login account:
> * Bulk Data Download
> * ...

The process for downloaded a report is [outlined here][guide]:

Create a report for "Full Unabridged Database Dump" of "Campaign Finance" data.

> This report will expose Contributions and Expenses report data filed within
our “Aurora” Campaign Financial Disclosure system. This would not include data
filed in a Financial Disclosure report. This bulk data report tool here should
be used to pull the entire database or slightly smaller subsets of data such as
all contributions filed after 1/1/2016 by groups of type “PAC”...

### Report

The site allows users to define the format for their data download. The site
generated the following summary of our specified data format:

> If "Text File - Fixed Width" is selected your report results will be inserted
into a standard ASCII text file where each field starts at a specific "fixed"
position for each line. For more specific information about the report format
for custom built reports, including the position and data type of each field,
click the "View Selected File Structure" button on the General tab when you
view\edit your report.

This file structure report is an HTML page with a description and six tables.

> Your report will generate 6 fixed width ASCII text file(s) compressed into one
Zip file named in the format "CampaignFinance.43993.<Today's Date>.zip"*. Below
you will find the format of each file:

### Age

> The data being reported off of is no more than 24 hours old. This data is
copied very late each night from live data to minimize the large burden of bulk
reporting on the production system.

### Format

The report data is split into multiple files, per the [NVSOS FAQ page][faq]:

> This is what is referred to as a normalized relational structure in the
database world. Data items such as business entities and officers have a direct
relation to one another. There can be any number of officers to one business
entity. Because of this many to one relationship, the officers data is stored in
a different data table (or file) than the business entities. Then we relate
officer records to a business entity record by a common key data column, in this
case the CorporationID... By separating officers and entities into separate
records we can eliminate the redundancy and added size associated with putting
the business entity data on each officer record or eliminate the complexity of
allocating an undeterminable amount of officers on the one business entity
record. This same many-to-one relationship is true of voter history records to
voter records, UCC actions to UCC liens or Corporation Stocks to Corporations,
to name a few.

[bulk]: https://www.nvsos.gov/sos/online-services/data-download
[guide]: https://www.nvsos.gov/SoSServices/AnonymousAccess/HelpGuides/DataDownloadUserGuide.aspx
[faq]: https://www.nvsos.gov/SOSServices/AnonymousAccess/HelpGuides/FAQ.aspx#5

#### Tables

The summary continues to provide individual structure summaries on each of the
six files included in the report along with an key to the file name. These six
tables contain columns describing both the data type and the width of each
column. This information is needed to properly read the flat text files.

```{r about_file}
st_dir <- here("nv", "contribs")
about_path <- path(st_dir, "File Format - Secretary of State, Nevada.html")
about <- read_html(x = about_path)
```

```{r about_tables}
about_tables <- about %>% 
  html_nodes(".entryform") %>% 
  html_table(fill = TRUE) %>% 
  map(as_tibble)
```

```{r about_tables_fix}
about_tables <- about_tables[map_lgl(about_tables, ~ncol(.) == 4)]
about_tables <- map(about_tables, row_to_names, row_number = 1)
about_names <- str_subset(html_text(html_nodes(about, "b")), "\\d")
```

```{r echo=FALSE}
names(about_tables) <- about %>% 
  html_nodes(css = "b") %>% 
  html_text(trim = TRUE) %>% 
  str_subset(pattern = "\\d") %>% 
  str_remove(pattern = "^\\d\\)\\s") %>% 
  str_remove(pattern = "\\sResults$") %>% 
  str_remove(pattern = "-Payees")
```

```{r about_tables_print, echo=FALSE}
kable(about_tables, align = "llrr")
```

#### Data Types

> The Data Types within this column correspond to the SQL Server 2012 data types
in which the source data is stored. The purpose of exposing these data types is
simply to provide a suggested guideline for any software programmers writing an
interface to process these report files. Below is a partial list of SQL Server
data types:

> * `bigint` - Numeric, 8 bytes
> * `int` - Numeric, 4 bytes
> * `smallint` - Numeric, 2 bytes
> * `tinyint` - Numeric, 1 byte
> * `bit` - Results are True\False which will be represented as "T"\"F"
> * `money` - Monetary data, 8 bytes, accuracy to a ten-thousandth of a unit
> * `float` - Floating precision number from -1.79E + 308 through 1.79E + 308.
> * `real` - Floating precision number from -3.40E + 38 through 3.40E + 38.
> * `datetime` - Date
> * `char` - Fixed-length character data
> * `varchar` - Variable-length data with a maximum of 8,000 characters

### Variables

Definitions for few variables can be found in the "Result Field" tab:

1. `Jurisdiction`:
    > This will be name of the city or county for city/county offices currently 
    held by the candidate (e.g. “CITY OF YERINGTON”, “DOUGLAS COUNTY”). This 
    will be set to “NV SOS” for statewide offices such as Governor, State 
    Controller or State assemblymen. An office assigned to a candidate could be
    updated by the NV SOS Elections staff as necessary when that candidate files
    for a new office.

2. `Contribution Type`:
    > Use this column to differentiate which one of four contribution types this
    contribution record is: Monetary Contribution, In Kind Contribution, In Kind
    Written Commitment, or Written Commitment.

3. `Last Name`:
    > When the contributor or payee is an organization as opposed to an 
    individual, the entire organization name will be in the Last Name field 
    only.

4. `Expense Type`:
    > Use this column to differentiate which type of expense record this is:
    Monetary Expense or In Kind Expense.

5. `Active`:
    > A value of F (False) indicates the group has been marked as inactive by 
    the NV Secretary of State's office Elections division due to submission of 
    a "notice of inactivity" or for failure to renew annual registration.

6. `Amended`:
    > A value of T (True) indicates this contributions and expense report has 
    been marked as an amended report by the original filer implying this report 
    supersedes a report for this same period, filed earlier. An amended report
    is to be full comprehensive for that report period and in essence replaces
    all contributions and expenses filed in the earlier report.

7. `Election Cycle`:
    > The Election Cycle is the 4 digit filing or reporting year defining a 
    filing period grouping together a collection of contribution and expenses 
    reports...

8. `Superseded`:
    > A report is Superseded when an amended report was filed later by the same
    filer for the same reporting period. In this case the Superseded field for
    the older report record will be set to T (True)...

### Records

> Total number of records returned: 1,104,096

```{r rows_total}
total_rows <- 1104096
```

> A record is one single entity or row from a database table. The "Total number
of records returned" displayed on the report preview page will be a summation of
all rows returned from each table you are reporting from. For example, if your
report queries for both Resident Agent and Corporation data from the
Corporations database, the number of records returned might be 1000, 700 of
which might be Corporation records and 300 being Resident Agent records.

## Download

Within seven days of running the report, the data can be downloaded from the
link provided to the account email address. The link will not work for anybody
not logged into that NVSOS account.

```{r raw_dir}
raw_url <- "https://www.nvsos.gov/yourreports/CampaignFinance.43993.012621101815.zip"
raw_dir <- dir_create(here("nv", "contribs", "data", "raw"))
raw_zip <- path(raw_dir, basename(raw_url))
file_size(raw_zip)
```

This URL contains the date the report was generated.

```{r raw_time}
report_time <- mdy_hms(str_extract(raw_url, "\\d+(?=\\.zip$)"))
with_tz(report_time, tzone = "PST")
```

```{r raw_download}
if (!file_exists(raw_zip)) {
  download.file(raw_url, raw_zip)
} else {
  aws_backup <- path("IRW/raw_backup/nv", basename(raw_zip))
  save_object(
    object = aws_backup,
    bucket = "publicaccountability",
    file = raw_zip
  )
}
```

This raw ZIP archive has been backed up to the IRW server.

```{r}
as_fs_bytes(object_size(
  object = aws_backup, 
  bucket = "publicaccountability"
))
```

## Unzip

The provided ZIP archive contains all six tables as fixed width text files.

```{r zip_unzip}
raw_txt <- unzip(raw_zip, exdir = raw_dir)
```

```{r echo=FALSE}
raw_zip %>% 
  unzip(list = TRUE) %>% 
  as_tibble() %>% 
  mutate(
    across(Length, as_fs_bytes),
    across(Name, md_code)
  ) %>% 
  kable()
```

We need to match the order of this vector to the order of the tables.

```{r zip_name}
names(raw_txt) <- c(
  "Candidates",
  "Contributions",
  "Contributors",
  "Expenses",
  "Groups",
  "Reports"
)
```

```{r zip_name_sort}
raw_txt <- raw_txt[match(names(about_tables), names(raw_txt))]
names(raw_txt) == names(about_tables)
```

## Columns

We can use the tables read from the HTML file, and described in the **About**
section above, to create (1) the column width tables expected by `read_fwf()`,
and (2) the readr column type specification objects. Two functions will
take the `Field Name`, `Data Type` and `Length` 

```{r as_fwf_width}
as_fwf_width <- function(.data) {
  fwf_widths(
    widths = as.integer(x = .data[[4]]),
    col_names = str_remove(
      string = .data[[1]], 
      pattern = "\\s\\(.*\\)"
    )
  )
}
```

```{r as_col_spec}
as_col_spec <- function(.data) {
  x <- .data[["Data Type"]]
  x <- case_when(
    str_detect(x, "varchar")  ~ "c",
    str_detect(x, "datetime") ~ "D",
    str_detect(x, "money")    ~ "d",
    str_detect(x, "bit")      ~ "l",
    str_detect(x, "int")      ~ "i",
    TRUE ~ "c"
  )
  as.col_spec(
    x = setNames(
      object = x,
      nm = str_remove(
        string = .data[["Field Name"]], 
        pattern = "\\s\\(.*\\)"
      )
    )
  )
}
```

```{r about_convert_show}
about_tables$Groups
as_fwf_width(about_tables$Groups)
as_col_spec(about_tables$Groups)
```

```{r about_format}
raw_widths <- map(about_tables, as_fwf_width)
raw_types  <- map(about_tables, as_col_spec)
```

## Read

One file seems to have been exported with some empty rows and other rows without
sufficient padding. This can be fixed with string manipulation.

```{r raw_fix_names}
tmp <- file_temp(ext = "txt")
raw_txt[["Contributors"]] %>% 
  read_lines(skip_empty_rows = TRUE) %>% 
  str_subset(pattern = "^\\d") %>% 
  str_pad(width = 170, side = "right", pad = " ") %>% 
  write_lines(file = tmp)
raw_txt[["Contributors"]] <- tmp
```

All six tables can then be read into a list using `readr::read_fwf()` and the
(1) width tables and (2) column type specifications.

```{r raw_read}
nv <- pmap(
  .f = read_fwf,
  locale = locale(
    date_format = "%m/%d/%Y",
    tz = "US/Pacific"
  ),
  .l = list(
    file = raw_txt,
    col_positions = raw_widths,
    col_types = raw_types
  )
)
```

The total number of rows read matches what we were told when exporting.

```{r raw_row_check}
sum(map_dbl(nv, nrow)) == total_rows
```

```{r raw_rename, echo=FALSE}
nv <- map(nv, clean_names)
```

## Join

The primary table of interest here is `Contributions`, which lists the
`r comma(nrow(nv$Contributions))` contributions made to committees and reported
to the state. This table does not identify the receiving committee or
contributing entity. This information is found in the `Groups` and
`Contributors` tables, respectively. We need to add variables identifying all
parties to each contribution.

```{r con_show}
nv$Contributions
```

### Recipient

We will first join the committees. Contributions can be made to either a
candidate or committee, each listed in their own table with their own
key column in Contributions. We can combine these keys and tables.

```{r rec_cand_fix}
rec_cands <- nv$Candidates %>% 
  select(candidate_id, first_name, last_name) %>% 
  filter(candidate_id %in% nv$Contributions$candidate_id) %>% 
  mutate(rec_type = "Candidate", .after = last_name) %>% 
  unite(
    col = recipient_name,
    first_name, last_name,
    sep = " ",
    remove = TRUE,
    na.rm = TRUE
  )
```

```{r rec_groups_fix}
rec_comms <- nv$Groups %>% 
  filter(group_id %in% nv$Contributions$group_id) %>% 
  select(group_id, recipient_name = group_name, recipient_type = group_type)
```

```{r con_coalesce, eval=FALSE, echo=FALSE}
nv$Contributions <- mutate(
  .data = nv$Contributions,
  .after = contribution_id,
  .keep = "unused",
  rec_id = coalesce(candidate_id, group_id)
)
```

```{r rec_join}
nv$Contributions <- left_join(
  x = nv$Contributions,
  y = bind_rows(rec_cands, rec_comms),
  by = c("candidate_id", "group_id")
)
```

### Contributors

```{r ctbr_nms}
ctbr_nms <- nv$Contributors %>% 
  filter(contact_id %in% nv$Contributions$ctbr_id) %>% 
  unite(
    col = ctbr_nm,
    first_nm, middle_nm, last_nm,
    sep = " ",
    remove = TRUE,
    na.rm = TRUE
  )
```

```{r ctbr_join}
nv$Contributions <- left_join(
  x = nv$Contributions,
  y = ctbr_nms,
  by = c("ctbr_id" = "contact_id")
)
```

### Finish

```{r}
nvc <- nv$Contributions %>% 
  relocate(ctbr_id, .before = ctbr_nm) %>% 
  mutate(
    cand_id = str_c("C", cand_id),
    group_id = str_c("G", group_id),
    rec_id = coalesce(cand_id, group_id),
    .before = rec_nm
  ) %>% 
  select(-cand_id, -group_id)
```

## Explore

There are `r comma(nrow(nvc))` rows of `r ncol(nvc)` columns. Each record
represents a single contribution to a political committee or candidate.

```{r glimpse}
glimpse(nvc)
tail(nvc)
```

### Missing

There are no columns missing values.

```{r na_count}
col_stats(nvc, count_na)
```

### Duplicates

We can also flag any record completely duplicated across every column.

```{r dupe_flag}
nvc <- flag_dupes(nvc, -con_id)
sum(nvc$dupe_flag)
```

```{r key_vars}
key_vars <- c("ctbr_nm", "con_dt", "con_amt", "rec_nm")
```

```{r dupe_view}
nvc %>% 
  filter(dupe_flag) %>% 
  select(con_id, rpt_id, all_of(key_vars))
```

### Categorical

```{r distinct_count}
col_stats(nvc, n_distinct)
```

```{r distinct_plots, echo=FALSE, fig.height=3}
explore_plot(nvc, con_type)
explore_plot(nvc, rec_type) + scale_x_wrap()
```

### Amounts

```{r amount_summary}
summary(nvc$con_amt)
mean(nvc$con_amt <= 0)
```

These are the records with the minimum and maximum amounts.

```{r amount_minmax}
glimpse(nvc[c(which.max(nvc$con_amt), which.min(nvc$con_amt)), ])
```

```{r hist_amount, echo=FALSE}
nvc %>%
  ggplot(aes(con_amt)) +
  geom_histogram(fill = dark2["purple"]) +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(
    breaks = c(1 %o% 10^(0:6)),
    labels = dollar,
    trans = "log10"
  ) +
  labs(
    title = "Nevada Contributions Amount Distribution",
    caption = "Source: {source}",
    x = "Amount",
    y = "Count"
  )
```

### Dates

We can add the calendar year from `con_dt` with `lubridate::year()`

```{r date_year}
nvc <- mutate(nvc, con_yr = year(con_dt))
```

```{r date_range}
min(nvc$con_dt)
max(nvc$con_dt)
sum(nvc$con_dt > today())
```

```{r bar_year, echo=FALSE}
nvc %>% 
  count(con_yr) %>% 
  mutate(even = is_even(con_yr)) %>% 
  ggplot(aes(x = con_yr, y = n)) +
  geom_col(aes(fill = even)) + 
  scale_fill_brewer(palette = "Dark2") +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(breaks = seq(2000, 2020, by = 2)) +
  theme(legend.position = "bottom") +
  labs(
    title = "Nevada Contributions by Year",
    caption = "Source: {source}",
    fill = "Election Year",
    x = "Year Made",
    y = "Count"
  )
```

## Wrangle

There are no geographic variables in the database.

## Conclude

```{r clean_glimpse}
glimpse(sample_n(nvc, 50))
```

1. There are `r comma(nrow(nvc))` records in the database.
1. There are `r comma(sum(nvc$dupe_flag))` duplicate records in the database.
1. The range and distribution of `amount` and `date` seem reasonable.
1. There are `r comma(sum(nvc$na_flag))` records missing key variables.
1. Consistency in geographic data has been improved with `campfin::normal_*()`.
1. The 4-digit `year` variable has been created with `lubridate::year()`.

## Export

Now the file can be saved on disk for upload to the Accountability server.

```{r clean_dir}
clean_dir <- dir_create(here("nv", "contribs", "data", "clean"))
clean_path <- path(clean_dir, "nv_contribs_clean.csv")
write_csv(nvc, clean_path, na = "")
(clean_size <- file_size(clean_path))
file_encoding(clean_path) %>% 
  mutate(across(path, path.abbrev))
```

## Upload

We can use the `aws.s3::put_object()` to upload the text file to the IRW server.

```{r aws_upload, eval=FALSE}
aws_path <- path("csv", basename(clean_path))
if (!object_exists(aws_path, "publicaccountability")) {
  put_object(
    file = clean_path,
    object = aws_path, 
    bucket = "publicaccountability",
    acl = "public-read",
    show_progress = TRUE,
    multipart = TRUE
  )
}
aws_head <- head_object(aws_path, "publicaccountability")
(aws_size <- as_fs_bytes(attr(aws_head, "content-length")))
unname(aws_size == clean_size)
```
